# Spark简介与特征

> Spark是基于内存计算的大数据并行计算框架，可用于构架大型的、低延迟的数据分析应用程序。
>
> 官网的描述：“Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.”
>
> 可见，**Spark主要专注于单机或集群的数据引擎**。
>
> 几个典型的特点需要先有个概览：
>
> 1. 运行速度快：使用DAG执行引擎以支持循环数据流与内存计算
> 2. 容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程
> 3. 流数据处理方案健全：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件
> 4. 可迁移性：Spark可以作为大数据应用的资源管理和调度器（有点像zookeeper的能力），可以处理所有Hadoop支持的数据，包括 HDFS 、 HBase 和 Cassandra 等
>
> 缺点：
>
> 1. 表达能力有限，只提供了Map与Reduce难以表达过于复杂的计算模型
> 2. 磁盘开销过大：计算过程会将中间过程写入磁盘，下个过程再从磁盘中取出（虽然说是说基于内存的）
> 3. 高延迟：任务之间的衔接涉及IO开销；下个任务的执行依赖于上个任务的结果，这样就造成了其无法完成过于复杂、多阶段的计算任务（流式数据带来的弊端，很难避免）

# 一、Spark简介

> 2009 年诞生于美国加州大学伯克利分校 的AMP 实验室
> 2014 年 2 月，Spark 成为 Apache 的顶级项目
>
> 2016年Spark2.0发布
>
> 2019年Spark3.0发布
>
> 现如今，Spark已经成为大数据处理中不可或缺的解决方案。

## 1.1 为什么会有Spark？

> 参考：https://blog.csdn.net/weixin_43597208/article/details/124831573

大数据相关的技术和系统，属于数据管理系统的范畴，这里面无非就是两个方面的问题：

1. 数据怎么算？（I/O密集型数据——>CPU密集型数据——>分布式系统的发展中，衍生除了HDFS、Hive、Spark等软件）
2. 数据怎么存？（Mysql、Redis、Memcached等）

> 补充一个知识：
>
> 即使是在云存储这么繁荣的现在，单机或者CVM机器方案仍然是有效的备选方案：
>
> - 比如，一个服务器上配6块4T的硬盘（24T的理论容量，减去一些数据报冗余、格式化的损失等，仍然能保证10T的可用容量），再配上128G的内存，2个CPU；
> - 这样的机器上面进行数据管理和存储，加上调优，**可以达到单表处理10亿条数据**，完全没有问题。
>
> 所以，并不是什么场景都需要大数据管理系统的，要根据场景进行合理选择。
>
> 只不过是因为在云计算的背景下，这些新的大数据解决方案才被提出来，作为数据管理方案。

Spark 产生之前，已经有 MapReduce 这类非常成熟的计算系统存在了，并提供了高层次的 API(map/reduce)，把计算运行在集群中并提供容错能力，从而实现分布式计算。（尽管在计算层面，Spark相对MapReduce有巨大的性能优势，但至今仍有许多计算工具是局域MapReduce架构的，比如Hive）

虽然 MapReduce 提供了对数据访问和计算的抽象，但是对于数据的复用就是简单的将中间数据写到一个稳定的文件系统中(例如 HDFS)，所以会**产生数据的复制备份，磁盘的 I/O 以及数据的序列化**，所以在遇到需要在多个计算之间复用中间结果的操作时效率就会非常的低。

认识到这个问题后，学术界的 AMPLab 提出了一个新的模型，叫做 <font color='red'>RDD</font>。**RDD 是一个可以容错且并行的数据结构(其实可以理解成分布式的集合，操作起来和操作本地集合一样简单)，它可以让用户显式的将中间结果数据集保存在内存中，并且通过控制数据集的分区来达到数据存放处理最优化.**同时 RDD 也提供了丰富的 API (map、reduce、filter、foreach、redeceByKey…)来操作数据集。后来 RDD 被 AMPLab 在一个叫做 Spark 的框架中提供并开源。

## 1.2 关键技术

Spark 中的 Spark SQL 、SparkStreaming 、MLLib 、GraphX 、R 五大子框架和库之间可以无缝地共享数据和操作。

- **Spark SQL：**Spark 用来操作**结构化数据**的程序包。通过 Spark SQL，我们可以使用 SQL 操作数据。
- **Spark Streaming：**Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API。
- **Spark MLlib：**提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能。
- **GraphX(图计算)：**Spark 中用于图计算的 API，性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法。
- 集群管理器：Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。
- Structured Streaming：处理结构化流，统一了离线和实时的 API。相当于SparkSQL的升级版，和SparkStreaming区别是：
  - SparkStreaming不支持“事件时间窗口”，同时不是一个完整的流计算模块，它的流计算是以“微批”的模式去提供的；
  - 所以Structured Streaming是更推荐的流处理模块



## 1.3 应用场景

Spark身体系统组件应用场景：

| 应用场景                 | 时间跨度     | 其他框架               | Spark生态系统   |
| ------------------------ | ------------ | ---------------------- | --------------- |
| 批数据处理               | 小时         | Hadoop MapReduce、Hive | Spark Core      |
| 基于历史数据的交互式查询 | 分钟级、秒级 | Impala、Dremel、Drill  | Spark SQL       |
| 实时数据流处理           | 毫秒、秒级   | Storm、S4              | Spark Streaming |
| 基于历史数据的数据挖掘   | -            | Mahout                 | MLlib           |
| 图结构数据               | -            | Pregel、Hama           | GraphX          |



# 二、Spark与Hadoop、Hive、Redis等区别

> 参考：
>
> 1. 【大数据技术生态中，Hadoop、Hive、Spark是什么关系？| 通俗易懂科普向】 https://www.bilibili.com/video/BV1LU4y1e7Ve/?share_source=copy_web&vd_source=48132fe31ff165cc6ae38ecd35647c88

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221130213831.png)

相比于Hadoop MapReduce，Spark主要具有如下优点：

1. Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供了多种数据集操
   作类型，编程模型比Hadoop MapReduce更灵活
2. Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高
3. Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制

## 2.1 Hadoop

1. Hadoop在数据存储中的作用？HDFS

Hadoop会统一管理集群中机器的存储空间，提供一个接口，让这个集群中100台机器上的空间像是在一台机器上，并且用户对于空间的操作也是透明的，保证了开发者在coding的时候，不用额外考虑存储的问题。

2. 集群中的数据应该怎么计算？MapReduce

这里就会遇到一个问题：怎么去写程序操作这100台机器来协作完成同一个计算任务？（比如：任务怎么分配到机器上？任务之间怎么做同步？如果这个过程中，某一台机器down掉应该怎么办？）——其实就是并行（不是==并发==）编程复杂性的问题！ 

为此，Hadoop中引入了一个模块——**MapReduce**（提供了一个任务并行的框架，通过它的API抽象，让用户把并行程序分成两个阶段，一个是Map阶段（很好理解，映射，就是把任务先分好），另一个就是Reduce阶段（对数据结果进行汇总））

## 2.2 Hive

> 上面的Hadoop已经解决了数据存储和计算的问题，似乎已经完美了，但是细看，还是有优化空间的。

在单机数据库处理时代，一般用SQL来做数据处理程序，但是在Hadoop中，不在使用SQL了，而是用MapReduce程序来进行数据处理（这不仅是一个技术迁移，还是一个难度很大的技术迁移）。

能不能在Hadoop上写SQL？Hive！

1. 为了让用户能够直接写SQL来处理数据，就需要对数据进行结构化处理，毕竟SQL里面的‘S'就是Structure的意思。

2. 为此，Hive里面一个核心模型MetaStore，就是用来存储这些结构化的信息（“表”信息——多少列，每个列是什么样的数据结构）

3. 然后，Hive里面的执行引擎就会对一条SQL进行处理（语法分析、生成语法树）（这里和普通的数据库没什么区别）
4. Hive的引擎会把SQL翻译成一个MapReduce的任务去执行，再把执行的结果进行加工后返回给用户

> 这就把之前的大数据工程师又变回了SQLboys。其实，在开发过程中，效率和灵活性就是一对矛盾体：Hive的出现使得大数据开发效率变高了，但是数据处理的表达力和灵活性却下降了（不如直接写MapReduce）。
>
> Impala、presto等和Hive一样，也是SQL on Hadoop的典型方案

## 2.3 Spark

- Spark经常被用来和Hadoop做比较，但是Hadoop明显是更复杂的框架（包含计算、存储、调度（YARN）），而Spark是一个纯计算工具（分布式）。

- 其次，Hadoop主要处理的是海量数据批处理，Spark主要用于批处理和流数据处理（更广泛）
- Hadoop是基于磁盘的迭代计算；Spark是内存迭代的交互式计算方式（RDD的方案）
- Hadoop的任务是以进程方式维护的，Sparl的任务是以线程方式维护的，启动快，可批量创建提高并行能力

其实精确地说是Spark和Hadoop里面的MapReduce做对比。（这才是一个层级，都是计算框架）

主要的不同就是：

1. Spark是基于内存的计算，MapReduce是基于磁盘的计算
2. Spark提供了其他的模块来完成其他场景的需求，比如Streaming模块可以让用户去写流处理的程序，MLib模块去写ML的程序等

> 一个极端的例子：对于一个小数据集，如果数据能完全有内存承接上，那么Spark会比MapReduce快100倍。正常情况下，Spark也会比MapReduce快2-3倍

相同点：

1. 在MapReduce上有类似Hive的方案去让用户能够去写SQL，在Spark的生态里，也有Spark SQL来完成类似操作

## 2.4 Redis

这个问题其实在1.1节已经给出了Redis是基于内存的存储方案，而Spark是基于内存的计算框架。他们惟一的共同点就是都是基于内存处理的。

> 查漏补缺：请问Redis和MemCached的区别？（一句话就是Redis更加成体系，但是由于其体量大，会牺牲一些速度）
>
> Redis是一个开源的内存数据结构存储，用作数据库，缓存和消息代理；Memcached是一个免费的开源高性能分布式内存对象缓存系统，它通过减少数据库负载来加速动态Web应用程序。
>
> 1. Redis支持字符串，散列，列表，集合，有序集，位图，超级日志和空间索引；而Memcached支持字符串和整数
> 2. Memcached的读写速度高于Redis
> 3. Memcached不支持复制。而，Redis支持主从复制，允许从属Redis服务器成为主服务器的精确副本



# 总结：

我们常常习惯于在一个给定的问题下挖的越来越深，不断优化；但是我们也要清楚这个问题是怎么产生的？多做对比，多横向比较，思考它为什么这么设计，有没有共同的出发点？有没有差异化的地方？

有的时候，一个好的问题定义，是要比一个解决方案更重要的。
