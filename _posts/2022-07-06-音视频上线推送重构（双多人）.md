---
layout:     post
title:     音视频上线推送重构
subtitle:   （双/多人）音视频
date:       2022-07-06
author:     ldf
header-img: img/post-bg-vedio.jpg
catalog: true
tags:
    - 开发技巧
    - 音视频项目重构
---


# 音视频上线推送重构（双/多人）

> 2022-06-30第一次讨论
>
> 2022-07-06方案形成
>
> 核心：
>
> 1. 读登录信息
> 1. 读存储
> 1. push给用户——其实就是组包之后，发送给msgAgent（它会负责发给不同用户）

# 一、原代码逻辑

## 1、双人上线推送逻辑：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/tapd_20398892_1550127806_85.jpg)

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220705213505.png)

## 2、多人上线推送逻辑：



![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/tapd_20398892_1551692433_34.jpg)









![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220705213429.png)



# 二、原代码问题分析

## 1、双人上线推送

### 1.1 目前存在的问题：

![双人上线推送](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220706102136.png)

1. push服务模块对策略模块的依赖。需要额外一个线程InfoSync以心跳的形式从策略机读取进房/退房/房间状态信息
2. 由于所有的信息都没有落盘，现在用的SHM来维护的room_list,uin_list;并且用hash来维护这两个之间的关系，使得程序很复杂，并且大量的数据放在内存中，也会拖慢程序

### 1.2 未来优化点(TODOLIST)

1. 只能双机运行（两层含义，不能扩（写死），也不能缩（分了奇偶处理））：两个实例的conf不一致，其中一个query.conf的SERVER_INDEX填1，另一个填2，来保证奇偶过滤。
2. 目前还未注册名字服务，上游都通过固定IP来寻址
3. 代码写死对方IP，发布有损（摘自https://git.woa.com/g_QQRTC/qq_av_push_srv，暂时不理解）
4. QueryInfo目前是从上线队列去for循环get数据，并且如果没get到，就sleep 2s；如果瞬时数据太多，服务器就会处理不过来（服务器会爆），同时服务器本身的get处理逻辑会让它对大规模数据不敏感（数据多了，还是一样的速度），考虑让上线队列服务器直接push数据给我们（数据多，那就处理得多，这个时候扩缩容感知就很明显）

5. QueryInfo.cpp中iphone的client_type需要随新手机更新，可以咨询rickycheng有没有方法可以获取终端类型（这个是type的问题，这个会一直变化，能不能给一个固定的list；因为我一直去读取新的这种代价也很大；或者，每次有client_type更新了，就notify我，我再去改代码，但是这个代价也很大）

6. 机器迁移需注意事项：以10.175.133.233迁9.148.160.46为例：目前的两台机器是 10.175.133.233，9.148.173.27，要将10.175.133.233迁移至9.148.160.46

   - 10.175.133.233的conf中query.conf的SERVER_INDEX填1, 9.148.173.27的conf中query.conf的SERVER_INDEX填2（一个过滤奇数UIN，一个过滤偶数UIN）

   - CC配置QQVideo.cmlb的server_sync_data_to_push_srv更改10.175.133.233为9.148.160.46

   - cmlb 5316 IP白名单限制（直接去织云cmlb路由上添加是无效的，需要联系dellwang来修改白名单IP）

   - QueryInfo.cpp写死了两台机器的IP 如果机器迁移的时候需要修改这两个IP

## 2、多人上线推送问题

> 这里梳理了一下代码，发现其实问题不大，主要它是spp框架做的，重构起来有难度。

1. 代码中很多无用的逻辑判断要check一下：比如企业QQ，QCall是没有的；与之对应的SharpAgent（带讨论组的手q），imAgent这些Agent服务器也要去掉
2. Grocery模块里面虽然已经改成了redis，但是它的监控数据上报还是用的Grocery的id，很奇怪，要确认
4. push模块的输入是登录信息，但是代码里面找不到（这里lamlz师兄给出建议是找那个分发器的sdk）



# 三、新方案的设计

> 背景：
>
> 1. 目前双人、多人上线推送服务较老，都部署在本地，未来需要迁移上云，需要项目重构；
> 2. 代码中的逻辑较老，很多模块没有及时随业务更新（e.g., 手机型号只更新到iphoneX）
> 3. 代码的设计不合理，依赖太多（e.g., 双人依赖策略机提供的信息，并且需要在SHM中维护这些信息，给机器带来压力）
> 4. 服务迁移或者服务扩缩容上很受限（e.g., 服务器ip写死）
> 5. 双人、多人上线推送服务从物理和逻辑上都是独立的，两者的代码框架也不一样（双C++，多SPP），但是推送服务的顶层逻辑应该是不区分双人/多人，实际部署状态和业务逻辑不符，需要把双人和多人整合在一起

## 1、目标

1. 整合双人和多人上线推送服务链路，以tRPC-Go框架实现功能；

2. 解决服务有状态写死ip的问题（多人的distributor；双人也是服务ip写死）

3. 尽量解耦，让PUSH服务直接从各个远端存储去读数据，不要再在本地维护大量的用户数据（变成无状态的）

   

## 2、方案思路

### 2.1 整体概述

**场景：**用户上线时，查看是否有正在进行的视频通话信息，如果有，则下发S2C消息提醒用户。（不区分双人/多人）

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220706120711.png)

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220706202159.png)

流程：

1. PUSH服务器收到上线队列服务器（可以是分发器）发送的登录信息后，根据UIN向OIDB拉取讨论组列表以及群列表
2. 返回组列表以及群列表，即roomInfoVec
3. 根据uin拉取roomid（这里的roomid只有双人的）；
   - 然后根据roomid(双人)/roomInfoVec(多人)查询流控信息
4. redis返回流控信息(流控ip，接口机实例，建房信息-是否有通话)
5. PUSH服务器再根据流控信息查询房间状态信息
6. 返回房间状态信息（是否仍在通话/视频等）
7. 最后，PUSH服务器组包、下发S2C消息（封包不一样）
   - 0X91(PC/手机)
   - 0x95(手机并且带讨论组)

### 2.2 一些疑惑点

> 其实主要是双人和多人推送一开始是切分导致的问题。

1. 最开始的目前双人和多人的登录信息是否一样？——指调用的服务接口，已经数据类型
2. 最后的S2C消息下发，组包是否一样？——目前看代码，双人和多人的实现是不一样的；
3. 目前为了保持服务变化小，多人的群/讨论组列表信息还是在OIDB上，然后（uin->roomid）这个双人的索引目前没有，是需要新维护的，放在redis是否合理？是否应该放在OIDB上来保持一致。（lamlm师兄指出这里访问OIDB是需要命令字，这种方案会增加复杂度）
4. 目前看到的多人上线推送代码是针对视频的推送，针对通话的是否有不一样？（需要hunting确认一下）
5. 读redis的操作需要找logen确认一下，数据结构是怎么样的；目前虽然多人推送中已经有读redis的操作，但是是否能和双人统一一下，也要考虑











**============================================分割=========================================**









## 一些疑惑（已解决-未整理-可忽略）：



![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220705164224.png)





  // 0x860 100个讨论组

  // 0x5d5 500个群

  // 0x911 1000个讨论组

// 0x860(拉取加入的讨论组列表)



1. 这些编号是什么意思？





`reportGroupNum((roomInfoVec.size() - disussCount) >> 1);     //群的数量  = (roomInfoVec.size() - disussCount)/2 ??`



我看roomInfoVec.size()是群和讨论组共享的，那在处理的时候为什么又要分开呢？





正常的PCQQ和MQQ是一个大逻辑，PCEIM和MEIM是另一个大逻辑；QCall是另一个大逻辑





为什么还要再请求一次Grocery？视频信息和语音信息的存放还不一样？之前不是说Grocery不用了吗？OIDB还用吗？

视频信息的依据是`for (vector<roomInfo>::iterator it = roomInfoVec.begin(); it != roomInfoVec.end();)`循环从oidb中的roomInfoVec



Grocery模块里面调用了redis？





为什么还要查流控服务？`InfoServe->流控中心：查询房间信息`？上面不是已经有了roomInfoVec吗？

流控里面还有`//查询Cent的数据`？



// pc 手机都下发0x95

上报房间数` reportRoomNum(roomInfoVec.size());`——>检查房间数是否过多`checkRoomNum(roomInfoVec);`——>下发s2c消息`getAddrRes = CONF::Instance()->getMsgAgentAddr(&msgAgentAddr);`

- pc 手机都下发0x95：`PackageBuilder::getInstance()->encode0x95Req(sendBuf, sizeof(sendBuf), sendPkgLen, pushUin, roomInfoVec, isPc, msgSeq)`进行组包

- 如果包体超长，也是break

- 发送之后，会有一个回包`sendRecvRes = mt_udpsendrcv(    &msgAgentAddr, (void *)sendBuf, sendPkgLen, recvBuf, recvPkgLen, CONF::Instance()->msgAgentTimeOut);`;也要检查一下，是不是正确的
- 检查回包之后，开始解包`decodeRes = PackageBuilder::getInstance()->decode0x95Rsp(recvBuf, recvPkgLen);`

- 手Q并且有讨论组,还需要下发0x91（针对手Q5.2之前的）：
  - 手Q上线发送到sharp失败？sharp是什么？这里对应着上面手q的msgAgent

//APPID_QCALL要另外发送到imMsg