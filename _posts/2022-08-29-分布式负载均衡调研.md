---
layout:     post
title:     分布式负载均衡调研
subtitle:   基础知识、扩展学习
date:       2022-08-29
author:     ldf
header-img: img/post-bg-distribute01.png
catalog: true
tags:
    - 分布式基础
    - 负载均衡
---


# 分布式负载均衡调研

> 目标背景：
>
> 1. 原始上线推送服务之前需要一个分发器，来对数据进行切割；原始distributor的分发逻辑是写死，按照1/2/3...讲用户信息区分好，然后由固定的机器去启动。——>现在要求能够动态地划分用户；动态有两方面的：
>    1. 分发的逻辑：支持分发器的扩缩容，并且扩缩容之后，各个机器之间的负载要均衡
>    2. 容灾的能力：如果A机器挂掉之后，A上的队列要能够由B/C机器去承担；A机器再次启动以后，要能够注册到一个”分发调度器“中，重新安排分发逻辑。
> 2. distributor既是接收器，又是分发器。每个distributor都有全亮的数据，我们这里还是着重设计分发器（即，consumer的角色）

## 1、目前考虑的几个问题（实时补充）

1. login_client的数据（包括控制模块）是否要和分发机器的逻辑一起上云？
2. 怎么判断分发机器挂掉了？——可以考虑加一个名字服务，里面有心跳机制。即，每一个分发机器怎么感知到其余节点的销毁和重建？（上游没有其余的服务）
3. login_client数据量和分发机器处理能力之间的关系：e.g.,login_client的数据量是10000人/s，但是每个分发机器的处理能力只有100人/s，那就要有大约10台机器。（如果是分布式锁，锁的竞争就会很频繁）
4. 当A机器（在处理字段1）挂掉之后，B机器被分派到接着处理字段1，应该从哪里开始？是重新开始？还是在A机器标注的offset开始？
   1. 这个问题即考虑——通知类型的数据，是否需要考虑做冗余？或者是可以直接丢弃？
5. A机器挂掉后，再次启动的时候，他应该怎么怎么处理？
   1. 稳定版：A再次启动，还是分配给他挂掉之前的字段，类似于固定了
   2. 动态版：A再次启动，就相当于是一个新的机器（类比一个新的进程过来竞争）——比较倾向于这个，因为既然都重启了，机器上就什么都没有了，再处理之前的分配也没有必要。

—————————2022-0620———————————

6. 当有热点用户出现是，即某一个字段的流量突增，应该怎么解决？某一台分发机器会维护一个大队列。（策略切换？那怎么判断我现在是出现了流量徒增的情况？）
7. 假如说要实时调整分配策略，那就要不断地重新计算权重，这种开销有没有必要？

## 2、现有方案

### 2.1、常见具体应用方案

> 分类依据：
>
> - 负载均衡所采用的设备对象（软/硬件负载均衡）
>
> - 应用的OSI网络层次（网络层次上的负载均衡）
> - 应用的地理结构（本地/全局负载均衡）

#### 2.1.1 软/硬件负载均衡

> 这个分类其实没什么好考虑的

- 软件负载均衡解决方案：是指在一台或多台服务器相应的操作系统上安装一个或多个附加软件来实现负载均衡，如DNS Load Balance，CheckPoint Firewall-1 ConnectControl，Keepalive+ipvs等，配置简单，使用灵活；但是每台服务器上安装额外的软件运行会消耗系统不定量的资源（消耗额外的资源）
- 硬件负载均衡解决方案：是直接在服务器和外部网络间安装负载均衡设备，这种设备通常是一个独立于系统的硬件，我们称之为负载均衡器。它是独立于系统，多种策略可选，性能提升大，但是价格贵。

#### 2.1.2 本地/全局负载均衡

> 这个也是一样，没有什么实际意义，只是一种思想，强调了服务器集群的位置。

- 本地负载均衡是指对本地的**服务器群**做负载均衡，有灵活多样的均衡策略把数据流量合理地分配给服务器群内的服务器共同负担。再给现有服务器扩充升级，也只是简单地增加一个新的服务器到服务群中，而不需改变现有网络结构、停止现有的服务。（支持扩缩容）
- 全局负载均衡是指对分别放置在不同的地理位置、有不同网络结构的服务器群间作负载均衡。要用于在一个多区域拥有自己服务器的站点，为了使全球用户只以一个IP地址或域名就能**访问到离自己最近的服务器，从而获得最快的访问速度**，也可用于子公司分散站点分布广的大公司通过Intranet（企业内部互联网）来达到资源统一合理分配的目的。

#### 2.1.3 OSI网络层次上的负载均衡

> 注意：其实高层次的负载均衡都是基于低层次的负载均衡技术的。

- 二层负载均衡（一般是用虚拟mac地址方式，外部对虚拟MAC地址请求，负载均衡接收后分配后端实际的MAC地址响应）
  - 让负债均衡服务器和业务服务器绑定同一个虚拟IP（即VIP），客户端直接通过这个VIP进行请求
  - 然后通过MAC物理地址，每台机器的MAC物理地址都不一样，当负载均衡服务器接收到请求之后，通过改写HTTP报文中以太网首部的MAC地址，按照某种算法将请求转发到目标机器上，实现负载均衡
  - **优点**是负载均衡服务器的压力会比较小，负载均衡服务器只负责请求的进入，不负责请求的响应（响应是有后端业务服务器直接响应给客户端），吞吐量会比较高。
  - 缺点：控制粒度低，只是去进行了转发
- 三层负载均衡（一般采用虚拟IP地址方式，外部对虚拟的ip地址请求，负载均衡接收后分配后端实际的IP地址响应）
  - 按照不同机器不同IP地址进行转发请求到不同的机器上。
  - 这种方式虽然比二层负载多了一层，**但是效果更差了**，请求的进出都要经过负载均衡服务器，会对其造成比较大的压力，性能也比二层负载均衡要差
- 四层负载均衡（在三次负载均衡的基础上，用 ip+port 接收请求，再转发到对应的机器），**主要分析IP层及TCP/UDP层**
  - 在三层负载均衡的基础上，通过发布三层的IP地址（VIP），**然后加四层的端口号，来决定哪些流量需要做负载均衡**
  - 对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。
- 七层负载均衡（根据虚拟的url或是IP，主机名接收请求，再转向相应的处理服务器）
  - 在四层负载均衡的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据**七层的URL（HTTP协议URI或Cookie信息）、浏览器类别、语言来决定是否要进行负载均衡**。常见例子有： haproxy，MySQL Proxy。
  - 举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。

这其中，最常见的是**四层**和**七层负载均衡**。我们可以理解一下他们直接的区别：

1. 四层负载均衡：以常见的TCP为例，负载均衡设备在接收到第一个来自客户端的SYN 请求时，即通过上述方式选择一个最佳的服务器，并对报文中目标IP地址进行修改(改为后端服务器IP），直接转发给该服务器。TCP的连接建立，即三次握手是客户端和服务器直接建立的，**负载均衡设备只是起到一个类似路由器的转发动作。**在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改（这就是说服务器要先回包给负载均衡设备，然后再由负载均衡设备回包给client）。
2. **七层负载均衡：**以常见的TCP为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，只能先代理最终的服务器和客户端建立连接(三次握手)后，才可能接受到客户端发送的真正应用层内容的报文，然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。**负载均衡设备在这种情况下，更类似于一个代理服务器**。负载均衡和前端的客户端以及后端的服务器会分别建立TCP连接。所以从这个技术原理上来看，**七层负载均衡明显的对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式**。（所以七层负载，要做的事情更多，一般大家也不这么用）

七层应用负载的好处，是使得整个网络更"智能化"：

1. **例如**访问一个网站的用户流量，可以通过七层的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术。
2. 从技术原理上，这种方式可以对客户端的请求和服务器的响应进行任意意义上的修改，很多在后台，例如Nginx或者Apache上部署的功能可以前移到负载均衡设备上，例如客户请求中的Header重写，服务器响应中的关键字过滤或者内容插入等功能。
3. **安全性**。网络中最常见的SYN Flood攻击，即黑客控制众多源客户端，使用虚假IP地址对同一目标发送SYN攻击，通常这种攻击会大量发送SYN报文，耗尽服务器上的相关资源，以达到Denial of Service(DoS)的目的。**在四层模式下这些SYN攻击都会被转发到后端的服务器上；而七层模式下这些SYN攻击自然在负载均衡设备上就截止。**比如，可以在七层层面设定多种策略，过滤特定报文，例如SQL Injection等应用层面的特定攻击手段。

**总结：**

1. **应用场景差别**：现在的**7层负载均衡**，主要还是着重于**应用HTTP协议**，所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。 **4层负载均衡**则对应其他TCP应用，例如IM即时通讯、**实时消息推送**等socket长连接系统。
2. **智能性**：七层负载均衡由于具备OIS七层的所有功能，所以在处理用户需求上能更加灵活；四层模型仅支持基于网络层的需求转发，不能修改用户请求的内容
3. **安全性**：七层负载均衡由于具有OSI模型的全部功能，能更容易抵御来自网络的攻击；四层模型从原理上讲，会直接将用户的请求转发给后端节点，无法直接抵御网络攻击。
4. **复杂度**：四层模型一般比较简单的架构，容易管理，容易定位问题；七层架构复杂，还会和四层模型混用，更加复杂
5. **效率比**：四层模型基于更底层的设置，通常效率更高，但应用范围有限；七层模型需要更多的资源损耗，在理论上讲比四层模型有更强的功能，现在的实现更多是基于http应用。

### 2.2 常见的负载均衡算法

> 常用的负载均衡算法分为两类：
>
> 1）一种是静态负载均衡；
>
> 2）一种是动态负载均衡。
>
> 业界用的：
>
> - Dubbo中有四种LoadBalance的方式：随机、轮询、最少活跃和一致哈希
> - 

#### 2.2.1 静态均衡算法

##### 1、 轮询法

将请求按顺序轮流地分配到每个节点上，不关心每个节点实际的连接数和当前的系统负载。

- 优点：简单高效，易于水平扩展

- 缺点：没有考虑机器的性能问题（集群性能瓶颈更多的会受性能差的服务器影响），并且不支持动态地扩充（静态的）。

##### 2、随机法

将请求随机分配到各个节点。由概率统计理论得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配，也就是轮询的结果。优缺点和轮询法一致

##### 3、源地址哈希法

源地址哈希的思想是根据客户端的IP地址，通过哈希函数计算得到一个数值，用该数值对服务器节点数进行取模，得到的结果便是要访问节点序号。采用源地址哈希法进行负载均衡，同一IP地址的客户端，当后端服务器列表不变时，它每次都会落到到同一台服务器进行访问。

- 优点：相同的IP每次落在同一个节点，可以人为干预客户端请求方向，例如灰度发布（前提是后端服务器集群是稳定）
- 缺点：
  - 如果某个节点出现故障，会导致这个节点上的客户端无法使用，无法保证高可用。
  - 当某一用户成为**热点用户**，那么会有巨大的流量涌向这个节点，导致冷热分布不均衡，无法有效利用起集群的性能。所以**当热点事件出现时，一般会将源地址哈希法切换成轮询法**。

##### 4、加权轮询法

> 其实也是一个静态的情况，只不过他有个手动调整的功能。其实不太好考虑，主要是不能实时地调整。基于估计的权重分配，也是太粗糙。（当然，可以考虑搞一个balance算法）

加权轮询算法要生成一个服务器序列，该序列中包含n个服务器。n是所有服务器的权重之和。在该序列中，每个服务器的出现的次数，等于其权重值。并且，生成的序列中，服务器的分布应该尽可能的均匀。比如序列{a, a, a, a, a, b, c}中，前五个请求都会分配给服务器a，这就是一种不均匀的分配方法，更好的序列应该是：{a, a, b, a, c, a, a}。

- 优点：可以将不同机器的性能问题纳入到考量范围，集群性能最优最大化
- 缺点：生产环境复杂多变，服务器抗压能力也无法精确估算，静态算法导致无法实时动态调整节点权重，只能粗糙优化。

##### 5、加权随机法

与加权轮询法一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重。不同的是，它是按照权重随机请求后端服务器，而非顺序。（类似于轮盘赌算法）

##### 6、键值范围法（目前用的）

根据键的范围进行负债，比如0到10万的用户请求走第一个节点服务器，10万到20万的用户请求走第二个节点服务器……以此类推。

- 优点：容易水平扩展，随着用户量增加，可以增加节点而不影响旧数据；

- 缺点：容易负债不均衡，比如新注册的用户活跃度高，旧用户活跃度低，那么压力就全在新增的服务节点上，旧服务节点性能浪费。而且也容易单点故障，无法满足高可用。

<font color='red'>**注：**</font>以上所提到的单点故障，都可以用**主从方式**来解决，从节点监听主节点心跳，当发现主节点死亡，从节点切换成主节点顶替上去。这里可以思考一个问题，怎么设计集群主从可以最大程度上降低成本



#### 2.2.2 动态均衡算法

##### 1、最小连接数法

根据每个节点当前的连接情况，动态地选取其中当前积压连接数最少的一个节点处理当前请求，尽可能地提高后端服务的利用效率，将请求合理地分流到每一台服务器。

- 优点：动态，根据节点状况实时变化；

- 缺点：提高了复杂度，每次连接断开需要进行计数；

- **实现：**将连接数的倒数当权重值。（能够很好地衡量权重）

##### 2、最快响应速度法

根据请求的响应时间，来动态调整每个节点的权重，将响应速度快的服务节点分配更多的请求，响应速度慢的服务节点分配更少的请求。

- 优点：动态，实时变化，控制的粒度更细，跟灵敏；
- 缺点：复杂度更高，每次需要计算请求的响应速度；
- **实现：**可以根据响应时间进行打分，计算权重。（但是这个响应时间打分不太现实，容易出现滞后，和不稳定的情况）

##### 3、观察模式法（就是个拼接整合）

观察者模式是综合了最小连接数和最快响应度，同时考量这两个指标数，进行一个权重的分配。

#### 2.2.3 业界常用的调度策略

##### 1、Dubbo中有四种loadbalance的方式：

- **随机：**先统计所有提供者上该接口方法的权重总和，然后对这个总和随机nextInt一下，看生成的随机数落到哪个段内，就调哪个提供者上的该服务。
- **轮询：**
  - 如果该接口方法的所有提供者的权重一样，则直接内部的序列计数器（sequences）+1然后对提供者的数量进行取模来决定调用哪个提供者上的服务；
  - 如果该接口方法的所有提供者的权重不一样，则找到其中最大的权重，然后将内部的权重计数器（weightSequences）+1并对该最大权重数取模，得到一个**权重基数**，然后再找出权重比**权重基数**大的提供者列表，最后通过内部的序列计数器（sequences）+1然后对这个提供者列表的数量进行取模，来轮询。（先筛选，再轮询）
    - 之所以使用**权重基数**的方式，是因为基数权重是会变的，它等于weightSequences这个原子类 对 最大权重取模的结果，并且每次都会进行自增。因此，**权重基数**会先变大后变小。在大于**权重基数**的提供者列表中进行轮询，那么权重大的提供者自然得到更多的轮询机会。
- **最少活跃**
  - **每个接口和接口方法都对应一个RpcStatus对象**，记录了他们的活跃数、失败数等等相关统计信息。（额外维护）；活跃数就像并发量降级中的计数器一样，开始调用时活跃数+1，调用结束时活跃数-1，所以活跃值越大，表明该提供者提供者的该接口方法耗时越长，而消费能力强的提供者接口往往活跃值很低。
  - **实现：**筛选出活跃数最低的提供者列表A，如果只有1个那就直接返回了 如果提供者列表A的所有提供者权重一样，那就随机选一个返回。 权重不一样，则计算出总权重，然后算出随机值，根据随机值在总权重哪一个位置，就返回对应的提供者。
  - 优势：Dubbo中的随机和轮询负载都没有考虑到提供者消费服务的能力，如果相差很大，“慢”提供者有可能被“快”提供给者给拖垮，其根本原因也是这两种负载均衡的加权因子考虑的不是服务耗时，而是服务器本身的能力。最少活跃的负载均衡就很巧妙的解决了此问题，而且它不是直接通过统计服务调用的耗时，而是采用统计调用差（活跃数）——但是这种方案就很依赖统计。
- **一致哈希**
  - 保证了同样的请求（参数）将会落到同一台提供者上，特别适用于有缓存的系统，这样缓存命中率会比较高
  - **实现：**生成虚拟节点，使用TreeMap保存（1对1），然后获取第一个节点进行调用

##### 2、北极星负载均衡策略

> 北极星负载均衡策略分2类：
>
> 1. 无状态负载均衡
> 2. 有状态负载均衡
>
> 北极星SDK**默认**负载均衡策略为权重随机，用户无需进行任何额外设置，即可使用权重随机负载均衡方式

**无状态负载均衡策略，**主要特点是每次负载均衡获取到的结果是由具体的负载均衡算法决定。目的是让负载**均匀**的分发到后端节点。

- 主要负载均衡策略包括：权重随机，权重轮询等

- 对于无状态的业务逻辑，为了保证后端节点能够均衡分配请求，此时应该选择权重随机负载均衡策略

**有状态负载均衡策略，**除了要达到让负载均衡分散到节点的目标以外，还需要实现将同一对象的请求分发到同一个节点（额外要保证数据分发到固定节点）。例如业务场景需要将同一个用户的全部请求发送到后端同一个节点处理的情况。

- 主要负载均衡策略是一致性hash

**北极星可选的负载均衡策略：**

1. 权重随机（weightedRandom）：利用区间算法，基于伪随机因子取模的方式选择对应服务实例（类似于轮盘赌）。对于**有状态的业务逻辑（比如通过用户ID或者请求ID进行hash分区的）**，为了保证同key请求能够持续命中同一个物理节点，此时应该选择一致性hash负载均衡策略；
2. 权重一致性hash（环算法）（ringHash）：基于ketama环算法，**每一个服务实例会按照权重分裂成若干个虚拟节点**（不同的实例水平分裂的虚拟节点数目不一样）。虚拟节点通过取hash值的方式，映射到长度为2^32的hash环中。  发起查询时，北极星会基于用户传入的hashKey，计算出具体的hash值，然后到环中寻找hash值刚刚好大于传入数据hash值的虚拟节点，并返回其对应的服务实例。
   - 异常：分布式缓存场景，总量很大需要分散到各机器缓存，通过一致性hash保证缓存命中率。目前北极星的一致性路由功能，只能select一个instance，因此存储是单点的，**挂了一台的时候**，虽然只rehash一部分数据，但也有损耗 。 如果能支持获取哈希ring上相邻的下一个物理节点，就可以同一key随机向两台机器请求，这样两台机器理论上就有同样缓存，**从而实现主从备份**。
3. 权重一致性hash（Maglev算法）（maglev）：首先为实例集创建一个长度为质数65537的向量表，算法根据节点的权重，将向量表进行填充，直到向量表全部被节点所占满。取节点时则根据用户传值的hashValue取模的方式，返回对应下标的节点（其实也是把实例分裂到表中不同的位置，并且个数也有多有少）。
   - **算法优点**：由于是直接取模寻址，因此算法性能相比ketama环要高（官方数据是在256K个插槽的情况下，性能是5X到10X的差距）。
   - **算法缺点**：当节点下线后，导致迁移的节点数量相比ketama环要多（官方数据为迁移节点数量是ketama的两倍）





## 3、基本设计

> 其实我们用的就是四层的负载均衡服务器，因为我们的流量请求（用户上线）基本上是一致的，不存在说对图片类的请求,或者对文字类的请求差别。
>
> 所以，基本的操作还是对全量用户数据按照ip级去切割。
>
> 这种场景，很类似分布式缓存场景——总量很大需要分散到各机器缓存。

基本想法：带虚拟节点的权重一致性hash（Ketama算法） + 权重随机（轮盘赌实现）

1.  定义服务器列表

2. 计算出服务器的hash值，添加到TreeMap中（TreeMap中元素是从小到大的顺序排列的；key是虚拟节点，value是真实节点）

   - 带虚拟节点后，也可以加上权重；权重越多，虚拟节点也越多；

3. 计算用户数据key的hash值

4. 在map中取出大于此hash值列表

   - 有；则在列表中按权重随机取

   - 无；则取出map的第1~N个节点，然后权重随机算法选取



