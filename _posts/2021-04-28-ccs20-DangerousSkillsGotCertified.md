---
layout:     post
title:      Dangerous Skills Got Certified:Measuring the Trustworthiness of Skill Certification in Voice Personal Assistant Platforms
subtitle:   CCS20-School of Computing, Clemson University, SC, USA
date:       2020-02-15
author:     bbq
header-img: img/post-bg-green-vpa.png
catalog: true
tags:
    - research
    - IoT
    - attacks
---
	


>authors: Long Cheng, Christin Wilson, Song Liao, Jeffrey Young, Daniel Dong, Hongxin Hu



![](RackMultipart20210428-4-uckc6v_html_9d53d30a06825381.png)

Abstract：

随着voice personal assistant（VPA）生态系统的出现，第三方开发者可以构建新的voice-apps1并发布到skills store，这大大扩展了VPA的功能。在新技能公开之前，该技能必须通过一个认证过程，该过程验证它是否符合必要的内容和隐私策略。技能认证的可信性对于平台提供商、开发人员和最终用户都非常重要。然而，对于违反策略的技能在VPA平台上获得认证和发布有多困难，我们知之甚少。在这项工作中，我们研究了Amazon-Alexa和Google-Assistant平台上技能认证的可信性，以回答三个关键问题：1）技能认证过程在捕捉第三方技能中的违反策略行为方面是否可信。2） 他们的技能库中是否存在违反策略的技能。3） VPA用户对技能认证和与VPA设备交互时易受攻击的使用行为有何看法？在长达15个月的时间里，我们精心设计并提交了234项Amazon Alexa技能和381项Google助手行为，这些行为故意违反VPA平台规定的内容和隐私政策。令人惊讶的是，我们成功地获得了234个（100%）违反策略的Alexa skills认证和148个（39%）违反策略的googleactions认证。我们的分析表明，当前技能库中存在违反策略的技能，因此用户（尤其是 **儿童** (因为儿童用的功能都比较娱乐性，所受防护较少；交互多而且随意)）在使用VPA服务时会面临风险。我们对203名参与者进行了一项用户研究，以了解用户对VPA平台的错误信任。不幸的是，领先的VPA平台的技能认证并没有满足用户的期望。

为了打击肆无忌惮的开发者，VPA平台已经实施了一系列第三方开发者必须遵守的政策要求[3–5，12]。亚马逊和谷歌都声明，如果某项技能违反了上述任何一项政策，该技能将被拒绝或暂停使用（见附录D）。技能提交到技能库后，需要通过认证/审核流程，然后才能向最终用户公开。

首先，VPA平台的分布式体系结构（技能运行在开发人员的服务器上）对可信的技能认证提出了挑战。由于技能的代码是在外部托管的，认证过程无法访问，因此使用静态代码分析来彻底探索技能的行为对于当前的VPA系统来说不是一种选择。其次，由于不同VPA平台定义的策略需求的多样性，缺乏有效的认证工具来检测恶意或有问题的技能。第三，amazonalexa和googleassistant平台都允许第三方开发人员随时更新技能代码，并且不需要重新认证。这可能导致代码更新攻击，恶意开发人员利用此功能在代码中添加违反策略的内容或侵犯隐私的问题，即使在技能获得认证之后也是如此。

宽松的认证将允许恶意或粗心的开发人员在技能库中发布危险的技能。我们很好奇目前的商店是否存在违反政策的技能。如果技能调用的名称存在冲突/歧义，我们将研究外部因素（如用户评论和评级）如何影响技能发现的结果。这可能会让敌对开发者增加恶意技能到达最终用户的机会。通过用户研究，我们旨在了解使用VPA服务的人们的使用习惯，以及他们对VPA平台的关注和期望。

# Contribution

•我们为234 Alexa技能精心编制并提交，这些技能故意违反VPA平台定义的55项内容和隐私政策。我们能让他们都得到认证。作为一项比较研究，我们还提交了381项违反谷歌行为的政策，其中148项行动得到认证，233项行动从未通过认证。虽然谷歌在基于我们的测量结果的认证过程中做得更好，但它也有潜在的可利用的缺陷，可能导致技能库2中提供恶意技能。

•为了确定当前商店（截至2020年7月）现有违反政策的技能，我们手动测试了儿童类别下的755项Alexa技能，并确定31项违反政策和34项问题技能。在Google Assistant平台中，家庭/儿童类别下的行动只有114项。我们测试了所有的政策，发现了一个违反政策的行为。

•我们分析了认证后的漏洞，并调查了如何在技能商店中认证和发布后增加接触最终用户的危险技能的机会。我们以自动化的方式测试了Alexa的技能发现过程，并揭示了对手可能通过发布虚假评论和评级来操纵技能发现机制，以增加恶意技能到达最终用户的机会。这与宽松的认证流程相结合，使得每天的VPA用户面临着高风险。

•为了了解由于技能认证缺乏可信度而造成的风险和后果，我们使用Amazon Mechanical Turk（MTurk）众包平台，与203名参与者进行了一项用户研究。我们的调查结果表明，用户对VPA平台的信任是错位的，领先的VPA平台没有达到用户的期望。

# 2 BACKGROUND &amp; THREAT MODEL

![](RackMultipart20210428-4-uckc6v_html_c38d5e07a9b7a8d6.png)

用户可以通过两种方式启用新的Alexa技能。第一种方法是通过智能手机上的Alexa companion应用程序或亚马逊网站上的Alexa skills商店启用它。用户可以浏览商店寻找新技能或使用关键字搜索特定技能。该技能的列表包括诸如技能描述、隐私策略和开发者提供的使用条款，以及该技能收集到的用户评论和评分等详细信息。另一种方法是通过语音启用技能，用户可以说&quot;enable{skill name}&quot;。用户也可以直接说&quot;Open{skill name}&quot;来调用一个新的技能，在这种情况下，Alexa将首先启用该技能，然后打开它。通过使用此方法，用户无法决定启用哪种技能，除非他/她给出了确切的技能名称。即使给出了确切的名称，由于Alexa中的重复命名（即，多个技能具有相同的名称），也会根据多个因素（如技能的流行程度[8]）从一组技能候选人中选择一个技能（详情见第5.3节）。Google不要求用户在使用前启用一个动作，用户可以直接说&quot;Talk to{action name}&quot;来调用一个动作。在两个VPA平台上使用此方法的问题是，用户看不到启用的技能的详细信息。

# 3 RELATEDWORK

有许多研究表明，用户关心VPA设备的安全/隐私[20、21、23、28、31、32、39、40、49、53]。Lau等人透露，隐私问题可能是新用户的主要阻碍因素[36]。Edu等人[29]对VPA生态系统中常见的攻击向量（如弱认证、弱授权、数据推断）及其对策进行了分类。由于用户对VPA设备缺乏适当的身份验证，对手可以生成人类无法理解或听不见的隐藏语音命令[24、25、42、43、45、46、48、50]，从而危害语音识别系统。相应的防御机制包括连续认证[30]、取消不需要的基带信号[50]、将磁性变化与语音命令相关联[26]以及基于用户存在的访问控制[37]。

另一方面，VPA生态系统的开放性给用户带来了新的身份验证挑战：恶意第三方技术可能会冒充合法技术。Kumar等人[35]提出了语音蹲起攻击，它利用由于语言歧义导致的语音解释错误，将用户秘密地路由到恶意技能。其思想是，考虑到语音识别系统中频繁发生且可预测的语音解释错误（例如，&quot;coal&quot;到&quot;call&quot;），对手构建了一个恶意技能，其名称与良性技能的名称混淆。由于误解，当收到对目标技能的请求时，Alexa很可能触发下蹲技能。除了利用技能调用名称的语音相似性，释义调用名称（&quot;capital one&quot; vs &quot;capital one please&quot;）也可以劫持受害者技能的品牌[51]。这是因为最长的字符串匹配用于在VPA平台中查找请求的技能。Zhang等人[51]也发现了伪装攻击（masquerading attack）。例如，恶意技能通过在其响应中提供&quot;再见Goodbye&quot;来伪造其终止，同时保持会话活动以窃听用户的私人会话。LipFuzzer[52]是一个基于黑盒突变的模糊工具，可以系统地发现现有VPA平台中容易误解的语音命令。Mitev等人[41]提出了一种介于用户和良性技能之间的中间人攻击，对手可以修改良性技能的任意响应。Shezan等人[44]开发了一种自然语言处理工具，用于分析敏感语音命令的安全性和隐私含义。

Hu等人[33]进行了初步的案例研究，以检验Amazon Alexa和Google Assistant平台是否需要第三方应用服务器来验证Alexa/Google云及其查询。作者发现amazonalexa需要技能来执行云认证，但是在第三方开发者身上执行它的能力很差。Liao等人[38]进行了数据分析，以衡量VPA平台中开发人员提供的隐私政策的有效性。SkillExplorer[19]是一个动态测试工具，用于探索技能行为并检测技能中的隐私侵犯。作者测试了28904项Amazon技能和1897项Google操作，发现超过1000项技能要求用户在不遵循开发者规范的情况下提供个人信息。尽管最近取得了一些进展，但现有的研究主要集中在解决用户和VPA设备的语音识别系统之间开放的语音/声学接口的挑战。虽然现有研究报告了危险技能（例如，声音蹲起或伪装攻击）[2、16、35、51]，但这些危险技能在认证阶段并不一定违反VPA的政策要求。对于违反策略的技能（例如恶意内容）获得VPA平台的认证和发布的难度，我们知之甚少。本文的研究重点和方法不同于现有的研究成果。我们的目标是全面评估领先VPA平台中技能认证的可信性，并描述第三方技能开发人员和VPA平台之间的威胁。

# 4 MEASURING THE TRUSTWORTHINESS OF SKILL CERTIFICATION

![](RackMultipart20210428-4-uckc6v_html_32ab27b3206cde0e.png)

## 模拟技能认证——通过刻意违反而触发违反策略

技能认证过程本质上是一个黑匣子，因为我们无法访问它的内部实现。为了测试可信性，我们设计了故意违反VPA平台定义的特定策略的违反策略的技能，并检查它是否得到认证并发布到商店。图2展示了我们的实验设置的高级视图。我们对儿童导向技能的政策执行特别感兴趣。与成人相比，儿童更容易受到此类潜在威胁，针对他们的技能需要VPA平台制定更严格的政策。亚马逊已经定义了内容政策指南[3]，分为14个主要部分和7个具体的隐私要求[4]。我们使用亚马逊的政策分类作为参考，并考虑到谷歌的政策要求[12]， **编制了**** 55 ****项政策的列表，如附录中的表**** 7 ****和表**** 8 ****所示。**

## Skill Development and Submission

在技能发展方面，亚马逊和谷歌在结构上都很相似。我们列出了它们之间的三个显著区别。

1） 亚马逊要求开发者只有在技能明确声明收集个人数据时才提供隐私政策。对于其他技能，不强制提供。而谷歌则要求每一项行动都提供提交时的隐私政策。

2） Amazon允许使用重复的技能名称（即多个技能具有相同或语音相似的名称）。但是，Google限制所有操作必须有一个语音上唯一的名称。

3）Amazon对每个开发人员帐户可以部署的技能数量没有限制。而Google只允许每个开发者帐户部署多达12个action项目。

因为我们的目标是评估向商店发布违反策略的技能的难度，所以我们开始测试facts技能，这些技能基本上只有一个自定义意图。这些技能在打开会话并结束会话时给出单个响应。技能中没有扩展的分支或控制流。我们开发的另一种技能是故事技巧，它要求在第一个欢迎词中提供个人信息。这样做是为了确保审核工具（或认证团队成员）能够在技能打开时轻松捕获违反策略的响应，并且不必采取额外的步骤来达到该响应。每种技能都有数量有限的正常响应和违反策略的响应（例如，有毒内容或广告）。

我们的实验于2019年4月至2020年7月进行。对于Amazon-Alexa平台，我们精心编制了234项认证技能，包括115项一般类别技能和119项儿童技能。我们的实验使用了11个Amazon开发者帐户和2个AWS（amazonwebservice）帐户。31个技能托管在我们的AWS帐户上，203个技能使用Alexa托管的后端。对于每项技能分发部分的隐私与合规表，我们对所问问题的回答各不相同，例如&quot;此技能是否收集用户的个人信息？&quot;而且&quot;这项技能是针对13岁以下的儿童，还是针对13岁以下的儿童？&quot;测试所有可能配置的效果。最初，提交的技能来自不同的开发者帐户，以逃避任何可疑活动的检测。后来，我们把重点从单一的开发人员帐户转移到了发布技能上，以故意引起怀疑。重新提交一次发布的技能，以检查认证的一致性，这些技能使用相同的模板、意图名称、槽名称等。对于googleassistant平台，我们提交了381个违反策略的操作（201个普通类操作和180个儿童操作）来观察他们是否能够通过认证。

**Policy Violations in Our Experiments**. Each testing skill violated one of the 55 policies listed in Table 7 and Table 8.We briefly summarize the policy violations in our crafted skills.

在技能发展方面，亚马逊和谷歌在结构上都很相似。我们列出了它们之间的三个显著区别。1） 亚马逊要求开发者只有在技能明确声明收集个人数据时才提供隐私政策。对于其他技能，不强制提供。而谷歌则要求每一项行动都提供提交时的隐私政策。2） Amazon允许使用重复的技能名称（即多个技能具有相同或语音相似的名称）。但是，Google限制所有操作必须有一个语音上唯一的名称。3） Amazon对每个开发人员帐户可以部署的技能数量没有限制。而Google只允许每个开发者帐户部署多达12个action项目。

- Overview of the Measurement Results

![](RackMultipart20210428-4-uckc6v_html_2592a39160f9caab.png)

Overall, our measurement results show that the current VPA platforms (in particular the Amazon Alexa platform) have not strictly enforced policy requirements in their skill certification processes, despite claimed to the contrary.

## 4.3 Key Observations

4.3.1 Inconsistency in checking.

平台对同样的技能可能会有不同的反馈回复（虽然都是被拒绝，但是拒绝的原因却不一致）

4.3.2 Limited voice checking.

我们观察到认证过程只测试了有限的次数（通常少于三次语音应答）。因此，违反策略的内容很容易从认证团队的测试范围中隐藏。

这些技能基于相同的交互模型（即模板），无论使用哪种开发人员帐户，前端的意图名称和后端的变量名称都是相同的。但认证过程忽视了这一点，或者没有引起认证小组的注意，这表明缺乏一种有效的自动认证工具，可以查明克隆技能或可疑批量技能等问题。所有这些都导致了这样的结论：测试只通过语音响应和提供的分发页面来完成，而不是通过检查技能的交互模型或后端代码来完成。技能测试是从用户的角度进行的，检查是基于用户可获得的技能的信息和访问权限进行的。

4.3.3 Overtrust placed on developers and negligence during certification.

开发人员提交的隐私与合规表在认证过程中起着重要作用。如果开发人员指定该技能不违反任何策略（但实际上确实违反了），则该技能很有可能获得认证。如果开发人员以指定违反任何策略的方式回答问题，那么提交时该技能将被拒绝。

# 5 ANALYZING POST-CERTIFICATION RISKS

5.1 CanWe Find Policy-Violating Kids Skills?

在表2中，我们提供了儿童技能的高水平统计。对于亚马逊Alexa平台，截至2020年7月，儿童类共有3401项技能，其中880项至少有一次评审或评级。我们注意到461项技能有隐私政策，其中37项技能的链接或链接已中断，或链接到不包含隐私政策的网页。Google Assistant平台的家庭/儿童类别中只有114项行动可用，其中80项行动至少有一次审查或评级。所有的操作都提供了隐私策略。与Alexa技能商店中可用的技能相比，Google可用操作的数量要少得多。这种行动数量少，隐私政策没有被打破，这可能是谷歌更严格的认证体系的结果。注意，我们提交的技能/行动不计入表2。在附录G中，我们还列出了AmazonAlexa技能商店的一些代表性关键用户评论。

![](RackMultipart20210428-4-uckc6v_html_2613590c1b8b62e5.png)

由于用户不容易获得技能代码（认证过程中也无法获得），因此使用静态代码分析来探索技能行为不是一种选择。为了确定现有的违反政策的技能，我们需要与他们互动，收集他们的回答（例如，陈述或问题），然后检查这些回答是否违反任何政策要求。考虑到手动测试技能非常耗时（我们将在第7节中讨论技能的动态分析），我们使用隐私策略和用户评级作为过滤器来缩小我们测试的技能范围。这是因为Amazon Alexa只对收集个人信息的技能要求强制性隐私政策。因此，我们假设提供隐私政策的技能可能是收集信息。另一方面，低评级用户评论可能反映出现有技能中潜在的违反政策的问题。我们使用Alexa开发人员控制台模拟器检查了755个Alexa技能，这些技能要么有隐私政策（461个技能），要么评分较低（即294个技能的星级低于3星），并记录了所有的回答。对于Google助手平台，我们手动测试了家庭/孩子中列出的所有114个动作类别。我们开发了一个简单的基于自然语言处理（NLP）的策略违反检测器，该检测器通过分析技能的输出来检测策略违反技能行为。我们定义不同的关键字集以检测违反不同策略的情况。我们专注于检测个人数据收集、非法和暴力内容、广告和促销活动，这些活动会引导用户参与技能之外的内容。我们使用SpaCy库[9]获取技能反应中的名词和动词。该检测基于技能响应和策略特定关键字之间短语的相似性比较。

![](RackMultipart20210428-4-uckc6v_html_ed27a57bfba21f6d.png)

COPPA规则要求开发者提供一份隐私政策，包括收集个人信息的所有运营商的名单、收集的个人信息的描述、如何使用以及父母权利的描述。然而，我们发现5个Alexa技巧（&quot;猜动物的声音&quot;、&quot;说请&quot;、&quot;获取实际日期的信息&quot;、&quot;我们做朋友吧&quot;和表3中突出显示的&quot;Ready Freddy&quot;）在不提供隐私政策的情况下收集个人数据。此外，在向孩子收集个人信息之前，必须直接通知家长，并应获得他们的核实同意。根据COPPA的规定，父母还必须能够在征得他们同意的情况下审查从他们的孩子那里收集的信息，并有权删除这些信息。此外，COPPA还要求向家长提供开发商的联系方式。亚马逊在注册开发者帐户时从开发者那里收集的信息没有经过验证，很容易被伪造。正如我们的实验所证明的，开发人员可以证明收集个人信息的技能，而不满足或遵守这些要求中的任何一项，从而违反了COPPA法规。

## 5.2 Code Update Vulnerabilities

当技能的后端代码发生更改时，amazonalexa和googleassistant平台都不需要重新认证。恶意开发人员可能利用此功能以认证技能任意更改响应（例如，使用户暴露于不适当的内容）或问题（例如，询问用户的个人信息）的内容，我们称之为代码更新漏洞。即使严格执行了策略要求，用户在技能认证后也容易受到代码更新攻击。虽然早期的研究提到了代码更新漏洞[16，51]，但利用这些漏洞来打造一种隐私入侵技能，成功地捕获和存储后端的敏感信息并不是那么简单。这是因为，即使攻击者可以修改某个技能的后端代码来欺骗用户提供敏感数据，该技能也可能无法捕获后端的敏感数据，而VPA系统中的自然语言理解模块[52]可以过滤掉这些敏感数据。对于收集特定类型数据的技能，它必须在认证阶段之前已经具备捕获特定类型数据的能力。只有当用户所说的内容与开发人员指定的示例语句相匹配时，开发人员才能掌握该用户所说的内容（以文本格式）。所有其他不匹配的响应都不会发送到技能的后端。例如，为了收集用户的地址信息，开发人员必须添加一个带有类型slot的示例语句亚马逊邮递公司一个预先定义的意图。因此，恶意开发人员必须使用适当的训练数据（即开发人员提供的自定义槽值）仔细地建模自定义槽，以便捕获在认证过程之前未声明的特定类型的数据。例如，收集密码需要自定义槽的训练数据，包括各种字母、数字和符号组合，以便完美地接受用户响应。然而，具有不同类型时隙值的自定义时隙对于秘密数据收集是可疑的。

在我们的实验中，由于宽松的技能认证，我们能够发布一个Alexa kids技能，其中有一个定制的槽，可以接受多种类型的值（例如，名/姓和城市/街道名）。在提交时，我们的技能只要求名字，这是可以接受的VPA的隐私要求，即使认证过程是正确执行政策要求。我们的目标是衡量认证团队是否能够检测到在一项技能中接受多种类型敏感值（例如，名/姓和城市/街道名）的可疑定制槽。我们能够毫不费力地获得这项技能的认证。认证之后，我们将问题改为询问其他几种类型的个人信息，这些信息可能会构建一个完整的用户配置文件。我们能够请求和接收用户的全名，并将个人信息保存到数据库中。为确保研究伦理，我们在内部测试后立即删除所有收集的数据。Alexa开发者控制台中的skill analytics数据证实没有实际用户受到影响。与Alexa skills类似，代码更新攻击也可以在Google actions上执行。操作的后端可以在认证后更新，并且不需要重新认证。

## 5.3 Is It Possible To Increase the Chance of Dangerous Skills Reaching End Users?

但是，在应用商店中发布违反策略的技能只会为恶意开发人员提供发动攻击的可能性。此类攻击的成功主要取决于在VPA平台上实现的技能发现/推荐算法。例如，如果VPA平台推荐/调用voicesquatting [35，51]来满足用户的请求，那么它只能模拟合法的语音占用技能。如果技能调用的名称存在冲突/歧义，我们将研究不同的因素（例如，用户评论和评级）如何影响技能发现的结果。由于googleassistant平台不允许重复命名，所以我们只研究amazonalexa的技能发现过程。如果外部因素和参数可以影响技能发现过程，那么对抗性开发人员可能会操纵这些因素（例如，发布虚假评论），以增加恶意技能到达最终用户的机会。

![](RackMultipart20210428-4-uckc6v_html_c0efb451f24c53cc.png)

我们首先选择了10组第三方Alexa技能，每组都有相同的调用名称，并获得了他们的技能id、用户评论、星级评定和开发人员信息。我们的目的是观察amazonalexa在调用一个与其他技能同名的技能时，是否经常发现某个特定技能。我们使用SeleniumWebDriver（SWD）[14]开发了一个工具，用于在Alexa开发人员控制台中自动化技能发现测试。对于每组测试，我们从一个新格式化的Alexa设备开始，该设备没有启用任何技能。SWD被设置为通过自动输入开发人员凭证进入Alexa开发人员控制台，然后启用设备日志记录。接下来，使用SWD将技能调用输入到Alexa模拟器中。设备日志捕获了Alexa模拟器和调用的技能之间的所有通信。然后，我们从设备日志中检索技能ID。有趣的是，从我们遇到的结果来看，启用的技能是商店中评论/评分最多的技能。然后，社署被设置为禁用该技能。下一次尝试打开具有调用名称的技能时，启用了具有相同调用名称的新技能。这项技能在根据评分/评论的数量进行排名时排名第二。对于在存储上发布了两个以上技能的调用名，我们重复执行上述过程。我们登录了另外两个亚马逊账户，重复了同样的实验。我们发现在每个帐户上启用的技能和它们的顺序是完全相同的。

我们的测量结果表明：i）优先考虑设备上已经启用的技能；ii）技能发现可能是用户评论和评分的功能（但很难使准确的技能排名算法神秘化，该算法考虑了许多因素，包括技能的质量和使用[8]）。第二个优先事项是拥有最多评论/评级的技能。用户对启用哪种技能的意见既不要求，也没有向用户适当告知特定技能，而不是在启用时仅说明技能的&quot;公共名称&quot;。在技能库中认证并发布恶意技能（例如语音蹲置技能）后，攻击者可以发布假正面评论和评级，以提高技能劫持的成功率。由于很少有技能具有大量的评论，在许多情况下，攻击者需要发布的所需假评论的数量可能会大大减少。此外，用户也不知道他们在使用哪种技能，除非他们检查他们移动设备上的Alexa同伴应用程序。我们认识到亚马逊的欺诈检测机制可能会引起虚假评论。然而，长期以来防止使用统计数据的操纵/膨胀问题使得敌对开发人员有可能操纵技能发现[47]。例如，亚马逊指出，尽管亚马逊做出了努力，但存在旨在逃避检测的虚假审查，审查滥用现象仍在继续[10]。认证不好，再加上潜在的可操作技能发现，VPA用户面临着高风险。（就是通过刷评分，可以把恶意程序刷上去）

# 6 USER STUDY

Different from existing user studies that focus on understanding users&#39; security and privacy attitudes towards VPA [21, 23, 27, 32, 39], our study investigates user behaviour about exploring new skills, when encountering something inappropriate, and their trust to VPA platforms.

表5显示了AmazonAlexa和GoogleAssistant用户的调查结果。我们想知道用户选择哪种方法在他们的设备上探索和启用新技能。42%的用户表示，他们通过语音启用技能，而另58%的用户浏览商店。（这只针对Amazon，因为Google上面的技能并不需要提前安装就可以直接调用了）

当遇到不合适的事情时，69%的亚马逊用户和72%的谷歌用户声称会在技能商店中对技能进行评估。这一结果表明，用户的负面反馈可能有助于我们识别现有的违反策略技能（在附录G中，我们通过初步的用户审查分析了解用户的共同关注点）。当被问及是否信任亚马逊和谷歌来检测和过滤不适当的内容时，71%的亚马逊用户和90%的谷歌用户回答肯定是或可能是。

来自这两个小组的大约95%的参与者认为，所有的技能和行动都必须经过亚马逊和谷歌的彻底测试和检查。91%的Alexa用户和94%的Google用户希望在技能/动作发布到商店之前完成认证，而不是在技能/动作被认证并向公众提供之后。

当被问及参与者是否知道他们目前使用的技能和行动能够从他们那里收集到什么样的个人数据时，只有大约10%的用户回答肯定是。这在一定程度上意味着这些技能在告知用户他们的数据收集和存储实践方面做得不好。两组中约50%的用户认为隐私政策只关注于防止诉讼，而不是为用户提供清晰的图片和有用的信息。当被问及Alexa用户是否会在Alexa智能手机应用程序上查看自己的活动时，45%的参与者声称至少有一半的时间会查看。

此外，我们还注意到，我们从设备收到的响应与Alexa智能手机应用程序的&quot;活动&quot;选项卡中记录的响应不一致。这可能是一个需要纠正的错误（我们已经向Amazon安全团队报告了这个错误）。在这个阶段，它不符合活动选项卡的目的。

![](RackMultipart20210428-4-uckc6v_html_bfc4cd70d92830f8.png)

## 6.2.2 Parents specific questions.

表6列出了调查结果。我们组中61%的亚马逊用户提供了使用儿童技能的许可，25%的用户不确定是否提供了，谷歌用户的比例也差不多。当被问及是否使用亚马逊自由时间订阅或谷歌家庭链接时，两组中只有大约50%的人都表示同意。虽然这样的机制可以让家长更好地控制孩子如何与VPA设备交互，但使用付费订阅的用户数量较少。只有33%的Alexa用户和48%的Google用户在为孩子们启用技能之前，至少有一半的时间会对他们进行广泛的技能测试。这显示了用户对亚马逊和谷歌的信任。另外，从用户的角度来看，在启用之前对每项技能进行广泛的测试是不可行的，应该由技能认证团队在发布之前进行测试。在我们的调查中，33%的Alexa用户和58%的Google用户为他们的孩子配备了专用的VPA设备。这表明当孩子们使用这个设备时，父母可能不会和孩子们在一起监视他们之间的互动。我们注意到，大多数用户对针对儿童的促销和广告感到不舒服。此外，当被要求回顾我们在提交的技能中添加的4个实际回答的舒适度时（表6中没有显示），两组中最多只有20%的用户对每个回答都给他们的孩子感到至少有点舒适。这表明，亚马逊和谷歌认证的技能中的回答确实违反了政策，对儿童造成了困扰。

![](RackMultipart20210428-4-uckc6v_html_eb5f73cfb7783c95.png)

# 7 DISCUSSION

## 7.1 Why Lenient Skill Certification?（放宽技能限制）

Alexa的技能商店里有超过10万种Alexa技能，但仔细观察发现，绝大多数技能都没有使用过。对技能认证过程的宽容鼓励开发人员开发出许多技能，优先考虑数量而不是质量。这一动机的进一步证据可以从与googleassistant开发人员控制台的比较中得出。谷歌限制开发者最多12个行动项目，除非开发者明确要求增加限制。相比之下，AmazonAlexa开发者帐户没有这样的限制。这些公司还制定了奖励开发多种技能的开发人员的计划，随着开发更多技能，奖励也会增加。虽然亚马逊和谷歌可能都没有恶意，但将各自技能库的增长置于技能质量之上的结果，导致技能认证过程没有充分检查提交的技能是否违反政策。

## 7.2 Mitigation Suggestions

- Training for the certification team.各种技能认证和拒绝的不一致性使我们相信技能认证主要依赖于手工测试。而技能认证团队可能不完全了解VPA平台所强加的各种政策要求和指导方针。这尤其是因为我们能够发布第一次答复中违反政策的技能。从我们收到的Alexa技能认证反馈时间（如附录F图9所示），证明似乎是由美国以外的团队完成的。在这种情况下，团队必须了解VPA服务目标所在地的相关政策和法规，例如：。，COPPA[22]和通用数据保护条例（欧盟GDPR）[6]。应向认证团队提供对政策指导方针的更好理解和适当培训，以防止违反政策的技能流入技能商店。
- In-depth checking during the skill certification. 鉴于技能的后端代码不能被获取，认证团队应在技能认证期间进行深入检查。我们的测量结果（特别是针对Amazon-Alexa平台）表明，技能的验证主要是通过手动方式和非常有限的基于语音响应的测试来完成的，而没有检查技能的交互模型（即技能的前端）。除了增加与测试技能的语音交互次数外，认证团队不应简单地信任开发者提供的信息，需要注意1）来自同一开发者帐户的批量技能提交；2）描述和隐私策略不一致；3） 可以接受多种类型敏感数据的自定义槽；以及4）skill交互模型上的剽窃。VPA平台还可以从现有的技能提交（包括认证的和拒绝的）中构建前端交互模型的数据集，提取特征并训练机器学习模型来自动识别可疑的技能提交。
- Deploying automated skill testing tools for policy violation detection. 建立一个可靠的、可扩展的基于语音的测试工具是非常重要的。最近，SkillExplorer[19]被提出，它由一组基于语法的规则驱动，以探索技能的交互行为。然而，它主要侧重于确定收集私人信息的技能，而不评估技能在更广泛的背景下是否符合各种政策（例如，表7中列出的内容政策）。随着技能变得更加智能化，SkillExplorer中基于硬编码语法的规则在处理来自技能的不同响应时可能无法伸缩。在第5.1节中，我们使用了初步的基于NLP的检测器来自动识别违反策略的儿童技能。作为我们未来的工作，我们计划开发一个基于公开对话数据集的数据驱动的动态分析工具[1]，以了解来自技能的各种问题并生成相应的响应，并扩展策略违反检测器的基本设计。
- Enforcing skill behavior integrity throughout the skill lifecycle. 我们的实验表明，开发人员可以在两个VPA平台上任意更改技能的功能。当一个技能选择Alexa托管的后端时，在该技能被审查时，后端代码将被阻止编辑。但技能认证后，它就被解锁了。为了防止内容更改攻击，每当开发人员更改前端或后端时，都应该执行重新认证过程。这是一个可行的解决方案，尽管它可能会增加技能发布延迟。在分析Alexa kids技能的过程中，我们也遇到了许多失败的技能（详情见附录C的表9）。如果技能被破坏或违反任何政策，应定期检查技能，并将其从技能库中移除。为了从根本上防止提交时违反策略的技能，VPA平台提供商可能需要要求技能开发人员提供查看其后端代码的权限。在这种情况下，可以在提交技能时执行代码分析，这可以极大地提高技能证书的可信度。

## 7.3 Limitation

首先，虽然我们在Amazon-Alexa和Google-Assistant平台上都做了大量工作来衡量技能认证过程的可信度，但我们的对抗性测试主要集中在技能中违反内容策略的情况。我们不测试技能的高级功能，例如与智能家居物联网设备的交互和技能连接。

第二，由于来自不同VPA平台的内容策略的多样性，我们在度量中手工编制并提交了违反策略的技巧。作为我们未来的工作，我们计划使用自然语言处理（NLP）技术来自动生成违反策略的内容，这些内容可以填充在技能模板中。给定一个精心设计的技能，可以通过使用SeleniumWebDriver来自动化技能部署和提交过程[14]。

第三，虽然我们开发了一个策略违反检测器，但是我们手动探索了技能的交互行为来识别存在问题的技能。未来的工作是需要设计动态分析工具来自动化与不同VPA平台中技能的交互。然而，我们收集了强有力的证据，揭示了领先的VPA平台中技能认证的不可信性，并根据经验描述了由于宽松的技能认证过程而带来的潜在安全风险。我们的工作有可能极大地改变当前对VPA平台可信性的看法（即，用户盲目相信VPA平台可以防止恶意技能被发布，并适当保护他们的隐私）。

# Conclusion

我们精心设计并提交了234项Alexa技能和381项谷歌行动，这些行动故意违反VPA平台定义的55项内容和隐私政策。令人惊讶的是，所有Alexa技能和39%的googleactions都能通过认证。我们进一步进行了一项以儿童技能为重点的实证研究，在Alexa平台上发现了31个（755个）违反政策的有问题技能和34个坏技能，以及一个收集用户名的Google action（114个）。我们对203名参与者进行的用户研究证明了用户对VPA平台的错误信任。我们讨论了可操作的缓解策略，以帮助VPA平台提供商增强其技能认证过程的可信度。