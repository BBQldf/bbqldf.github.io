---
layout:     post
title:     Spark共享变量和内核调度方式
subtitle:   广播变量、累加器、DAG、内存迭代
date:       2023-02-18
author:     ldf
header-img: img/post-bg-spark01.png
catalog: true
tags:
    - Spark
---

# 一、Spark共享变量

> Spark两种共享变量：
>
> - 广播变量（broadcast variable）：高效分发较大的对象
>
> - 累加器（accumulator）：信息进行聚合

## 共享变量出现的原因

通常在向 Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以/需要 使用驱动器程序(Driver)中定义的变量。Driver会向所有任务都发一个数据副本，更新这些副本的值也不会影响驱动器中的对应变量。但是这样发送，会存在大量的冗余数据备份和冗余网络I/O，如图：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230218212951.png)

因此，Spark设置了一个共享变量声明！Driver只将副本发送给一个Executor（**之前是task级别，现在是Executor级别**），任务之间可以共享这个副本。通过使用共享变量，Spark可以避免在节点之间来回发送数据的需要（这可能是缓慢和低效的）。

Spark 的两个共享变量，**广播变量**与**累加器**，分别为结果广播与聚合这两种常见的通信模式突破了这一限制。

这两种类型的共享变量都是为了在分布式环境中工作而设计的，在这种环境中，数据分散在集群的多个节点上。这使得Spark在大数据处理方面更加高效和可扩展。

> 问题：类比于Java的共享变量，多线程在操作同样数据的时候，如何保证数据一致性？
>
> Spark设置的时候考虑了这一点，**广播式变量是只读变量；累加器是只写变量**。

## 1、广播变量

Spark 会自动把闭包中所有引用到的变量发送到工作节点上。虽然这很方便，但也很**低效**，因为：

1. 默认的任务发射机制是专门为小任务进行优化的
2. 会在多个并行操作中使用同一个变量，但是 Spark 会为每个操作分别发送（冗余数据发送）

上面的图已经进行了反映。

要使用广播变量也很简单：

1. 使用`sc.broadcast(Array(1,2,3))`来定义广播变量；
2. 使用`broadcastVar.value`来访问变量



```Scala
val conf = new SparkConf().setMaster("local").setAppName("broadcast")
val sc = new SparkContext(conf)
val list = List("hello java")
val broadcast = sc.broadcast(list)	# 声明 
val linesRDD = sc.textFile("./word")

    linesRDD.filter(line => {

      broadcast.value.contains(line)

}).foreach(println)

 sc.stop()

  }

}
```

**注意事项：**

1. 广播变量只适用于Driver（非RDD代码都是由Driver执行）上的数据，**RDD上的数据不能广播**。但是可以把RDD的结果数据广播。
2. 广播变量只能在Driver端定义，不能在Executor端定义
3. 在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。（即使excutor端修改了变量，driver端也不会修改，因为executor上只是一个数据副本，这个就是**累加器出现的原因**）



## 2、累加器

> 累加器提供了将**工作节点**中的值**聚合到驱动器**（Driver）程序中的简单语法；
>
> 它的作用有点把内存指针(而不是数据拷贝)发送给了Executor

用法：

1. 通过在driver中调用 `SparkContext.accumulator(initialValue) `方法，创建出存有初始值的累加器。返回值为 `org.apache.spark.Accumulator[T]` 对象，其中 T 是初始值initialValue 的类型
2. Spark闭包（函数序列化）里的excutor代码可以使用累加器的 += 方法（在Java中是 add ）增加累加器的值
3. driver程序可以调用累加器的 value 属性（在 Java 中使用 value() 或 setValue() ）来访问累加器的值



![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230218214740.png)



累加器的使用也很简单：

```Scala
  val conf = new SparkConf().setMaster("local").setAppName("accumulator")

    val sc = new SparkContext(conf)

    val acmlt = sc.accumulator(0); //创建accumulator并初始化为0

    val linesRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10],2)	# 设置两个分区

    val result = linesRDD.map(s => {

      	System.out.println(s);
        acmlt.add(1); //有一条数据就增加1

    })

    result.collect();

    println("words lines is :" + acmlt.value)

    sc.stop()

  }
}
/**
输出：
1
2
3
4
5
1
2
3
4
5
words lines is : 10
*/
```

**注意：**

1. 累加器在Driver端定义赋初始值，累加器只能**在Driver端读取，在Excutor端更新**
2. **只允许added操作**，常用于实现计数，调试时对作业执行过程中的事件进行计数
3. 对于累加器的使用，要特别注意RDD的生命周期，重复使用RDD会使得累加器也重复。看下面这个例子：

```scala
  val conf = new SparkConf().setMaster("local").setAppName("accumulator")

    val sc = new SparkContext(conf)

    val acmlt = sc.accumulator(0); //创建accumulator并初始化为0

    val linesRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10],2)	# 设置两个分区

    val result = linesRDD.map(s => {

      	System.out.println(s);
        acmlt.add(1); //有一条数据就增加1

    })
    result.collect();	// action函数执行
	rdd2 = result.map(lambda x:x)	// 这里会出现溯源result的情况
    
    System.out.println("words lines is :" + acmlt.value)

    sc.stop()

  }
}
/**
输出：
1
2
3
4
5
1
2
3
4
5
1
2
3
4
5
1
2
3
4
5
words lines is : 20
*/
```

**现象：**

1. 打印部分重复了两遍
2. 最后的输出从10变成了20

**原因：**

1. `result.collect()`Action函数执行后，前面的 RDD 算子result等就失效了（因为RDD是过程数据）
2. `rdd2 = result.map(lambda x:x)`再次执行，就要去进行溯源，根据血缘关系重新构造执行链条，导致`val result = linesRDD.map(s =>...)`这一行被重复执行

解决方案：在`collect()`函数之前作一个持久化操作`cache()`

```scala
    val linesRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10],2)	# 设置两个分区

    val result = linesRDD.map(s => {

      	System.out.println(s);
        acmlt.add(1); //有一条数据就增加1

    })
    result.cache();
    result.collect();	// action函数执行
	rdd2 = result.map(lambda x:x)	// 这里会出现溯源result的情况
```



## 3、综合案例

### 1). 需求

对一个文件夹中的字符进行区分：

1. 对正常的单词进行统计词频
2. 统计特殊字符出现的次数

特殊字符的定义如下：`abnormal = [",", ".", "!", "#", "$", "%"]`

文件数据：

```txt
hadoop spark # hadoop spark spark
mapreduce ! spark spark hive !
hive spark hadoop mapreduce spark %
spark hive sqL sqL spark hive，hive spark !
! hdfs hdfs mapreduce mapreduce spark hive
#



```

### 2). 代码实现

1. 构建SparkContext对象

```python
config = SparkConf().setMaster("local[*]").setAppName("accumulator")
sc = SparkContext(conf = config)
```

2. 读取数据文件

```python
file_rdd = sc.textFile("../data/data.txt")
```

3. 特殊字符的list定义（在Driver中）；并且包装成广播变量

```python
abnormal = [",", ".", "!", "#", "$", "%"]
broad = sc.broadcast(abnormal)
```

4. 定义特殊字符出现的次数为累加器

```python
acmlt = sc.accumulator(0)
```

5. 过滤掉无用的 空格/空行

```Python
# 去除空行
line_rdd = file_rdd.filter(lambda line:line.strip())	# strip()函数是去除首尾的空格后返回字符串本身，如果是空行的话，就返回None；在Python中，有内容就自动识别为True，否则就是False；

# 去除空格
data_rdd = line_rdd.map(lambda line:line.strip())
```

6. 对数据进行切分，按照正则表达式切分（因为单词之间可能存在一个或者多个空格）

```python
data_rdd.flatMap(lambda line:re.split("\s+"),line)
```

7. 过滤掉特殊字符，并且顺带统计过滤字符的个数

```python
filter_rdd = data_rdd.filter(filterData)

def filterData(data):
	global acmlt	# 获取累加器
	abnorList = broad.value	# 获取广播变量的值
	if data in abnorList:
		acmlt += 1
		return False
	else:
		return True
```

8. 把剩余字符(都是正常字符了)进行词频统计

```python
result = filter_rdd.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).collect().foreach(println)
print("特殊字符个数：",acmlt)

'''
输出：
[('hadoop',3),('hive',6),('hdfs',2),('spark',11),('mapreduce',4),('sql',2)]
特殊字符个数：8
'''
```

## 4、总结

**广播变量解决了什么问题?**

- 分布式集合 RDD 和本地集合进行关联使用的时候,降低内存占用以及减少网络 I/O 传输,提高性能.

**累加器解决了什么问题?**

- 分布式代码执行中,进行全局累加

# 二、Spark内核调度(重点)

> 这一部分涉及到Spark的任务调度原理、Stage划分、内存迭代、并行度等，是Spark内核的重要部分，也是**面试知识点考察的重要环节**！！

Spark的核心是根据RDD来实现的，**Spark Scheduler**则为Spark核心实现的重要一环，其作用就是**任务调度**：

- 组织任务(Task)去处理RDD中每个分区的数据
  1. 根据RDD的依赖关系构建DAG
  2. 基于DAG划分Stage
  3. 将每个Stage中的任务发到指定节点运行



基于Spark的任务调度原理，可以合理规划资源利用，做到尽可能用最少的资源高效的完成任务计算！

**举几个面试常考的题目：**

1. 简述Spark的作业提交流程？Spark提交作业的参数？
2. 简述Spark的宽窄依赖？
3. Spark如何划分stage，每个stage又根据什么决定task个数?

> 这一部分内容，在Spark基础中已经做过介绍了，这里我会适当删减部分描述，但这并不意味着它不重要。

## 1、Spark 任务调度概述

一个 Spark 应用程序包括**Job**、**Stage** 以及 **Task** （他们的关系是：`Job > Stage > Task`）：

1. Job 是以 Action 算子为界，遇到一个Action算子则触发一个Job
2. Stage 是 Job 的子集，以 RDD 宽依赖(即 Shuffle )为界，遇到 Shuffle 做一次划分
3. Task 是 Stage 的子集，以并行度(分区数)来衡量，这个 Stage 分区数是多少，则这个Stage 就有多少个 Task

Spark 的任务调度总体来说分两路进行，一路是 Stage 级的调度（逻辑层面，**横向**），一路是 Task 级的调度（物理层面，**纵向**），总体调度流程如下图所示：

![调度概述](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230219171541.png)

对于Stage级的调度：

- **由DAGScheduler负责**，主要是将job切分成若干Stages，并将每个Stage打包成TaskSet交给TaskScheduler调度

对于Task级的调度：

- **由TaskScheduler负责**，将DAGScheduler传过来的**TaskSet**按照指定的调度策略分发到Executor上执行，调度过程中**SchedulerBackend**负责提供可用资源（SchedulerBackend有多种实现，分别对接不同的资源管理系统）

Driver在初始化SparkContext的时候，同时会初始化初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver，并**启动** SchedulerBackend以及HeartbeatReceiver。

前面几个的功能已经介绍了，**HeartbeatReceiver**负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。

## 2、DAG(Directed Acyclic Graph) - 有向无环图

DAG本质上就是标识代码的**逻辑**执行流程，可以看做是Spark任务的排班表，第一步做什么，第二步做什么。Stage就是根据DAG来进行划分的。

在Spark 4040端口上，每个Job有一个`DAG Visualization`属性，这里构建了一个可视化的DAG图。

### 1). Job和Action的关系

Action算子：

- 是执行链条的开关，会将action算子之间的一串RDD依赖链条执行，并且将他们(未做持久化的RDD)内存 回收/销毁
- 返回值不是RDD算子

以之间的`搜索引擎日志分析`案例为例：

![3个DAG](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230220095448.png)

这个可以提交执行以后，去Spark:4040端口可以看到有三个**Job**。

**结论：**

1. 1个Action会产生1个Job(一个应用程序内的子任务)，每一个Job对应1个DAG
2. 如果在代码中有 **N** 个Action就产生 **N** 个DAG
3. 所有的 Job 合在一起形成了 Application

### 2). DAG和分区的关系

> 我们上面画的图，一个action就是对应着一条链条，但是实际上，每个RDD算子是运行在相应的分区上的，DAG图是Spark代码的逻辑执行图。
>
> 而DAG的**最终作用**，是为了构建**物理上**的Spark详细执行计划。

比如，最经典 reduceByKey() 案例图：

```python
rdd1 = SC.textFile()
rdd2 = rdd1.flatMap()
rdd3 = rdd2.map()
rdd4 = rdd3.reduceByKey()
rdd4.action()
```

对应的<font color='red'>**带分区关系的DAG图**：</font>

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230220101519.png)

在这个图里，我们可以看到一些后面将要提到的概念：

1. 每个分区上都有自己的RDD链条，这个就是内存迭代的管道
2. 三个管道间互不干扰，实现并行计算
3. 所有的数据最后都会收集到Driver上



## 3、DAG的宽窄依赖和阶段关系

**Spark RDD 有两种依赖关系：**

- 窄依赖：父RDD的分区，将数据**全部**发给子RDD的一个分区("**一对一**")
- 宽依赖：父RDD的分区，将数据**分发**给多个子RDD分区("**一对多**")；宽依赖还有一个别名叫`shuffle`，因为只有每次shuffle的时候才会出现分区数据 打乱/重置。

**阶段(Stage)划分：**

划分依据：**从后往前**，只要遇到 **宽依赖** 就划分出一个阶段，称之为 Stage

在Stage内部，一定都是 窄依赖。



## 4、内存迭代计算

> 内存迭代计算就是说在同一个task在同一个进程运行，也就是内存计算管道pipeline。
>

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230220163815.png)

上图可以看到：

- Stage1 有 3个 独立的“内存计算管道”，由 3个 线程 并行工作；
- 如果全部放在一个内存进程里面跑（即 Local 模式），不用网络IO，但是这样并行度会大大下降。
  - 如上图，假如Task1和Task4都在同一个Executor上运行，那他们直接只用进行内存通信；但是Task2&Task3 与 Task4 之间就需要网络I/O了
  - 这样的设计，是为了分布式计算，提高计算的并行度，让计算分布在不同的Executor上，提高计算效率

<font color='red'>原则：</font>

- **优先并行计算，再保证“内存计算管道”；**（因为如果一味追求内存计算，那就相当于local模式，其实无意义）





所以，回答之前的一个问题：**为什么不能随便更改RDD的分区数？**

- Spark默认受到**全局并行度**的限制，除了个别算子有特殊分区情况,大部分的算子，都会遵循全局并行度的要求，来规划自己的分区数；
  - 比如上面的图中，原本在shuffle之前我们一直是三个分区，对应也有三个管道，但是我们在操作中强制变为两个分区（增加为5个分区），那么就要**重新shuffle**。

### 面试题1：Spark为什么比MapReduce快？

- 从编程模型上来看，**Spark算子多**，处理速度快。MR编程模型很难在一套MR中处理复杂任务，需要多套MR进行串联，多个MR串联通过磁盘交互数据。
- Spark可以执行内存迭代，算子间形成DAG基于依赖划分阶段后，在阶段内形成**内存**迭代管道，但是MR的交互还是通过**磁盘**来实现。



### 面试题2 : Spark是怎么做内存计算的? DAG的作用? Stage阶段划分的作用?

1. Spark会产生DAG图，然后依据宽窄依赖划分阶段，在每个阶段内产生一系列的内存迭代管道，这些管道就是一个个具体执行的Task
2. DAG图是Spark代码的逻辑执行图，而他的最终作用是为了构建物理上的任务执行管道
5. Stage划分就是为了确定内存管道，走内存计算



> <font color='bule'>Tips(查漏补缺):</font>
>
> HTTP什么时候发生缓存替换？
>
> HTTP 缓存是一种保存资源副本并在下次请求时直接使用该副本的技术。也就是说，当 HTTP 缓存发现请求的资源已经被存储，它会拦截请求，返回该资源的副本，而不会去源服务器重新下载。
>
> - 默认情况下，请求方法如 GET、HEAD的响应内容是可缓存的，在包含新鲜度信息的情况下，POST的响应内容也可以被缓存；
> - 默认情况下，响应码如 200、206、300、301、302、404 等的响应内容可以被缓存；
> - 响应头和请求头没有指明不使用缓存，如 Cache-Control: no-store。
>
> ### 客户端查找缓存的顺序：
>
> 1. 先查找内存，如果内存中存在，从内存中加载；
> 2. 如果内存中未查找到，就从硬盘获取，如果硬盘中有，从硬盘中加载；
> 3. 如果硬盘中未查找到，就进行网络请求；
> 4. 加载到的资源缓存到硬盘和内存。



## 5、Spark并行度

> Spark的并行：在同一时间内，有多少个Task在同时运行。
>
> 全局并行度参数：`spark.default.parallelism`
>
> - **有多少个并行度，就会有多少个分区数**

在 代码/配置文件/客户端提交 中设置，他们的优先级如下：

1. 代码hardCode：`SparkConf().set("spark.default.parallelism","100")`
2. 客户端提交的参数中：`bin/spark-submit --conf "spark.default.paramllelism=100"`
3. 配置文件：`conf/spark-defaults.conf中设置：spark.default.parallelism 100`
4. 默认1（但是不会全部以1来跑，多数时候会基于读取文件的分片数量来作为默认并行度）

### 1). 集群中如何规划并行度	

结论：设置为集群CPU总核心数的2~10倍。比如可用CPU是128个，那么设置并行度为256~1280是比较合适的。

**为什么并行度最少为2倍？**

- CPU的一个核心同一时间只能干一件事情。如果只设置1倍，那N个CPU都会工作，但是这种设置下，如果task的压力不均衡,某个task先执行完了，就导致某个CPU核心空闲。
- 所以,我们将Task(并行)分配的数量变多，比如800个并行,同一时间只有100个在运行，700个在**等待**。（同时Task的大小也相应变小了，等待时间也会缩短）

**为什么并行度最多为10倍？**

- 因为Task的调度也需要开销。并行度太大，任务数会更多，分区数也更多，首先要申请这些分片好的资源，其次要进行任务调度，都是耗时的。



## 6、Spark的任务调度详解

Spark的任务，由Driver进行调度：

1. 根据代码，生成逻辑DAG
2. 申请分区资源，生成带分区的DAG
3. Task划分
4. 将task分配给Executor，并监控其工作



上面已经说了，“Spark 的任务调度总体来说分两路进行，一路是 Stage 级的调度（逻辑层面，**横向**），一路是 Task 级的调度（物理层面，**纵向**）”，下面我们具体来看一下他们的作用。

### 1). Spark Stage 级别调度

Spark的任务调度是从DAG切割开始，主要是由**DAGScheduler**来完成：

1. SparkContext将Job交给DAGScheduler提交，它会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages
   - **策略：**最终的RDD不断通过依赖回溯（深度优先）判断父依赖是否是宽依赖，即**以Shuffle为界，划分Stage**
2. 窄依赖的RDD之间被划分到同一个Stage中，可以进行pipeline式的计算
3. 划分的Stages分两类，一类叫做ResultStage，为DAG最下游的Stage，由Action方法决定，另一类叫做ShuffleMapStage，为下游Stage准备数据

一个Stage是否被提交，需要判断它的父Stage是否执行，只有在父Stage执行完毕才能提交当前Stage，**Stage提交时**：

- DAGScheduler将Task信息（分区信息以及方法等）序列化并打包成TaskSet交给TaskScheduler
- 另一方面TaskScheduler会监控Stage的运行状态，只有Executor丢失或者Task由于Fetch失败才需要 **重新提交** 失败的Stage以调度运行失败的任务，其他类型的Task失败会在TaskScheduler的调度过程中**重试**

### 2). Spark Task 级别调度

> 相对来说DAGScheduler做的事情较为简单，仅仅是在Stage层面上划分DAG，提交Stage并监控相关状态信息， TaskScheduler则相对较为复杂。

Spark Task 的调度是由**TaskScheduler**来完成，TaskScheduler会将TaskSet封装为TaskSetManager加入到调度队列中：

- **TaskSetManager**负责监控管理同一个Stage中的Tasks，

![TaskSetManager](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230220174402.png)

TaskScheduler就是以TaskSetManager为单元来调度任务。

前面也提到，TaskScheduler初始化后会启动SchedulerBackend，它负责跟外界打交道，接收Executor的注册信息，并维护Executor的状态：

1. SchedulerBackend是管“粮食”的，同时它在启动后会定期地去“询问”TaskScheduler有没有任务要运行
2. TaskScheduler在SchedulerBackend“问”它的时候，会从调度队列中按照指定的**调度策略**选择TaskSetManager去调度运行

具体的调用流程：

![TaskSetManager调度流程](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230220180028.png)

这个图其实不必太过深究，就分几步：

1. **申请资源：**TaskScheduler将TaskSetManager加入rootPool调度池中之后，调用SchedulerBackend的riviveOffers方法给**driverEndpoint**发送ReviveOffer消息
2. **准备资源：**driverEndpoint收到ReviveOffer消息后调用makeOffers方法，过滤出活跃状态的Executor（这些Executor都是任务启动时反向注册到Driver的Executor），然后将Executor封装成WorkerOffer对象
3. **分配任务：**准备好计算资源（WorkerOffer）后，taskScheduler基于这些资源调用resourceOffer在Executor上分配task

#### 2.1 调度策略

TaskScheduler支持两种调度策略，一种是FIFO，也是默认的调度策略，另一种是FAIR（公平性算法）。

**FIFO调度策略：**

- 直接简单地将TaskSetManager按照先来先到的方式入队，出队时直接拿出最先进队的TaskSetManager，其树结构如下图所示，TaskSetManager保存在一个FIFO队列中。

**FAIR调度策略(0.8 开始支持)：**

![FAIR模式中有一个rootPool和多个子Pool，各个子Pool中存储着所有待分配的TaskSetMagager](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230220183636.png)

- 对子Pool进行排序，再对子Pool里面的TaskSetMagager进行排序，因为Pool和TaskSetMagager都继承了Schedulable特质，因此使用相同的排序算法
- 排序过程的比较是基于Fair-share来比较的，每个要排序的对象包含三个属性: runningTasks值（正在运行的Task数）、minShare值、weight值，比较时会综合考量runningTasks值，minShare值以及weight值
  - minShare、weight的值均在公平调度配置文件fairscheduler.xml中被指定，调度池在构建阶段会读取此文件的相关配置
- 排序规则：
  1. 如果 A 对象的runningTasks大于它的minShare，B 对象的runningTasks小于它的minShare，那么B排在A前面；（runningTasks 比 minShare 小的先执行）
  2. 如果A、B对象的 runningTasks 都小于它们的 minShare，那么就比较 runningTasks 与 math.max(minShare1, 1.0) 的比值（minShare使用率），谁小谁排前面；（minShare使用率低的先执行）
  3. 如果A、B对象的runningTasks都大于它们的minShare，那么就比较runningTasks与weight的比值（权重使用率），谁小谁排前面。（权重使用率低的先执行）
  4. 如果上述比较均相等，则比较名字
- FAIR模式排序完成后，所有的TaskSetManager被放入一个ArrayBuffer里，之后依次被取出并发送给Executor执行。



关于Spark Stage/Task 级别任务调度源码分析，可以参考：https://cloud.tencent.com/developer/article/1733855



## 7、失败重试与黑名单机制

除了选择合适的Task调度运行外，**TaskScheduler**还需要监控Task的执行状态。

1. Task被提交到Executor启动执行后，Executor会将执行状态上报给SchedulerBackend，
2. SchedulerBackend则告诉TaskScheduler，
3. TaskScheduler找到该Task对应的TaskSetManager，并通知到该TaskSetManager，这样TaskSetManager就知道Task的失败与成功状态

**对于失败的Task，**

- 会记录它失败的次数，如果失败次数还没有超过最大重试次数，那么就把它放回待调度的Task池子中，否则整个Application失败

**黑名单：**

- 记录Task上一次失败所在的Executor Id和Host，以及其对应的“拉黑”时间，“拉黑”时间是指这段时间内不要再往这个节点上调度这个Task了



## 8、Spark各个名词层级梳理

| Term            | Meaning                                                      |
| --------------- | ------------------------------------------------------------ |
| Application     | 一个用户程序，对应一个Driver                                 |
| Application jar | 一个打包好的可运行的程序；在runtime的时候会连接上Hadoop or Spark libraries |
| Driver          | 运行main()函数的进程，并且负责application的创建              |
| Cluster manager | 集群管理器，管理服务资源的                                   |
| Deploy mode     | cluster和Yarn                                                |
| Worker node     | 单个 机器/容器 的资源管理者                                  |
| Executor        | 程序具体运行器，占有CPU资源，能运行task                      |
| Task            | 最小工作单元，是一个线程                                     |
| Job             | 并行化的计算集合，一个application中可以有多个Job             |
| Stage           | 逻辑存在，根据宽窄依赖进行划分；一个Job可有有1个或者多个Stage |

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230220184155.png)

层级关系梳理：

1. 一个Spark环境可以运行多个Application
2. 一个代码运行起来, 会成为一个Application
3. 一个Application内部可以有多个Job
4. 每个Job由一个Action产生, 并且每个Job有自己的DAG执行图
   - 一个Job的DAG图会基于宽窄依赖划分成不同的Stage
5. **不同Stage内基于分区数量，形成多个并行的内存迭代管道**
   - 每一个内存迭代管道形成一个Task 
6. 同一个Job被划分出来的task在逻辑上称之为这个job的taskSet





### 1). partition、task、executor关系？

- 一个executor能够并行执行多个task，实际上一个executor是一个进程，task是executor里的一个线程。
- 一个task对应一个partition









> <font color='bule'>Tips(查漏补缺):</font>
>
> 能介绍下你所知道和使用过的Spark调优吗?
>
> ### 资源参数调优
>
> - num-executors：设置Spark作业总共要用多少个Executor进程来执行
> - executor-memory：设置每个Executor进程的内存
> - executor-cores：设置每个Executor进程的CPU core数量
> - driver-memory：设置Driver进程的内存
> - spark.default.parallelism：设置每个stage的默认task数量
>
> ### 开发调优
>
> 1. 避免创建重复的RDD
> 2. 尽可能复用同一个RDD
> 3. 对多次使用的RDD进行持久化
> 4. 尽量避免使用shuffle类算子
> 5. 使用map-side预聚合的shuffle操作
> 6. 使用高性能的算子
>    - 使用reduceByKey/aggregateByKey替代groupByKey
>    - 使用mapPartitions替代普通map
>    - 使用foreachPartitions替代foreach
>    - 使用filter之后进行coalesce操作
>    - 使用repartitionAndSortWithinPartitions替代repartition与sort类操作
> 7. 广播大变量
>    - 在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC(垃圾回收)，都会极大地影响性能。
> 8. 使用Kryo优化序列化性能
> 9. 优化数据结构
>    - 在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性(说一个底层的实现差异，链表是随机内存，而数组是连续内存，对数组的优化可以参考redis的zipList)