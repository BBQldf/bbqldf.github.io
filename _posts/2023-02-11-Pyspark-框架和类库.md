---
layout:     post
title:     Pyspark-框架和类库
subtitle:   Pyspark介绍、Spark集群角色分析回顾
date:       2023-02-11
author:     ldf
header-img: img/post-bg-spark04.png
catalog: true
tags:
    - Spark
---

# Pyspark-框架和类库

# 一、框架和类库定义

- **框架：**<font color='red'>可以独立运行</font>，并提供编程结构的一种软件产品。Spark就是一个独立的框架；文件目录`bin/pyspark`是一个应用程序，提供了交互式的Python客户端，用于写SparkAPI程序

- **类库：**一堆别人写好的代码，可以方便地导入(import)使用。Pandas就是Python的类库

> 注意点：
>
> - Pandas：用于小规模数据集的处理
> - Spark：用于大规模数据集的处理

## 1、PySpark类库

使用方式：在Python代码中`import pyspark`。PySpark是一个Python类库，内置了完全的SparkAPI，可以通过PySpark类库来编写Spark应用程序，并将其提交到Spark集群中运行。**所以，类库只是提供一个写程序的窗口，后续我们还是调用了标准的API，提交到bin/pyspark集群中运行。**

PySpark**类库**和标准的Spark**框架**(Scala)的对比：

| 功能               | PySpark                                          | Spark                    |
| ------------------ | ------------------------------------------------ | ------------------------ |
| 底层语言           | Python                                           | Scala(JVM)               |
| 上层语言支持       | 仅Python                                         | Python\Java\Scala\R      |
| 集群化\分布式运行  | 不支持，仅支持单机;                              | 支持                     |
| 定位               | Python库(客户端)                                 | 标准框架(客户端和服务端) |
| 是否可以Deamon运行 | No(库本身是不能运行的，也不能成为一个服务端程序) | Yes                      |
| 使用场景           | 本地开发调试Python程序                           | 生产环境集群化运行       |

# 二、应用开发

## 1、应用入口 SparkContext

Spark Application程序入口都是SparkContext，任何一个应用首先需要构建SparkContext对象。构造方法为两步：

1. 创建SparkConf对象
   - 设置SparkApplication基本信息，比如应用的名称AppName和应用运行Master
   - `config = SparkConf().setAppName(appName).setMaster(master)`
2. 基于SparkConf对象，创建SparkContext对象
   - `sc = SparkContext(conf = config)`

3. 结合例子说明使用方法

### 1). 一道经典面试题: Spark中reduceByKey 和 groupByKey 的区别？

先看代码：

- 案例需求：**wordCount单词计数，读取HDFS的words.txt文件，对其内部的单词统计出现的数量**

```Python
from pyspark import SparkConf, SparkContext

if __name__ == '__main__':
	# 创建SparkConf对象
	config = SparkConf().setMaster("local[*]").setAppName("wordCountHelloWorld")
	# 通过SparkConf对象构建SparkContext对象
	sc = SparkContext(conf=config)
	# 1.方式一：读取远端文件
	file_rdd = sc.textfile("hdfs://node1:8020/input/words.txt")
	# 2.方式二：读取本地文件
	file_rdd = sc.textfile("../data/input/words.txt")
	# 将单词进行切割，得到一个存储全部单词的集合对象
	words_rdd = file_rdd.flatMap(lamda line:line.split(" "))
	# 将单词转换为元组对象，key是单词，value是数字1，方便统计
	words_with_one_rdd = words_rdd.map(lambda x:(x:1))
	
	# 将元组的value按照key来分组，对所有的value执行聚合操作(相加);reduceByKey方法中写入操作函数
	words_with_one_rdd.reduceByKey(lambda a,b:a+b)
	# 将元组的value按照key来分组，对所有的value执行聚合操作(相加),再进行map操作;groupByKey方法后还需要一步map
	words_with_one_rdd.groupByKey().mapValues(len(words_with_one_rdd))
	# 通过collect方法收集RDD的数据并打印，方法1
	print(words_with_one_rdd.collect())	#[('hadoop',6),('spark',3),('flink',1)]
    # 通过foreach输出
    words_with_one_rdd.collect().foreach(print)
    # 关闭对象
    sc.stop()
```

![一图看懂代码原理](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230213202303.png)

区别：

1. groupByKey 实现 WordCount

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20210904202351570.png)

**解读：**

1.红色RDD是数据源，包含两个分区的(word,1)数据

2.Shuffle过程（Shuffle过程是需要磁盘IO的）

3.groupByKey后的RDD，根据key分组对Value进行聚合

4.Map操作计算WordCount

**总结：**groupbykey 会导致数据打乱重组，存在shuffle操作。groupByKey工作时会将分区的所有元素都发给由分区器指定的分区，因此对于同样的key，所有的键值对都将包含在同一个分区中。等此动作完成之后，聚合操作就很容易完成了。

> 一旦使用，groupByKey需要将所有的key对应的键值对存储在内存中，如果一个key由太多value，则可能导致OOM(内存溢出错误)。

2. **reduceByKey 实现 WordCount**

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230213200523.png)

**解读：**

1.红色RDD是数据源，包含两个分区的(word,1)数据，在分组前先对分区内的数据进行预聚合

2.Shuffle操作

3.根据指定的聚合公式，对Value进行两两聚合后的结果RDD

<font color='red'>有哪些变化呢？</font>

1.分组前对数据进行了预聚合，参与分组的数据量变小，也即参与Shuffle的数据量变小

2.因为参与Shuffle的数据量变小，所以Shuffle时的磁盘IO次数将变少

3.聚合计算时量量计算的次数变少

<font size='5'>**结论**：</font>

1. 从Shuffle的角度

reduceByKey 和 groupByKey都存在shuffle操作，但是reduceByKey可以在shuffle之前对分区内相同key的数据集进行预聚合（combine），这样会**减少落盘的数据量**，而groupByKey只是进行分组，不存在数据量减少的问题，reduceByKey性能比较高。

2. 从功能的角度

 reduceByKey其实包含分组和**预聚合**（combine）的功能；groupByKey只能分组，不能聚合。

所以在分组聚合的场合下，推荐使用reduceByKey，如果仅仅是分组而不需要聚合，那么还是只能使用groupByKey。

### 2). 提交代码到Linux集群代码

```
from pyspark import SparkConf, SparkContext

if __name__ == '__main__':
	# 创建SparkConf对象
	config = SparkConf().setAppName("wordCountHelloWorld")
	# 通过SparkConf对象构建SparkContext对象
	sc = SparkContext(conf=config)
	# 1.方式一：读取远端文件
	file_rdd = sc.textfile("hdfs://node1:8020/input/words.txt")
	# 将单词进行切割，得到一个存储全部单词的集合对象
	words_rdd = file_rdd.flatMap(lamda line:line.split(" "))
	# 将单词转换为元组对象，key是单词，value是数字1，方便统计
	words_with_one_rdd = words_rdd.map(lambda x:(x:1))
	
	# 将元组的value按照key来分组，对所有的value执行聚合操作(相加);reduceByKey方法中写入操作函数
	words_with_one_rdd.reduceByKey(lambda a,b:a+b)
	# 将元组的value按照key来分组，对所有的value执行聚合操作(相加),再进行map操作;groupByKey方法后还需要一步map
	words_with_one_rdd.groupByKey().mapValues(len(words_with_one_rdd))
	# 通过collect方法收集RDD的数据并打印，方法1
	print(words_with_one_rdd.collect())	#[('hadoop',6),('spark',3),('flink',1)]
    # 通过foreach输出
    words_with_one_rdd.collect().foreach(print)
    # 关闭对象
    sc.stop()
```

相较于上面的代码，这里几个注意点：

1. SparkConf对象不要设置本地模式，因为代码的优先级（写死）高于shell中的配置级别
2. 文件不要从本地读取，因为不适合集群模式，每个读取的文件应该是分布式的，公共的。

保存上述代码到`/root/helloworld.py	`文件中，在集群的一台机器上先`本地模式`运行一下：

```
/bin/spark-submit --master local[*]	/root/helloworld.py	
```

在集群的一台机器上先`集群模式`运行一下：

```
/bin/spark-submit --master yarn	/root/helloworld.py	
```

涉及到容器的构建，结果会比较慢（这并不是代码运行的速度慢）

# 三、回顾：Spark集群角色回顾（以YARN为例）

> 之前介绍了一下，Spark中各类进程的作用；也对比了本地模式、Standalone(伪分布式)、集群模式(YARN)的差别。这里重新回顾一下，加强一下理解。

### 四个主要组成部分（物理层级）：

1. 任务的管理层面（这两个我们是很难感知的）：

   1. **ResourceManager**(相当于原Master)：集群的大管家，负责整个集群的资源管理和分配，即图中的Cluster Manager（负责在不同的应用之间调度和划分资源，同时也为集群分配任务，并且**负责初始化executor进程**）

   2. **NodeManager**(相当于原Worker Node)：单个机器的小管家，负载在单个服务器上提供**运行容器**，管理当前机器资源并完成具体计算

2. 任务的运行层面：
   1. **Driver Program**(或者简称Driver)：控制程序，负责为 Application 构建 DAG 图；**单个Spark任务**的管理者，管理Executor的任务分解分配和执行
   2. **Executor：**是运行在工作节点(Worker Node)上的一个进程，负责**运行** Task，并为应用程序**存储**数据

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230214154404.png)

<font color='red'>这个图还缺失一块</font>，就是Worker Node上面的任务会最后**落盘**，这一部分的存储可以是HDFS、HBase，或者是Mysql。

### 任务的四个逻辑单元：

> Application -> Job -> Stage -> Task

1. **Application：**用户编写的 Spark 应用程序，一个 Application 包含多个 Job
2. **Job：**作业，一个 Job 包含多个 RDD 及作用于相应 RDD 上的各种操作
3. **Stage：**阶段，是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为“阶段”
4. **Task：**任务，运行在 Executor 上的工作单元，是 Executor 中的一个线程。

### Spark运行流程：

1. Application 首先被 **Driver** 构建 DAG 图并分解成 Stage
2. **Driver** 向 Cluster Manager 申请资源
3. Cluster Manager 向某些 Work Node 发送征召信号
4. 被征召的 Work Node 启动 **Executor** 进程响应征召，并向 Driver 申请任务
5. **Driver** （以序列化的形式）分配 Task 给 Work Node
6. **Executor** 以 Stage 为单位执行 Task，期间 Driver 进行监控
7. **Driver** 收到 Executor 任务完成的信号后向 Cluster Manager 发送注销信号
8. Cluster Manager 向 Work Node 发送释放资源信号
9. Work Node 对应 Executor 停止运行

可见，一头一尾都是Driver来控制，Executor只负责中间的RDD计算过程。

# 四、PySpark执行原理深度剖析

> 参考：[PySpark 的背后原理](https://cloud.tencent.com/developer/article/1005418)
>
> 这篇文章做了非常详尽的解释，学完这个，就能清晰地感受Spark跨语言编程的魅力，以及“为什么PySpark的速度比Spark本身要慢，慢在哪里？”

这一块内容之前做过简单的了解，用一个图可以表示：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230214171743.png)

当时对这个图理解不够深刻，只是认识到有这样两个side，并且两侧的运行程序不一致。在Driver侧，运行的是JVM程序，python只提供编写，之后交由py4j库翻译；在Executor侧，JVM会通过socket通信将任务发给PySpark.worker进程执行。

真正比较全面的解释是下图：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230214192021.png)

白色部分是新增的 Python 进程，

- **在 Driver 端，**

通过 **Py4j** 实现在 Python 中调用 Java 的方法，即将用户写的 PySpark 程序"映射"到 JVM 中

例如，用户在 PySpark 中实例化一个 Python 的 SparkContext 对象，最终会在 JVM 中实例化 Scala 的 SparkContext 对象

**对于大数据量，例如广播变量等，Python 进程和 JVM 进程是通过本地文件系统来交互，以减少进程间的数据传输**

- **在 Executor 端，**

不需要借助 Py4j，因为 Executor 端运行的 Task 逻辑是由 Driver 发过来的，那是**序列化后的字节码**，虽然里面可能包含有用户定义的 Python 函数或 Lambda 表达式，Py4j 并不能实现在 Java 里调用 Python 的方法

为了能在 Executor 端运行用户定义的 Python 函数或 Lambda 表达式，则需要为每个 Task 单独启一个 Python 进程，通过 **socket** 通信方式将 Python 函数或 Lambda 表达式发给 Python 进程执行

## Driver运行原理

整个过程相当于py直接和executor直接通讯！jvm只认java对象不认python对象，所以用py4j；源码位置在$SPARK_HOME/python/pyspark/java_gateway.py

当我们通过 spark-submmit 提交 pyspark 程序，首先会上传 python 脚本及依赖，并申请启动 Driver，当申请到 Driver 资源后，会通过 PythonRunner(其中有 main 方法) 拉起 JVM：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230214193958.png)

PythonRunner 入口 main 函数里主要做两件事：

- 开启 Py4j GatewayServer
- 通过 Java Process 方式运行用户上传的 Python 脚本

用户 Python 脚本起来后，首先会实例化 Python 版的 SparkContext 对象，在实例化过程中会做两件事：

- 实例化 Py4j GatewayClient，连接 JVM 中的 Py4j GatewayServer，后续在 Python 中调用 Java 的方法都是借助这个 Py4j Gateway
- 通过 Py4j Gateway 在 JVM 中实例化 SparkContext 对象

经过上面两步后，SparkContext 对象初始化完毕，Driver 已经起来了，开始申请 Executor 资源，同时开始调度任务。

- 用户 Python 脚本中定义的一系列处理逻辑最终遇到 action 方法后会触发 Job 的提交，提交 Job 时是直接通过 Py4j 调用 Java 的 PythonRDD.runJob 方法完成，映射到 JVM 中，会转给 sparkContext.runJob 方法；

- Job 运行完成后，JVM 中会开启一个本地 Socket 等待 Python 进程拉取，对应地，Python 进程在调用 PythonRDD.runJob 后就会通过**Socket** 去拉取结果。

## Executor运行原理

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230214194422.png)

Executor 端收到 Task 后，会通过 launchTask 运行 Task，最后会调用到 PythonRDD 的 compute 方法，来处理一个分区的数据，PythonRDD 的 compute 方法的计算流程大致分三步走：

- 如果不存在 pyspark.deamon 后台 Python 进程，那么通过 Java Process 的方式启动 pyspark.deamon 后台进程。
  - 注意每个 Executor 上只会有一个 pyspark.deamon 后台进程，否则，直接通过 Socket 连接 pyspark.deamon，请求开启一个 pyspark.worker 进程运行用户定义的 Python 函数或 Lambda 表达式。
  - pyspark.deamon 是一个典型的多进程服务器，来一个 Socket 请求，fork 一个 pyspark.worker 进程处理，一个 Executor 上同时运行多少个 Task，就会有多少个对应的 pyspark.worker 进程
- 紧接着会单独开一个线程，给 pyspark.worker 进程喂数据，pyspark.worker 则会调用用户定义的 Python 函数或 Lambda 表达式处理计算
- 在一边喂数据的过程中，另一边则通过 Socket 去拉取 pyspark.worker 的计算结果



## 总结

总体上来说，PySpark 是借助 Py4j 实现 Python 调用 Java，来驱动 Spark 应用程序，本质上主要还是 JVM runtime，Java 到 Python 的结果返回是通过本地 Socket 完成。虽然这种架构保证了 Spark 核心代码的独立性，但是在大数据场景下，JVM 和 Python 进程间频繁的数据通信导致其性能损耗较多，恶劣时还可能会直接卡死，所以建议对于大规模机器学习或者 Streaming 应用场景还是慎用 PySpark，尽量使用原生的 Scala/Java 编写应用程序，对于中小规模数据量下的简单离线任务，可以使用 PySpark 快速部署提交