# Spark3.2在Linux环境下安装（集群部署）

# 一、系统环境

1. ubuntu-16.04.1-desktop-amd64
2. jdk8以上
3. Python3.7（这里我们直接安装anaconda，再在anaconda内部安装python环境）-因为用的是pySpark
4. hadoop3.3及以上
5. ~~Scala（Spark项目是由Scala语言编写的,因此需提前配置Scala环境才能运行Spark程序）~~
6. spark3.3.1（安装好Spark以后,里面就自带了scala环境,不需要额外安装scala）——Spark版本有很多，但是向前兼容是ok的，所以下最新的即可（但是要注意他对应的hadoop版本要求）

## 1.1 安装ubuntu

### 1.1.1 安装一个典型模式下的ubuntu16.04

#### 1、解决VMwareTools解决Bmware虚拟机与windows主机之间不能复制粘贴的问题

可以参考：https://blog.csdn.net/A779929/article/details/127703007

对于Vware Tools无法点击，最好是关掉虚拟机再重启一遍就好了。

如果还不行，可以执行下面几个命令：

```txt
sudo apt autoremove open-vm-tools	#删除原来安装过的文件
#输入安装命令：
sudo apt install open-vm-tools
sudo apt install open-vm-tools-desktop
```

### 1.1.2 解决集群之间的网络通信问题

> 因为我们想要创建的是Spark集群，在VMware上，模拟集群的方案，就是设置多个不同ip的机器，并且完成机器之间的通信

#### 1、为虚拟机器的网络分配网关和IP

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221201220312.png)



![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221201220709.png)

以上，相当于为VMware配置好了网关参数，下一步要去windows下吧这个VMnet8虚拟网卡进行配置，使其能上网（否则就算linux有了网卡，但是没办法链接到外部网路）

#### 2、为虚拟网络连接外网

- 进入我们自己电脑的 网络和Internet设置(右击网络图标)，进入更多网络适配器页面：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221201222102.png)

注意：这个是通过VMnet8搭建起来的虚拟网络的网关，就是编辑VMnet8虚拟网络时进行设置的网关地址，**如果设置错误，宿主机访问不到虚拟机，虚拟机访问不到外网**（<font color='red'>这一步在win10下配合VMware其实是不需要的；在上面的步骤中，在VMware上配置好参数之后，这一步就自动帮我们完成了；可以直接进入第3步</font>）

上面的**IP地址：**这个地方设置的IP地址，是宿主机访问VMnet8局域网内虚拟机时，宿主机使用的IP地址，代表宿主机在VMnet8虚拟网络中的地址， 需要注意的是他并不是网关，并且不能使用已经分配给网关的地址。它需要单独占用一个VMnet8局域网内的主机地址，供宿主机使用。让VMnet8网络中的虚拟机可以看到宿主机，宿主机也可以看到虚拟机。

#### 3、在虚拟机器（ubuntu）上手动配置信息


然后再去Ubuntu 中去手动配置好IP地址：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221201221405.png)

两种方案：

1. 可视化配置，利用desktop的network，setting - network - add
2. 修改配置文件：

```
sudo vim /etc/network/interfaces
# auto eth0
# iface eth0 inet dhcp
auto eth0			# 设置网卡为0；（其实在linux下的eth0就是对应于系统的eth8的，那是Vmware的虚拟网卡）
iface eth0 inet static
address 192.168.85.130
netmask 255.255.255.0
gateway 192.168.85.2
dns-nameservers 8.8.8.8 114.114.114.114   # dns服务器：设置才能访问互联网，不设置只能访问纯ip地址的网页
```

注意：自 ubuntu 17.10 开始，Ubuntu 已放弃在 /etc/network/interfaces 里设置静态 IP 的办法了，即使配置也不会生效，而是改成 netplan 方式 ，配置写在 /etc/netplan/01-network-manager-all.yaml 或者类似名称的 yaml 文件里（此处请注意 yaml 的语法格式）

（配置好后，记得应用，否则不会进行切换）

用本机ping一下：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221201221553.png)

PS：如果主机之间都能ping通，但是`ping www.baidu.com`显示`ping: unknown host www.baidu.com`：

可以试一下`ping 14.215.177.38`，这个是百度的ip地址。如果这个是成功的话，那么说明是DNS服务的问题，在ip配置中增加dns服务器的地址：`8.8.8.8`或者`114.114.114.144`

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221203121733.png)

<font color='red' size = '5'>采用相同的方式，为其他几个slave机器分配ip(可以采用VMware的克隆功能加快流程，但是要相应地更换机器名，机器密码，机器ip等)，并互相完成ping测试 </font>

现在集群就是这样一种情况：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221203123629.png)

### 1.1.3 设置ssh免密链接

> 为了能让集群内的机器进行无感链接（无需密码；主节点无需每发送一次命令给从节点就输入一次密码），需要为他们配置好ssh免密链接。

#### 1、修改主机名

有两种方法：

1. 对slave1虚拟机进行操作的命令，操作的时候会让你输入密码（对slave2虚拟机进行相同操作）：

```
hostnamectl set-hostname slave1          这个命令是永久更改主机名
sudo login           执行完这个命令会让你重新登录以下直接hadoop用户登录
```

2. 配置节点名字：

```
sudo gedit /etc/hostname
将三台设备的hostname分别改为 master，node1，node2
```



#### 2、配置ssh链接

Xshell是一个安全终端模拟软件，可以进行远程登录。我使用XShell的主要目的是在Windows环境下登录Linux终端，传输一些大文件到Linux环境上去。

1、下载安装xshell客户端

2、安装完成之后，如果你直接连接Ubuntu主机会发现连接不上，这是因为Ubuntu主机没有开启SSH服务，需要开启openssh-server：`sudo apt-get install openssh-server`

3、使用`ps -e | grep ssh`；如果只有ssh-agent表示还没启动，需要`/etc/init.d/ssh start`

#### 3、配置ssh免密链接

> 核心思想就是主节点公钥复制到从节点上，使得三台虚拟机可以通过ssh命令互相访问和控制而不需要密码。

1. **修改/etc/hosts文件，补充集群的机器名和IP**

```
192.168.123.130		# master
192.168.123.131		# slave1
192.168.123.132		# slave2
```

在局域网或是INTERNET上，每台主机都有一个IP地址，它区分开每台主机，并可以根据ip进行通讯。但IP地址不方便记忆，所以又有了域名。在一个局域网中，每台机器都有一个主机名，**用于区分主机，便于相互访问。**所以，对于服务器类型的linux系统其作用还是不可忽略的。

比如文件中有这样的定义：

```
192.168.1.100 linumu100 test100
假设192.168.1.100是一台网站服务器，在网页中输入http://linumu100或http://test100就会打开192.168.1.100的网页。
```

通常情况下这个文件首先记录了本机的ip和主机名：

127.0.0.1 localhost.localdomain localhost

2. **把三个机器的公钥共享一下，并且放到authorized_keys文件中**

生成公钥和私钥：

```
ssh-keygen -t rsa
```

把机器的公钥共享：

```
scp /home/slave1/.ssh/id_rsa.pub master1@192.168.123.130:/home/master1/.ssh/id_rsa.pub.slave1			# slave1的公钥发给master1 scp /home/slave1/.ssh/id_rsa.pub master1@192.168.123.130:/home/master1/.ssh/id_rsa.pub.slave2		# slave2的公钥发给master1
```

在master1机器上，放到authorized_keys文件：

```
cat id_rsa.pub >> authorized_keys
cat id_rsa.pub.slave1 >> authorized_keys
cat id_rsa.pub.slave2 >> authorized_keys
```

把authorized_keys文件传给其他两个机器：

```
scp authorized_keys slave1@slave1:/home/slave1/.ssh/authorized_keys	# 把authorized_keys文件slave1
scp authorized_keys slave2@slave2:/home/slave2/.ssh/authorized_keys # 把authorized_keys文件slave2
```

测试一下免密登录效果：

```
master1@ubuntu:~/.ssh$ ssh slave1@slave1
Welcome to Ubuntu 16.04.7 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

13 packages can be updated.
2 updates are security updates.

New release '18.04.6 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

*** System restart required ***
Last login: Sat Dec  3 06:40:13 2022 from 192.168.123.1
slave1@ubuntu:~$ 

```

成功！

##### 2.1、.ssh文件夹下各个文件功能

| 文件名称                        | 文件功能                              |
| ------------------------------- | ------------------------------------- |
| known_hosts                     | 记录ssh访问过计算机的公钥(public key) |
| id_rsa                          | 生成的私钥                            |
| id_rsa.pub                      | 生成的公钥                            |
| authorized_keys（自己手动生成） | 存放授权过的无密登录服务器公钥        |



### 1.1.4 Ubuntu换源

在Linux系统中，apt源的网址保存在/etc/apt/sources.list

```
sudo cp /etc/apt/sources.list sources_backup.list	#备份源文件
sudo vim /etc/apt/sources.list	#修改文件内容
```

以直接删除原有内容，替换为如下内容：

```
#阿里云源
deb-src http://archive.ubuntu.com/ubuntu xenial main restricted
deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe
deb http://mirrors.aliyun.com/ubuntu/ xenial universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe
deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse
deb http://archive.canonical.com/ubuntu xenial partner
deb-src http://archive.canonical.com/ubuntu xenial partner
deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse
```

再更新一下软件check一下走的是哪个：

```
sudo apt-get update
sudo apt-get -f install
sudo apt-get upgrade
```

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221203213940.png)



## 1.2 安装JDK

### 1、OpenJDK和oracleJDK区别

在linux上可以安装两个版本的JDK：一个开源版本Openjdk，还有一个oracle官方版本jdk。

他们的区别：

1. Oracle JDK版本将每三年发布一次，而OpenJDK版本每三个月发布一次；**Oracle JDK将更多地关注稳定性，**它重视更多的企业级用户，而OpenJDK经常发布以支持其他性能，这可能会导致不稳定
2. Oracle JDK的构建过程基于OpenJDK，因此OpenJDK与Oracle JDK之间没有技术差异；
3. OpenJDK 采用GPL V2协议放出，而OracleJDK则采用JRL放出。两者协议虽然都是开放源代码的，但是在使用上的不同在于GPL V2允许在商业上使用，而JRL只允许个人研究使用
4. Oracle JDK具有Flight Recorder，Java Mission Control和Application Class-Data Sharing功能，Open JDK具有Font Renderer功能，这是OpenJDK与Oracle JDK之间的显着差异
5. Oracle JDK具有良好的GC选项和更好的渲染器，而OpenJDK具有更少的GC选项，并且由于其包含自己的渲染器的分布，因此具有较慢的图形渲染器选项
6. **在响应性和JVM性能方面，Oracle JDK与OpenJDK相比提供了更好的性能**
7. Oracle JDK在运行JDK时不会产生任何问题，而OpenJDK在为某些用户运行JDK时会产生一些问题
8. **总的来说,对于普通开发,使用OpenJDK和OracleJDK并没有太大差别,两者的差别更体现在商业考量上面**

但是为了系统稳定性，我们还是选择安装oracleJDK。

### 2、安装oracleJDK

>在Linux环境下安装JDK比windows上要方便很多。
>
>JDK官网：https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html
>
>下为了稳定，安装JDK8；

1. **去官网找到相应的JDK相应的版本，和对应的内核、位数文件**

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221204113847.png)

这个官网的指示性不够，在上图中进行选择。

2. **把下载好的jdk-8u341-linux-x64.tar.gz放到一个目录下面（我这里是/usr/local/，用户级的程序目录）**

```
tar zxvf jdk-8u341-linux-x64.tar.gz #解压缩
cp jdk1.8.0_341 /usr/local/			#-r指“递归持续复制",拷贝到/usr/local/目录下
```

3. 修改系统环境变量，把java环境添加进去

```
sudo gedit /etc/profile		#命令打开环境配置文件
```

配置环境变量，在文件的最下面添加下面的一段内容：

```
export JAVA_HOME=/usr/local/jdk1.8.0_341		# 这里的路径换成自己的
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=${JAVA_HOME}/lib:${JRE_HOME}/lib:${CLASSPATH}
export PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin:${PATH}
```

执行 source /etc/profile 使环境文件生效

```
master1@ubuntu:~/Downloads$ java -version
java version "1.8.0_341"
Java(TM) SE Runtime Environment (build 1.8.0_341-b10)
Java HotSpot(TM) 64-Bit Server VM (build 25.341-b10, mixed mode)
```



> 查漏补缺：JDK和java SE有什么区别？
>
> 经常有人问的是JDK和JRE的区别，其实看名字就能分别，JDK（Java develop kit）java开发工具
>
> JRE（Java runtime environment），java运行时环境
>
> - jdk用来开发的，其中包含了jre；如果不用开发而只需要运行java程序，jre就够了。
>
> - Java SE 以前称为 J2SE。它允许开发和部署在桌面、服务器、嵌入式环境和实时环境中使用的 Java 应用程序。Java SE 包含了支持 Java Web 服务开发的类，并为 Java Platform，Enterprise Edition（Java EE）提供基础。所以JavaSE = JDK+JRE

## 1.3 安装Hadoop

> 下载链接：http://archive.apache.org/dist/hadoop/core/

### 1、关闭防火墙

```shell
systemctl stop firewalld.service
systemctl disable firewalld.service
```

执行完要重启一下

### 2、解压文件并复制到/usr/local中去

```
tar zxvf hadoop-3.3.0.tar.gz		#解压缩
cp -r hadoop-3.3.0 /usr/local/		#拷贝到/usr/local/目录下, -r指“递归持续复制"
```

### 3、添加Hadoop执行环境到系统环境

1. 打开/etc/profile文件，添加PATH：

```vim
#set hadoop environment
export HADOOP_HOME=/usr/local/hadoop/hadoop-3.3.0
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_HOME=$HADOOP_HOME
```

2. `source /etc/profile`让配置生效，看hadoop版本

```shell
master1@ubuntu:~/Downloads$ hadoop version
Hadoop 3.3.0
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r aa96f1871bfd858f9bac59cf2a81ec470da649af
Compiled by brahma on 2020-07-06T18:44Z
Compiled with protoc 3.7.1
From source with checksum 5dc29b802d6ccd77b262ef9d04d19c4
This command was run using /usr/local/hadoop-3.3.0/share/hadoop/common/hadoop-common-3.3.0.jar
```

### 4、配置Hadoop中相应的文件

1. **在Hadoop的配置文件中`./etc/hadoop/hadoop-env.sh`修改Java环境（以自己的安装路径为准）：**

```
export JAVA_HOME=${JAVA_HOME}		# 原

export JAVA_HOME=/usr/local/jdk1.8.0_341	# 替换为
```

2. **在yarn-env.sh中配置JAVA_HOME**（这个配置步骤似乎是多余的）

```
export JAVA_HOME=${JAVA_HOME}		# 原

export JAVA_HOME=/usr/local/jdk1.8.0_341	# 替换为
```

3. **在workers中添加slaves节点：(`gedit workers`)**

```
#原来的workers有一行参数要删除：
localhost
#替换为
slave1
slave2
# 因为已经配置过/usr/hosts，可以直接用域名替代ip
```

4. **配置core-site.xml文件**

```xml
<configuration>
  <property>
      <!-- 指定HDFS中NameNode的地址 -->
       <name>fs.default.name</name>
       <value>hdfs://master1:9000</value>
   </property>
<!-- 缓存存储路径 -->
	<property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/hd_space/tmp</value>
    </property>
 </configuration>
```

- fs.default.name是master1（NameNode）的URL。hdfs://主机名:端口/
- hadoop.tmp.dir：Hadoop的默认临时路径，这个最好配置，如果新增节点或者其他情况下莫名其妙的dataNode启动不了，就删除此文件中的tmp目录；（不过如果删除了NameNode机器的此目录，那么就需要重新执行NameNode格式化的命令）

5. **建立HDFS相关目录**

```
sudo mkdir -p /home/hadoop/hd_space/tmp
sudo mkdir -p /home/hadoop/hd_space/hdfs/name
sudo mkdir -p /home/hadoop/hd_space/hdfs/data
sudo mkdir -p /home/hadoop/hd_space/mapred/local
sudo mkdir -p /home/hadoop/hd_space/mapred/system
```

6. **配置hdfs-site.xml文件**

```xml
<configuration>
	<!-- 这个参数设置为1，因为是单机版hadoop;设置为2，表示有两个datanode -->
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
   <property>
       <name>dfs.data.dir</name>
       <value>/home/hadoop/hd_space/hdfs/data</value>
   </property>
   <property>
       <name>dfs.name.dir</name>
       <value>/home/hadoop/hd_space/hdfs/name</value>
   </property>
    <property>
       <name>dfs.namenode.secondary.http-address</name>
       <value>slave1:50090</value>
   </property>
       <property>
       <name>dfs.namenode.secondary.https-address</name>
       <value>slave1:50091</value>
   </property>
</configuration>
```

- dfs.namenode.http-address：namenode web 端访问地址
- dfs.namenode.secondary.http-address：2号namenode web 端访问地址
- dfs.replication：数据文件的副本数量, 默认是3
- dfs.blocksize:块大小
- dfs.namenode.name.dir:namenode数据的存储
- dfs.datanode.data.dir: datanode数据存储位置
- dfs.namenode.checkpoint.dir: 2nn数据存储位置

7. **配置mapred-site.xml文件**

增加mapreduce配置：

```xml
<configuration>
	<property>
        <name>mapreduce.cluster.local.dir</name>
		 <value>/home/hadoop/hd_space/mapred/local</value>
    </property>
    <property>
        <name>mapreduce.cluster.system.dir</name>
        <value>/home/hadoop/hd_space/mapred/system</value>
    </property>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master1:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master1:19888</value>
    </property>
</configuration>
```

- mapreduce.framework.name：yarn
- mapreduce.jobhistory.address：jobhistory使用地址
- mapreduce.jobhistory.webapp.address：jobhistory的web地址

8. **配置yarn-site.xml文件**

```xml
<configuration>
    <property>
      <description>The valid service name should only contain a-zA-Z0-9 and can not start with numbers
        </description>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    <property>
        <description>The hostname of the RM. (It's usually namenode.)
        </description>
      <name>yarn.resourcemanager.hostname</name>
      <value>master1</value>
    </property>
</configuration>
```

配置详情：

- hadoop.registry.client.auth：客户端认证方式
- hadoop.http.cross-origin.allowed-origins：允许跨域访问的来源，如果有多个，用逗号(,)分隔
- hadoop.registry.dns.bind-address：与 DNS 侦听器应绑定到的网络接口关联的地址
- hadoop.registry.dns.bind-port：YARN Registry DNS的默认端口53
- hadoop.registry.dns.enabled：YARN 注册DNS开启
- hadoop.registry.dns.zone-mask：区域 IP 范围关联的网络掩码。 如果指定，则用于确定可能的 IP 范围并提供适当的反向区域名称
- hadoop.registry.zk.quorum：配置ZK
- yarn.acl.enable：是否启用ACL权限控制
- yarn.admin.acl：yarn用户是YARN集群管理员的 ACL
- yarn.application.classpath：适用于YARN应用程序的CLASSPATH
- yarn.client.nodemanager-connect.max-wait-ms：定义client连接到nodemanager的最大超时时间
- ...

9. 将文件的所有者改为master1自己

```
sudo chown -R hadoop:hadoop /home/hadoop/hd_space/
sudo chown -R hadoop:hadoop /usr/local/hadoop-3.3.0
```



### 5、简历分布式集群——在从节点上安装JDK和HAdoop

1. **同步资源及配置文件到其他两个数据节点**(先切换到root用户`sudo su`)

```
for target in slave1 slave2
do
scp -r /usr/local/jdk1.8.0_341 $target:/usr/local
scp -r /usr/local/hadoop-3.3.0 $target:/usr/local
scp -r /home/hadoop/hd_space $target:/home/hadoop
scp -r /etc/profile $target:/etc
done
```

如果这里出现`scp: /usr/local//jdk1.8.0_341: Permission denied`

这是因为上面是在root用户下，`for target in slave1 slave2`相当于`for target in root@slave1 root@slave2`

登录到slave1的root用户级别，但是它的密码我们没有设定（如果是手动安装的系统，是可以有设置root密码的；如果是自动的，每次密码在开机的时候会随机）

==**解决方案：**==

1. 为slave1和slave2的root用户设置密码
2. 修改代码`for target in hadoop@slave1 hadoop@slave2`，并且同步要把`/usr/local`、`/home/hadoop`、`/etc/profile`的权限进行修改（这些都是只有root权限才能修改的）

第二个解决方案比较复杂，建议选第一个：[ubuntu安装ssh以及开启root用户ssh登录](https://blog.csdn.net/oNelson123/article/details/125417939)

1. root用户密码修改：`sudo passwd root`
2. 开启root用户ssh登录：`sudo vim /etc/ssh/sshd_config`
3. 将 `#PermitRootLogin prohibit-password` 改为：` PermitRootLogin yes`
4. 重启sshd 使配置生效：`systemctl restart sshd`



2. **在slave1和slave2上加载/etc/profile文件，使配置生效**

```
source /etc/profile
#或者使用
. /etc/profile
```

3. 修改hadoop运行时文件权限（不能少，否则后面会出现`permission deny`）

```
# slave1 文件权限
sudo chown -R hadoop:hadoop /home/hadoop/hd_space/
sudo chown -R hadoop:hadoop /usr/local/hadoop-3.3.0

# slave2 文件权限
sudo chown -R hadoop:hadoop /home/hadoop/hd_space/
sudo chown -R hadoop:hadoop /usr/local/hadoop-3.3.0
```

### 6、启动Hadoop(切换到hadoop用户`su hadoop`)

hadoop目录下的文件：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/29O1C1ZBPQOG57FN%5DT%5DNJ5L.png)

1. **格式化文件系统**（`cd /usr/local/hadoop-3.3.0`）——只有首次运行需要格式化（当然，如果配置文件改了什么的，最好也格式化一下）

```
bin/hdfs namenode -format
```

```
master1@ubuntu:/usr/local/hadoop-3.3.0$ bin/hdfs namenode -format
2022-12-04 23:03:39,814 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.3.0
...
...
...
2022-12-05 07:57:13,490 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/

# 修改hostname之后，注意最后一行的差别(其实这里已经有报错了，只是没有注意到java.io.IOException， /home/hadoop/hd_space/hdfs/name/current)
2022-12-05 22:25:11,037 INFO util.ExitUtil: Exiting with status 1: java.io.IOException: Invalid directory or I/O error occurred for dir: /home/hadoop/hd_space/hdfs/name/current
2022-12-05 22:25:11,043 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master1/192.168.123.130
************************************************************/
```

**(其实这里已经有报错了，只是没有注意到java.io.IOException， /home/hadoop/hd_space/hdfs/name/current)**

2. **运行sh文件**

```
./sbin/start-all.sh		#启动所有hadoop的监护进程
```

看到：

```
WARNING: Attempting to start all Apache Hadoop daemons as hadoop in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [localhost]
localhost: Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
Starting datanodes
slave1: WARNING: /usr/local/hadoop-3.3.0/logs does not exist. Creating.
slave2: WARNING: /usr/local/hadoop-3.3.0/logs does not exist. Creating.
Starting secondary namenodes [ubuntu]
ubuntu: Warning: Permanently added 'ubuntu' (ECDSA) to the list of known hosts.
Starting resourcemanager
Starting nodemanagers

#2022年12月6日13:55:37
hadoop@ubuntu:/usr/local/hadoop-3.3.0$ ./sbin/start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hadoop in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [master1]
master1: Warning: Permanently added 'master1,192.168.123.130' (ECDSA) to the list of known hosts.
Starting datanodes
Starting secondary namenodes [slave1]
Starting resourcemanager
Starting nodemanagers
```

或者，分步骤启动：

```
#启动dfs，停止命令为stop-dfs.sh
./sbin/start-dfs.sh

#启动yarn，停止命令为stop-yarn.sh
start-yarn.sh
```

#### **测试效果**

浏览器输入`http://192.168.123.130:50070/dfshealth.jsp`(hadoop 3.x的端口号改成了9870：) 查看启动是否成功

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206165755.png)

输入`192.168.123.130:8088/cluster`(yarn的WebUI)

输入`192.168.123.130:8088/cluster/nodes`

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206142202.png)

如果下面的nodes的名字不是`slave1/slave2`，是因为之前没有把slave2的主机名进行修改，默认为ubuntu；（**但是这会导致和master的主机名冲突，所以要记得在ssh免密登录那里，顺带修改好`/etc/hostname`文件配置**）

看到Hadoop界面代表安装成功！



- 开启状态时使用以下命令查看hadoop运行进程：`jps`(4个进程，一个也不能少！)

```
hadoop@ubuntu:/usr/local/hadoop-3.3.0$ jps
5218 NameNode
5458 SecondaryNameNode
5669 ResourceManager
6231 Jps
```

- 登录到其他节点查看hadoop运行状态：

```
ssh slave1
jps
# 看到下面三个进程就表示成功了：
root@slave1:/home/hadoop# jps
3393 DataNode
3607 Jps
3517 NodeManager
```

- 运行一个小案例：

```
hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar pi 10 10    （输出结果为3.20000说明运行成功）
```

```
Number of Maps  = 10
Samples per Map = 10
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Wrote input for Map #3
Wrote input for Map #4
Wrote input for Map #5
Wrote input for Map #6
Wrote input for Map #7
Wrote input for Map #8
Wrote input for Map #9
Starting Job
2022-12-06 19:27:37,532 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at master1/192.168.123.130:8032
2022-12-06 19:27:38,207 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1670326035668_0001
2022-12-06 19:27:38,385 INFO input.FileInputFormat: Total input files to process : 10
2022-12-06 19:27:38,482 INFO mapreduce.JobSubmitter: number of splits:10
2022-12-06 19:27:38,706 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1670326035668_0001
2022-12-06 19:27:38,707 INFO mapreduce.JobSubmitter: Executing with tokens: []
2022-12-06 19:27:39,007 INFO conf.Configuration: resource-types.xml not found
2022-12-06 19:27:39,007 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2022-12-06 19:27:39,332 INFO impl.YarnClientImpl: Submitted application application_1670326035668_0001
2022-12-06 19:27:39,454 INFO mapreduce.Job: The url to track the job: http://master1:8088/proxy/application_1670326035668_0001/
2022-12-06 19:27:39,469 INFO mapreduce.Job: Running job: job_1670326035668_0001
2022-12-06 19:27:49,637 INFO mapreduce.Job: Job job_1670326035668_0001 running in uber mode : false
2022-12-06 19:27:49,638 INFO mapreduce.Job:  map 0% reduce 0%
2022-12-06 19:27:59,741 INFO mapreduce.Job:  map 20% reduce 0%
2022-12-06 19:28:17,843 INFO mapreduce.Job:  map 20% reduce 7%
2022-12-06 19:28:28,917 INFO mapreduce.Job:  map 100% reduce 7%
2022-12-06 19:28:30,933 INFO mapreduce.Job:  map 100% reduce 100%
2022-12-06 19:28:31,953 INFO mapreduce.Job: Job job_1670326035668_0001 completed successfully
2022-12-06 19:28:32,065 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=226
		FILE: Number of bytes written=2926990
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=2640
		HDFS: Number of bytes written=215
		HDFS: Number of read operations=45
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
		HDFS: Number of bytes read erasure-coded=0
	Job Counters 
		Launched map tasks=10
		Launched reduce tasks=1
		Data-local map tasks=10
		Total time spent by all maps in occupied slots (ms)=308796
		Total time spent by all reduces in occupied slots (ms)=28329
		Total time spent by all map tasks (ms)=308796
		Total time spent by all reduce tasks (ms)=28329
		Total vcore-milliseconds taken by all map tasks=308796
		Total vcore-milliseconds taken by all reduce tasks=28329
		Total megabyte-milliseconds taken by all map tasks=316207104
		Total megabyte-milliseconds taken by all reduce tasks=29008896
	Map-Reduce Framework
		Map input records=10
		Map output records=20
		Map output bytes=180
		Map output materialized bytes=280
		Input split bytes=1460
		Combine input records=0
		Combine output records=0
		Reduce input groups=2
		Reduce shuffle bytes=280
		Reduce input records=20
		Reduce output records=0
		Spilled Records=40
		Shuffled Maps =10
		Failed Shuffles=0
		Merged Map outputs=10
		GC time elapsed (ms)=4649
		CPU time spent (ms)=6470
		Physical memory (bytes) snapshot=2141585408
		Virtual memory (bytes) snapshot=28430401536
		Total committed heap usage (bytes)=1406902272
		Peak Map Physical memory (bytes)=218996736
		Peak Map Virtual memory (bytes)=2583801856
		Peak Reduce Physical memory (bytes)=118923264
		Peak Reduce Virtual memory (bytes)=2592382976
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1180
	File Output Format Counters 
		Bytes Written=97
Job Finished in 54.704 seconds
Estimated value of Pi is 3.20000000000000000000
```

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206193213.png)

### 7、遇到的问题

#### 7.1、**启动时hdfs的时候没有办法通过ssh登录slave1,和slave2**

```shell
master1@ubuntu:/usr/local/hadoop-3.3.0$ ./sbin/start-dfs.sh
Starting namenodes on [master1]
master1: namenode is running as process 5789.  Stop it first.
Starting datanodes
slave2: Permission denied (publickey,password).
slave1: Permission denied (publickey,password).
Starting secondary namenodes [master1]
master1: secondarynamenode is running as process 6001.  Stop it first.
```

其实这个问题，主要是对ssh的登录不熟悉：

- ssh登录时，需要xxx@yyy；其中xxx表示用户名，yyy表示机器IP；

在上面的实验中，我们设置的三台机器名称分别为`master1/slave1/slave2`，==因此我们登录的时候需要指定好用户名==：

```
scp -r hadoop-3.3.0 slave1:/usr/local		# 默认登录的是master1@slave1
#包括上面复制文件的时候也要指定：
scp -r hadoop-3.3.0 slave1@slave1:/usr/local
```

当我们不指定用户名时，就会默认用本机的用户名补全（注意下面的机器名，变成master1@slave1；因此密码也不对）：

```
master1@ubuntu:~$ ssh slave1
master1@slave1's password: 
Permission denied, please try again.
```

<font color='red'>解决方案：</font>更改机器的账户名

> 可以参考：https://blog.csdn.net/qq_34160841/article/details/106886306

<font color='red' size = 5>一定要</font>先切换到root用户下（`sudo su`）进行修改，普通用户下修改用户名后，执行sudo命令会提示密码错误

1. 修改passwd文件：

```shell
sed -i "s/\bmaster1\b/hadoop/g" `grep master1 -rl /etc/passwd`
```

2. 修改shadow文件：

```shell
sed -i "s/\bmaster1\b/hadoop/g" `grep master1 -rl /etc/shadow`
```

3. 修改home目录下文件夹名(将home目录下用户文件夹名修改为新用户的名)：`mv /home/master1/ /home/hadoop`
4. 修改sudo权限：

```
sed -i "s/\bmaster1\b/hadoop/g" `grep master1 -rl /etc/group`
```

**操作玩这些之后，需要重新配置ssh免密（因为校验信息发生了变化）**（最好是全部重头(1.3开始；我已经在上面的流程中进行了校准)来一遍。。。o(╥﹏╥)o）

#### 7.2、jps 后没有 namenode 等输出的情况

这种情况需要删除 $HADOOP_HOME 目录下的 tmp 目录：

1. 先关闭所有 HDFS 服务：`./sbin/stop-all.sh`
2. 删除 tmp 目录：`sudo rm -rf /home/hadoop/hd_space/tmp/`（注意要把master1，slave1，slave2上面的都删掉）
3. 接着再重新启动 HDFS：`./sbin/start-dfs.sh`

#### 7.3、yarn的WebUI对应的nodes与实际不符

输入`192.168.123.130:8088/cluster/nodes`

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206140605.png)

如果下面的nodes的名字不是`slave1/slave2`，是因为之前没有把slave2的主机名进行修改，默认为ubuntu；（**但是这会导致和master的主机名冲突，所以要记得在ssh免密登录那里，顺带修改好`/etc/hostname`文件配置**）

`sudo gedit /etc/hostname`

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206142202.png)



#### 7.4、hadoop的主页UI无法访问

这个问题和上面是一样的：也是由于master1没有配置主机名导致（这个情况的出现是因为我的主机名还是默认的ubuntu，而不是master1）

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206141558.png)

**解决方案：**

1. 执行`sudo gedit /etc/hostname`后，==重启主机后（别忘了，否则不生效）==
2. 由于修改了配置，需要重新格式化一下hadoop，执行`bin/hdfs namenode -format`，**再重启服务**

可以看到两次的变化（下面的才是对的——这是因为在core-site.xml配置中`fs.default.name`是master1（NameNode）的URL，即`hdfs://主机名:端口/`；我们在前面配置的是`hdfs://master1:9000`）：

```
master1@ubuntu:/usr/local/hadoop-3.3.0$ bin/hdfs namenode -format
2022-12-04 23:03:39,814 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.3.0
...
...
...
2022-12-05 07:57:13,490 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/

# 修改hostname之后，注意最后一行的差别
2022-12-05 22:25:11,037 INFO util.ExitUtil: Exiting with status 1: java.io.IOException: Invalid directory or I/O error occurred for dir: /home/hadoop/hd_space/hdfs/name/current
2022-12-05 22:25:11,043 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master1/192.168.123.130
************************************************************/
```

#### 7.5、hadoop namenode 节点无法启动，50070端口无法访问（hadoop3.x之后换成9870端口来看集群状态）

##### 问题表现：

1. namenode节点对应的服务器上使用`jps`命令参看查看只有：

```
hadoop@master1:/usr/local/hadoop-3.3.0$ jps
7668 Jps
7370 ResourceManager
```

而在我们的备用节点上（载hdfs-site.xml文件中配置的`dfs.namenode.secondary.http-address`）使用`jps`命令参看查看有四个进程：

```
hadoop@slave1:~/hd_space$ jps
7600 NodeManager
7490 SecondaryNameNode
7368 DataNode
7690 Jps
```

说明主namenode没有启动成功。

2. 执行hadoop命令，进行一些运算操作

```shell
hadoop@master1:~$ hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar pi 10 10 
Number of Maps  = 10
Samples per Map = 10
java.net.ConnectException: Call From master1/192.168.123.130 to master1:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
...
```

报的是`connection exception`，说明没办法连接到`master1:9000`。

也说明了主namenode没有启动成功。

##### 问题分析：

参看网上的一些分析：

1. 有说是防火墙、VPN等网络问题导致的：https://blog.csdn.net/xingyue0422/article/details/85316822
2. 也有说是文件权限导致的：https://blog.csdn.net/mingyuli/article/details/83028543
3. 有一个比较全面，也和我这个问题表现一致的：https://blog.csdn.net/qq_33792843/article/details/107507047

在上面hdfs-site.xml文件中，我们只配置了备用节点，同时在core-site.xml中，我们也只配置了`fs.default.name`来指定HDFS中NameNode的地址是`master:9000`

##### 解决方案：

[问题分析2](https://blog.csdn.net/mingyuli/article/details/83028543)提供了一种通过logs找问题的方案(==非常推荐==)：

去hadoop的./logs目录下，找到`hadoop-hadoop-namenode-master1.log`文件，打开看一下：

```
2022-12-05 23:04:45,727 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/hd_space/hdfs/name/in_use.lock acquired by nodename 8054@master1
2022-12-05 23:04:45,729 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
java.io.IOException: NameNode is not formatted.
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:252)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1197)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:779)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:673)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:760)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1014)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:987)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1756)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1821)
2022-12-05 23:04:45,750 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@51c693d{hdfs,/,null,UNAVAILABLE}{file:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/webapps/hdfs}
2022-12-05 23:04:45,765 INFO org.eclipse.jetty.server.AbstractConnector: Stopped ServerConnector@29d80d2b{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}
2022-12-05 23:04:45,765 INFO org.eclipse.jetty.server.session: node0 Stopped scavenging
2022-12-05 23:04:45,773 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@72bc6553{static,/static,file:///usr/local/hadoop-3.3.0/share/hadoop/hdfs/webapps/static/,UNAVAILABLE}
2022-12-05 23:04:45,773 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@4d0d9fe7{logs,/logs,file:///usr/local/hadoop-3.3.0/logs/,UNAVAILABLE}
2022-12-05 23:04:45,778 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2022-12-05 23:04:45,779 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2022-12-05 23:04:45,779 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2022-12-05 23:04:45,779 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.io.IOException: NameNode is not formatted.
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:252)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1197)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:779)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:673)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:760)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1014)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:987)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1756)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1821)
2022-12-05 23:04:45,781 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: java.io.IOException: NameNode is not formatted.
2022-12-05 23:04:45,797 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master1/192.168.123.130
************************************************************/
```

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206154411.png)

发现问题出在：`java.io.IOException: NameNode is not formatted.`导致`Stopping NameNode metrics system... NameNode metrics system stopped.`

问题说的很明确，就是namenode格式化失败，再往下面一点，`recoverTransitionRead`报错，说明namenode元数据被破坏了，需要修复。

解决方法：[参考](https://www.cnblogs.com/mediocreWorld/p/15189001.html)

进行下面步骤前，最好把服务关闭（`./sbin/stop-all.sh`）;在执行前面1-3步的时候切换到root用户，后面的步骤都切换回hadoop用户组

1. **三台机器上，递归删除以下目录删除：**

`rm -rf /home/hadoop/hd_space/`

2. **三台机器上，重新创建以下目录：**

```
mkdir -p /home/hadoop/hd_space/tmp/
mkdir -p /home/hadoop/hd_space/hdfs/name/
mkdir -p /home/hadoop/hd_space/hdfs/data/
mkdir -p /home/hadoop/hd_space/mapred/local/
mkdir -p /home/hadoop/hd_space/mapred/system/
```

3. **修改目录所属的用户和组**

```
# master1,slave1,slave2 文件权限
chown -R hadoop:hadoop /home/hadoop/hd_space/
```

4. **~~输入命令：`hadoop namenode -recover`~~(这个可以跳过直接到下一步，因为报错没变。。。)**

```
2022-12-06 16:14:22,372 INFO common.Storage: Lock on /home/hadoop/hd_space/hdfs/name/in_use.lock acquired by nodename 8709@master1
2022-12-06 16:14:22,373 WARN namenode.FSNamesystem: Encountered exception loading fsimage
java.io.IOException: NameNode is not formatted.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:252)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1197)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:779)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery(NameNode.java:1661)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1743)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1821)
2022-12-06 16:14:22,375 INFO namenode.MetaRecoveryContext: RECOVERY FAILED: caught exception
java.io.IOException: NameNode is not formatted.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:252)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1197)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:779)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery(NameNode.java:1661)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1743)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1821)
2022-12-06 16:14:22,377 ERROR namenode.NameNode: Failed to start namenode.
java.io.IOException: NameNode is not formatted.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:252)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1197)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:779)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery(NameNode.java:1661)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1743)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1821)
2022-12-06 16:14:22,380 INFO util.ExitUtil: Exiting with status 1: java.io.IOException: NameNode is not formatted.
2022-12-06 16:14:22,384 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master1/192.168.123.130
************************************************************/
```

5. 切换回hadoop用户（`su hadoop`），**执行hadoop namenode –format**

```
************************************************************/
2022-12-06 16:15:29,716 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2022-12-06 16:15:29,888 INFO namenode.NameNode: createNameNode [–format]
Usage: hdfs namenode [-backup] | 
	[-checkpoint] | 
	[-format [-clusterid cid ] [-force] [-nonInteractive] ] | 
	[-upgrade [-clusterid cid] [-renameReserved<k-v pairs>] ] | 
	[-upgradeOnly [-clusterid cid] [-renameReserved<k-v pairs>] ] | 
	[-rollback] | 
	[-rollingUpgrade <rollback|started> ] | 
	[-importCheckpoint] | 
	[-initializeSharedEdits] | 
	[-bootstrapStandby [-force] [-nonInteractive] [-skipSharedEditsCheck] ] | 
	[-recover [ -force] ] | 
	[-metadataVersion ]

2022-12-06 16:15:29,950 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master1/192.168.123.130
************************************************************/
```

这时候就没报错了。

6. 再格式化一下：bin/hdfs namenode -format

```
2022-12-06 16:55:22,625 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2022-12-06 16:55:22,646 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2022-12-06 16:55:22,646 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master1/192.168.123.130
************************************************************/
```

6. **再重启服务`./sbin/start-all.sh`**

```
hadoop@master1:/usr/local/hadoop-3.3.0$ ./sbin/start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hadoop in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [master1]
Starting datanodes
Starting secondary namenodes [slave1]
Starting resourcemanager
Starting nodemanagers
```

看一下jps进程状态：

```
# master1上
hadoop@master1:/usr/local/hadoop-3.3.0$ jps
11945 NameNode
12603 Jps
12303 ResourceManager
# slave1上
hadoop@slave1:~$ jps
10359 Jps
9752 DataNode
9992 NodeManager
9885 SecondaryNameNode
# slave2上
hadoop@slave2:~$ jps
5600 Jps
5364 NodeManager
5228 DataNode
```

ok,问题解决！

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206165755.png)



#### 7.7、执行MapReduce运算失败，提示` Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster Please check whether your <HADOOP_HOME>/etc/hadoop/mapred-site.xml contains the below configuration: `

##### 问题表现：

```
Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster
Please check whether your etc/hadoop/mapred-site.xml contains the below configuration:
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=${full path of your hadoop distribution directory}</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=${full path of your hadoop distribution directory}</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=${full path of your hadoop distribution directory}</value>
</property>
```

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206171413.png)

##### 问题分析：

参考：

1. https://copyfuture.com/blogs-details/202211210558367494
2. https://blog.csdn.net/ly_5683/article/details/106123335
3. http://www.classinstance.cn/detail/155.html

报错信息提示我们找不到main class 可能是**环境变量的问题。**

##### 解决方案：

<font color='red'>注意：下面的三种方案都要在三台机器上同步修改</font>

1. 修改系统环境变量，把Hadoop相关的变量添加进去：

```
# 除了之前export的HADOOP_HOME
export HADOOP_HOME=/usr/local/hadoop/hadoop-3.3.0
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

#下面是新增的
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_HOME=$HADOOP_HOME
```

2. 按照上面的方法，

   1. 输入`hadoop classpath`：

   ```shell
   /usr/local/hadoop-3.3.0/etc/hadoop:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/common/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/*:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn/*
   ```

   2. 编辑yarn-site.xml，添加上`yarn.application.classpath`属性：

```xml
vim yarn-site.xml
#添加如下内容
<configuration>
    <property>
        <name>yarn.application.classpath</name>
        <value>/usr/local/hadoop-3.3.0/etc/hadoop:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/common/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/*:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn/*</value>
    </property>
</configuration>
```

3. 按照上面的方法，==**（亲测可用）**==

   1. 修改mapred-site.xml配置：

   ```xml
   <property>
     <name>yarn.app.mapreduce.am.env</name> <value>HADOOP_MAPRED_HOME=/usr/local/hadoop-3.3.0/etc/hadoop:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/common/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/*:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn/*</value>
   </property>
   <property>
     <name>mapreduce.map.env</name>
     <value>HADOOP_MAPRED_HOME=/usr/local/hadoop-3.3.0/etc/hadoop:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/common/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/*:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn/*</value>
   </property>
   <property>
     <name>mapreduce.reduce.env</name>
     <value>HADOOP_MAPRED_HOME=/usr/local/hadoop-3.3.0/etc/hadoop:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/common/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/*:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn/*</value>
   </property>
   ```

**把修改同步到其他的机器上。**（否则只有修改过的机器能够有效执行）

输出：

```
Job Finished in 54.704 seconds
Estimated value of Pi is 3.20000000000000000000
```



#### 7.6、其他ubuntu安装Hadoop方案和Hadoop的问题解决方案汇总

> 超级无敌详细使用ubuntu搭建hadoop完全分布式集群：https://blog.csdn.net/weixin_45000490/article/details/109522601
>
> hadoop3.3.0多节点（ubuntu下）完全分布式安装：https://blog.csdn.net/lskbk/article/details/114138011
>
> Ubuntu18.04 安装hadoop3.3.0框架（完全分布式）：https://blog.csdn.net/weixin_51181574/article/details/118990868
>
> Hadoop的问题解决方案汇总1：https://www.cnblogs.com/mediocreWorld/p/15189001.html
>
> CDH重启换了Namenode节点后，提示Encountered exception loading fsimage：http://www.ckzixun.com/jishuzixun/10689.html
>
> namenode节点无法启动：https://blog.csdn.net/xingyue0422/article/details/85316822
>
> 【HBase数据开发】集群搭建NameNode未格式化：https://blog.csdn.net/qq_33792843/article/details/107507047
>
> 安装Hadoop伪分布式模式教程，版本为Hadoop3.1.4：http://www.classinstance.cn/detail/154.html



## 1.4 安装Spark（local版本 + 集群版本）

> 下载链接：https://spark.apache.org/downloads.html
>
> 也可以通过：https://archive.apache.org/dist/spark/ 下载其他版本的
>
> 我下载的版本是3.3.1，注意要能支撑上面的hadoop版本(3.0往上)
>
> **不需要再安装Scala了，因为官网写了**：Note that Spark 3 is pre-built with Scala 2.12 in general and Spark 3.2+ provides additional pre-built distribution with Scala 2.13.
>
> 但是如果后续还要用到scala开发的话，还是再装一下比较好。
>
> Pre-build with user-provided Hadoop: 属于“Hadoop free”版，这样，下载到的Spark，可应用到任意Hadoop 版本。

### 1、下载Spark3.3.1（注意后面是hadoop）

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221206130813.png)

#### 1.1、配置步骤

1. **切换到root用户，解压缩文件**

```
tar zxvf spark-3.3.1-bin-hadoop3.tgz	#解压缩
cp -r spark-3.3.1-bin-hadoop3 /usr/local/	#拷贝到/usr/local/目录下, -r指“递归持续复制"
```

2. **修改Spark的配置文件spark-env.sh**

```
cd /usr/local/spark
cp ./conf/spark-env.sh.template ./conf/spark-env.sh		# 复制spark-env.sh.template文件到spark-env.sh中
```

3. **编辑spark-env.sh文件**

增加配置1：

```sh
export JAVA_HOME=/usr/local/jdk1.8.0_341
```

增加配置2（hadoop的classpash：终端输入``）：

```sh
export SPARK_DIST_CLASSPATH=/usr/local/hadoop-3.3.0/etc/hadoop:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/common/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/*:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn/*
```

有了上面的配置信息以后，**Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。**如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。

网上有个[教程](https://dblab.xmu.edu.cn/blog/931/)，

```
编辑spark-env.sh文件(vim ./conf/spark-env.sh)，在第一行添加以下配置信息:
export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
```

不要直接赋值过来，他这个是让我们填入自己hadoop的classpath。

- 到这里，就已经完成了Spark基础配置，可以执行`bin/run-example SparkPi 2>&1 | grep "Pi is"`命令，并且看到输出。但是当我们使用自带的Python Shell测试——`./bin/pyspark`测试的时候，出现问题：

```
root@master1:/usr/local/spark-3.3.1-bin-hadoop3# ./bin/pyspark 
Python 3.5.2 (default, Jan 26 2021, 13:30:48) 
[GCC 5.4.0 20160609] on linux
Type "help", "copyright", "credits" or "license" for more information.
Traceback (most recent call last):
  File "/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/shell.py", line 29, in <module>
    from pyspark.context import SparkContext
  File "/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/__init__.py", line 53, in <module>
    from pyspark.conf import SparkConf
  File "/usr/local/spark-3.3.1-bin-hadoop3/python/pyspark/conf.py", line 110
    _jconf: Optional[JavaObject]
          ^
SyntaxError: invalid syntax
>>> ines = sc.textFile("/usr/local/spark/README.md")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'sc' is not defined
>>> print("123")
123
```

后续我还做了一个打印测试`print("123")`是正常的，给我的感觉好像是Python和spark的关联没有成功（==google了一圈没有答案，未解决；由于后面还要重新配置Python调用spark，这里先放一放==；有兴趣的朋友可以研究下，有结果的话可以contact me ~1049668169@qq.com）；因为这里的报错` _jconf: Optional[JavaObject]`，我去看了一下这个应该是在python中使用java的语法，但是没有被识别成功。源文件可以参考[这个](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/conf.html)

也有一种说法，设置PYSPARK_PYTHON环境变量，`source /etc/profile`中 添加`export PYSPARK_PYTHON=python3`



- 同样地，我们使用自带的Spark Shell测试——`./bin/spark-shell`：

```sh
root@master1:/usr/local/spark-3.3.1-bin-hadoop3# ./bin/spark-shell
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/spark-3.3.1-bin-hadoop3/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/12/12 10:33:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://master1:4040
Spark context available as 'sc' (master = local[*], app id = local-1670812396120).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.1
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_341)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221212103556.png)

这个运行是没什么问题的，但是有一个slf4j的警告在（需要留意一下）。同时我们可以发现，这里已经有个Scala2.12.15的版本（预装——正好为后文安装Scala埋伏笔），spark对应的版本是3.3.1。（ctrl + d或者`:quit`退出）

- 再查看当前节点运行情况——`./sbin/start-all.sh`（这个不需要看，是分布式场景中才会用到的；并且要先启动hadoop才行）

<font size='5'>当然，如果你想要实现一个伪分布式（注意不是yarn这种），可以参考[这篇](https://www.cnblogs.com/luengmingbiao/p/11216383.html)</font>

4. **修改环境变量**（vim /etc/profile）

```
export SPARK_HOME=/usr/local/spark-3.3.1-bin-hadoop3/
export PATH=$PATH:$SPARK_HOME/bin
# 使配置文件生效
source /etc/profile 
```

PYTHONPATH环境变量主要是为了在Python3中引入pyspark库，PYSPARK_PYTHON变量主要是设置pyspark运行的python版本。 

> 查漏补缺：/etc/profile和~/.bashrc的区别？
>
> /etc/profile： 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行。是系统全局针对终端环境的设置，它是login时最先被系统加载的，是它调用了/etc/bashrc，以及/etc/profile.d目录下的*.sh文件，如果有一个软件包，系统上只安装一份，供所有开发者使用，建议在/etc/profile.d下创建一个新的xxx.sh，配置环境变量。
> ~/.bashrc:是用户相关的终端（shell）的环境设置，通常打开一个新终端时，默认会load里面的设置，在这里的设置不影响其它人。如果一个服务器多个开发者使用，大家都需要有自己的sdk安装和设置，那么最好就是设置它。
>
> ps:
> /etc/bashrc: 是系统全局针对终端环境的设置，修改了它，会影响所有用户的终端环境，这里一般配置终端如何与用户进行交互的增强功能等（比如sudo提示、命令找不到提示安装什么包等），新开的终端，已经load了这个配置，最后才load用户自己的 ~/.bashrc。
> ~/.bash_profile:每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.
>
> **总结：**我们一般在自己的电脑上进行操作的时候，为了图省心，一般都是直接修改了/etc/profile全局系统环境变量；但是如果是在公司的电脑上，我们一般会选择修改~/.bashrc文件，保证这个环境只会在我这个用户下生效（所以一般不会用到root权限）。

#### 1.2、测试效果

配置完成后就可以直接使用，不需要像Hadoop运行启动命令：

- 通过运行Spark自带的示例，验证Spark是否安装成功

```
bin/run-example SparkPi
执行时会输出非常多的运行信息，输出结果不容易找到，可以通过 grep 命令进行过滤（命令中的 2>&1 可以将所有的信息都输出到 stdout 中，否则由于输出日志的性质，还是会输出到屏幕中）:
bin/run-example SparkPi 2>&1 | grep "Pi is"
```

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221211173342.png)







### 2、Spark on Yarn配置（集群版本）

其实Yarn和其它模式（比如：上面的local模式）没有太大的区别，确保**HADOOP_CONF_DIR或YARN_CONF_DIR**指向的目录包含Hadoop集群的（客户端）配置文件。增加一些额外配置即可，这些配置用于写数据到dfs和连接到YARN ResourceManager。

1. **修改Spark的配置文件spark-env.sh**

```
cd /usr/local/spark
cp ./conf/spark-env.sh.template ./conf/spark-env.sh		# 复制spark-env.sh.template文件到spark-env.sh中
```

2. **编辑spark-env.sh文件**

增加配置1：

```sh
export JAVA_HOME=/usr/local/jdk1.8.0_341
export HADOOP_HOME=/usr/local/hadoop3.3.0
export HADOOP_CONF_DIR=${HADOOP_HOME}etc/hadoop
export YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export SPARK_MASTER_IP=master1 #这里填主机的ip，因为我之前已经修改过host到ip的映射，这里直接填入master1
export SPARK_MASTER_PORT=7077
# 避免和hadoop其他应用端口冲突
export SPARK_MASTER_WEBUI_PORT=8180
export SPARK_HOME=/usr/local/spark-2.3.3
export SPARK_LIBRARY_PATH=/usr/local/spark-2.3.3/jars

```

增加配置2（hadoop的classpash：终端输入``）：

```sh
export SPARK_DIST_CLASSPATH=/usr/local/hadoop-3.3.0/etc/hadoop:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/common/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/*:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/*:/usr/local/hadoop-3.3.0/share/hadoop/yarn/*
```

有了上面的配置信息以后，**Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。**如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。

可以看一下最后整体的spark-env.sh的配置项（加上之前的local模式）：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221212120657.png)

yarn的capacity-scheduler.xml文件修改配置保证资源调度按照CPU + 内存模式（这一步可以先不做修改，明白是要干嘛之后再做决定会比较好）：

```xml
<property> 
    <name>yarn.scheduler.capacity.resource-calculator</name> 
    <!-- <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value> --> 
    <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value> 
</property>
```

3. **修改workers文件（在比较老的版本中叫slaves）**

```
cp workers.template workers
注视到workers中原有内容，增加配置：
slave1
slave2
```

4. **同步配置到slave1和slave2主机上**

```
for target in slave1 slave2
do
scp -r /usr/local/spark-3.3.1-bin-hadoop3 $target:/usr/local
scp -r /etc/profile $target:/etc
done
```

5. 修改master1,slave1和slave2上的文件所有者

```
sudo chown -R hadoop:hadoop /usr/local/spark-3.3.1-bin-hadoop3
```



#### 2.2、启动Spark，测试效果

注意，要先切换到hadoop用户才能启动，在root下会报错：

```
root@master1:/usr/local/hadoop-3.3.0# ./sbin/start-all.sh 
Starting namenodes on [master1]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [slave1]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
Starting resourcemanager
ERROR: Attempting to operate on yarn resourcemanager as root
ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.
Starting nodemanagers
ERROR: Attempting to operate on yarn nodemanager as root
ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.
```

1. 第一步，启动之前要保证Hadoop启动成功，先使用jps看下进程信息：

```
hadoop@master1:/usr/local/spark-3.3.1-bin-hadoop3$ jps
7314 NameNode
8217 Jps
7676 ResourceManager
hadoop@slave1:~$ jps
4053 NodeManager
3806 DataNode
3935 SecondaryNameNode
4175 Jps
hadoop@slave2:~$ jps
2645 NodeManager
3434 Jps
2508 DataNode
```

五个进程都启动并没有自动断开，说明Hadoop启动成功。

2. 第二步，启动Spark：进入Spark的sbin目录下执行start-all.sh启动Spark，启动后，通过jps查看最新的进程信息（**比之前多了Master进程**）：

```
hadoop@master1:/usr/local/spark-3.3.1-bin-hadoop3$ jps
7314 NameNode
8217 Jps
8154 Master
7676 ResourceManager
```

去slave机器上看jps（**多了worker进程**）：

```
hadoop@slave1:/usr/local/spark-3.3.1-bin-hadoop3$ jps
4053 NodeManager
4362 Worker
3806 DataNode
4479 Jps
3935 SecondaryNameNode

hadoop@slave2:~$ jps
4213 Jps
2645 NodeManager
4103 Worker
2508 DataNode
```

3. **访问http://192.168.123.130:8180/：**

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221212124125.png)

4. 使用命令./bin/spark-shell启动SparkContext，**再访问spark-shell web控制台页面http://192.168.123.130:4040/（**根据终端的提示来访问具体的端口，比如我这里提示了就是4041端口）：

```
22/12/12 12:44:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
```

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221212124607.png)

如果某台机器上运行多个SparkContext，它的Web端口会自动连续加一，例如4041，4042，4043等。为了浏览持久的事件日志，设置spark.eventLog.enabled就可以了。

5. **在$SPARK_HOME目录下，提交计算Pi的任务，验证Spark是否能正常工作，运行如下命令**：

```
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.11-2.3.3.jar 10
```

上面这个报错：`Error: Failed to load class org.apache.spark.examples.SparkPi.`,原因是在spark3.3.0中换名字了`spark-examples_2.12-3.3.1.jar`，自己做实验的时候最好check一下：

```
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.12-3.3.1.jar 10
```

执行之后有很多的输出，自己去check一下（输出中是否有ERROR这些；以及最后是否能输出`Pi is roughly 3.1411671411671414`）：

```
#看这个输出，他会先把tmp文件上传到hdfs文件系统中
22/12/11 21:06:22 INFO Client: Uploading resource file:/tmp/spark-a18fa3bd-5980-47f5-b514-4e86c5fcb104/__spark_conf__9027821922390663810.zip -> hdfs://master1:9000/user/hadoop/.sparkStaging/application_1670818859608_0002/__spark_conf__.zip
```

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221212130509.png)

再看一下http://192.168.123.130:4041/，有新的executor：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221212133749.png)

在slave1上跑的时候还遇到了错误：

```
22/12/11 21:06:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)
```

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20221212130946.png)

[==解决方案==](https://blog.csdn.net/weixin_39971163/article/details/100573811)：

```
在spark配置文件/conf目录下的spark-env.sh中加入下面梁行代码即可（增大core和内存值）
export SPARK_WORKER_CORES=2
export SPARK_WORKER_MEMORY=2g
```

7. 测试完成，**关闭spark**

进入Spark目录，执行：



进入Hadoop目录，执行：





#### 2.3、YARN是什么？

我们知道MapReduce1.0，是把计算框架和资源调度框架都弄在一起了，所以Master端的JobTracker会大包大揽去执行任务。存在很多问题，比如资源分配不均、单点故障会导致整个集群不可用、没办法集成多个不同的计算框架（比如Spark、Storm）。

因此，**YARN的设计思路就是把原先JobTracker的资源管理调度和监控的功能剥离出来**。

在YARN中实现，而MapReduce2.0仅仅就是做计算框架的事情。同时，YARN还可以兼容搭建多个不同的计算框架，实现同一个集群内资源和数据的共享。

YARN系统架构中有3个核心组件：**ResourceManager（全局的资源管理器）、ApplicationMaster（任务调度、监控以及容错机制）和NodeManager（节点上的代理。负责单个节点上的资源管理，处理来自于 ResourceManager和ApplicationMaster的命令）。**（感兴趣的同学可以看一下，下面的参考文献：[一起揭开 YARN 的神秘面纱](https://cloud.tencent.com/developer/article/1890805)）

<font color='red' size='5'>YARN工作流程</font>

1. 用户编写客户端应用程序向YARN提交
2. YARN中的ResourceManager负责接收和处理来自客户端的请求，为应用分配一个容器，在容器中启动一个ApplicationMaster，并且在ResourceManager内注册
3. ApplicationMaster采用轮询的方式向ResourceManager申请资源（Resource Scheduler）
4. ResourceManager以容器的形式向提出申请的ApplicationMaster分配资源
5. 在容器中启动任务（资源的二次分配）
6. 各个任务向ApplicationMaster汇报自己的状态和进度
7. 应用程序运行完成后，ApplicationMaster向ResourceManager的Application Manager注销并关闭自己，释放资源



### 3、Spark的几个重要的概念

#### 2.1、Spark的bin目录下



- spark-shell是为了验证Scala代码的解释器环境

#### 2.2、Spark的webUI上

![image-20221210151751761](C:\Users\705lab\AppData\Roaming\Typora\typora-user-images\image-20221210151751761.png)



#### 2.3、spark-submit提交任务参数





### 4、配置Scala环境

> 写在前面：很多教程在安装Spark之前会默认安装一下Scala，但是却不说原因。
>
> 其实Scala、Python、Java这些都是开发语言，只不过spark是Scala原生的，所以要安装，但是安装逻辑并不是一定要先Scala再spark。

根据上面的叙述，可见scala是一门语言，只需要进行环境变量配置即可





### 5、安装并配置anaconda（Python环境）

> 一起揭开 PySpark 编程的神秘面纱：https://cloud.tencent.com/developer/article/1883774
>
> 一起揭开 YARN 的神秘面纱：https://cloud.tencent.com/developer/article/1890805

#### 3.1、什么是PySpark？为什么要PySpark？

Spark支持很多语言的调用，包括了Java、Scala、Python等，其中用Python语言编写的Spark API就是PySpark。**PySpark在Spark最外层封装了一层Python API，借助了Py4j来实现Spark底层API的调用，从而可以实现实现我们直接编写Python脚本即可调用Spark强大的分布式计算能力。**

调用流程：

1. 用户通过实例化Python的SparkContext对象
2. 接着Py4j会把Python脚本映射到JVM中，同样地实例化一个Scala的SparkContext对象
3. 然后Driver端发送Task任务到Executor端去执行
   - 因为Task任务中可能会包含一些Python的函数，所以每一个Task都是需要开启一个Python进程，通过**Socket通信方式**将相关的Python函数部分发送到Python进程去执行。

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/RMQWW%24R4SE%5D67%7B8FATN5YAA.jpg)

综上所述，**PySpark是借助于Py4j实现了Python调用Java从而来驱动Spark程序的运行**，这样子可以保证了Spark核心代码的独立性。

但是在大数据场景下，**如果代码中存在频繁进行数据通信的操作，这样子JVM和Python进程就会频繁交互，可能会导致我们的任务失败**。

所以，如果面对大规模数据还是需要我们使用原生的API来编写程序（Java或者Scala）。但是对于中小规模的，比如TB数据量以下的，可以直接使用PySpark来开发。



### 6、参考：

> Spark的安装及配置：https://segmentfault.com/a/1190000019472239
>
> Ubuntu - 安装Spark3.0.0：https://www.cnblogs.com/gwzz/p/13304520.html
>
> Spark 编程指南简体中文版：https://doc.yonyoucloud.com/doc/spark-programming-guide-zh-cn/index.html
>
> Spark on Yarn配置（详细）：https://www.cnblogs.com/luengmingbiao/p/12985143.html
>
> Spark教程（三）—— 安装与使用：https://blog.csdn.net/m0_37565948/article/details/105611658
>
> spark-3.0.1安装超详细教程(一)：https://blog.csdn.net/qq_44665283/article/details/121031190
>
> 一起揭开 PySpark 编程的神秘面纱：https://cloud.tencent.com/developer/article/1883774
>
> 一起揭开 YARN 的神秘面纱：https://cloud.tencent.com/developer/article/1890805
>
> Spark源码编译 + 伪分布式搭建 + Hive on Spark配置：https://www.cnblogs.com/luengmingbiao/p/11216383.html



# 总结：

## 在安装过程中几个重要的节点：

对于master：

1. 完成IP配置
1. hadoop配置-更新机器名
1. 配置免密
1. 配置hadoop的conf配置
1. 配置spark local
1. 配置spark yarn
1. 配置anaconda（python环境整合）



对于slave：

1. 完成IP配置
2. hadoop配置-更新机器名
3. 配置免密
4. 后面的步骤可以直接拷贝master机器上的配置
   1. 配置hadoop的conf配置
   2. 配置spark local
   3. 配置spark yarn
   4. 配置anaconda（python环境整合）




## 一些注意点：

1. 要注意分别每次命令是在root用户下，还是在普通hadoop用户下执行的
1. 关于配置文件的问题，很多属性需要了解之后再去做配置是比较合理的，不然直接配置会让人摸不着头脑
1. 对于linux的操作要熟悉，比如chmod，chown，scp，mv这些操作，还要基础的ssh免密配置这些，都要熟悉
1. 一个感悟：spark的配置真的很麻烦，不向Java开发这些，只用几个环境变量的配置之后，很多东西在idea中就可以集成使用了。作为第一次上手的新人，每个步骤最好都细细地想一下原因。说个增强自信心的东西（这篇blog记录了很多），我整个配置环境满打满算花了**将近一周**的时间，╮(╯﹏╰）╭╮(╯﹏╰）╭╮(╯﹏╰）╭
