---
layout:     post
title:     最基本的数据抽象-RDD(Resilient Distributed Dataset)
subtitle:   RDD数据结构、RDD五大特性、RDD的操作
date:       2023-02-14
author:     ldf
header-img: img/post-bg-spark04.png
catalog: true
tags:
    - Spark
---

# 一、RDD数据结构

> Spark 的核心是建立在统一的抽象弹性分布式数据集（Resiliennt Distributed Datasets，RDD）之上的，这使得 Spark 的各个组件可以无缝地进行集成，能够在同一个应用程序中完成大数据处理。
>
> 代表了一个**不可变**(immutable)、**可分区**(partitioned)、里面的元素**可并行计算**(parallel)的集合。
>
> 和Mysql数据库中的视图view概念类似，其本身不存储数据，仅作为数据访问的一种虚拟结构。

从名字中去理解：

- Resilient：RDD中的数据可以存储在内存中或者磁盘中
- Distributed：RDD中的数据是分布式存储的，可用于分布式计算；说明RDD的数据是跨机器/进程的
- Dataset：一个数据集合，用于存放数据的

## 1、为什么需要RDD？

我们想要完成一个分布式计算的话，需要这些东西：

- 分区控制
- Shuffle控制
- 数据存储、序列化、发送
- 数据计算API

等一系列功能。

但是这些功能，不能简单地通过Python内置的本地集合对象（如 List、字典等）去完成。

没有RDD之前：

1. MR：只提供了map和reduce的API，而且编写麻烦，运行效率低！
   - 不再适用于现在大规模数据处理，机器学习这种迭代式的算法场景。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作
   - 对于交互式数据挖掘，MR显然不擅长

2. 使用Python/Scala/Java的本地集合：但是只能完成本地单机版的，如果要实现分布式的，需要很多机制的配合

因此，我们**在分布式框架中，需要有一个统一的数据抽象对象**，来实现上述分布式计算所需功能，这个抽象对象就是 RDD。





# 二、Spark RDD的核心特性

## 1、RDD的五大特性

RDD 数据结构内部有五个特性，前三个特征每个RDD都具备的，后两个特征可选的。

### 1). A list of partitions（RDD是有分区的）

> RDD是有分区的；RDD的分区是RDD数据存储的最小单位。

对于RDD来说，每个分区都会被一个计算任务处理，分区数决定并行度；

用户可以在创建RDD时指定RDD的分区个数，例如：

```
rdd = sc.parallelize([1,2,3,4,5,6,7,8,9],3)	# 指定三个分区
# glom函数 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变
rdd.glom().collect()	# 三个分区:[[1,2,3],[4,5,6],[7,8,9]]
```

如果没有指定，那么就会采用默认值；

### 2). A function for computing each split

> 计算方法都会作用到每一个分片(分区)之上

例证：

```shell
sc.parallelize([1,2,3,4,5,6,7,8,9],3).glom().collect()	# 输出:[[1,2,3],[4,5,6],[7,8,9]]
sc.parallelize([1,2,3,4,5,6,7,8,9],3).map(lambda x:x*10).glom().collect()	# 输出:[[10,20,30],[40,50,60],[70,80,90]]
```

如上，RDD的3个分区，在执行了map操作以后，都将数据乘以10了。可见，map操作作用到了所有的分区之上。

这个也好理解为什么这么设计。很明显，在分布式的环境中，我们不希望操作是只针对单个设备/分区，或者要手动指定分区，默认情况下就应该是全选，减少developer的开销。

### 3). A list of dependencies on other RDDs

> 每一个RDD都会和其他的RDD有依赖（血缘）关系，或者说，RDDs会有一个依赖链条。

操作确定了 RDD 之间的依赖关系，例如：

```python
rdd1 = sc.textFile("../t.txt")
rdd2 = rdd1.flatMap(lambda x:x.split(' '))
rdd3 = rdd2.map(lambda x:(x,1))
rdd4 = rdd3.reduceByKey(lambda a,b:a+b)
print(rdd4.collect())
```

在上面的操作中，存在如下的依赖关系链：`textFile -> rdd1 -> rdd2 -> rdd3 -> rdd4`

在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算**（这也就是Spark的容错机制）**

### 4). Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)

> K-V型RDD是可以有一个分区器，即RDD的分区函数。
>
> 注意，这里是分区器，不是说分区。

Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。

当前Spark中实现了两种类型的分区函数，一个是**基于哈希的HashPartitioner**（默认），另外一个是**基于范围的RangePartitioner**。也可以手动设置一个分区器（rdd.partitionBy的方法）

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230215223718.png)

比如上面这个图，根据hash分区规则，相同的key会被hash到相同的分区上。这一行为会在groupBy，reduceBy等操作中表现出来。

同样的，上面的WordCount中也有分区器的存在（红框标出）：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230215225758.png)



### 5). Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)

> RDD分区数据的读取会尽量靠近数据所在地；或者说，“移动数据不如移动计算”，这样读取效率最高，因为只需要磁盘I/O，不需要网络的I/O。
>
> - 本地读取：Executor所在的服务器，同样是一个DataNode；同时这个DataNode上有它要读的数据，所以可以直接读取机器硬盘即可
> - 网络读取：数据需要经过网络传输才能读取到

在**初始化RDD**(读取数据的时候)规划的时候，分区会尽量规划到 存储数据所在的服务器 上。

那机器怎么知道数据是存在哪台服务器上的呢？

- 有Application Master，它记录了数据存放在哪里；它之所以会有记录，是因为Driver和RM之间有通讯

在Spark的Web UI监控页面中可以看到，这个数据是否是本地读取的。



## 2、RDD的核心特点

- 内存计算
- 惰性计算
- 容错
- 不可变性
- 分区
- 持久性
- 粗粒度操作
- 位置粘性

这些特性都是RDD具体操作的时候才会遇到，比如操作Transformation的时候是Lazy的。在这里展开会不知所云，所以在下一节中逐步讨论这些问题。

## 3、RDD血缘关系 - 依赖

上面我们已经讲过了，RDD的第三特性就是RDDs之间存在依赖关系，但是只是简单说明。其实，RDD之间的依赖关系有两种类型，即**窄依赖**和**宽依赖**。

- **窄依赖**时，父 RDD 的分区和子 RDD 的分区的关系是一对一或者多对一的关系（反过来看，就是一个父RDD最多只能对应一个子RDD）
- **宽依赖**时，父 RDD 的分区和子 RDD 的分区是一对多或者多对多的关系
  - 宽依赖关系相关的操作一般具有 shuffle 过程，即通过一个 Patitioner 函数将父 RDD 中每个分区上 key 不同的记录分发到不同的子 RDD 分区。（这个很好理解，shuffle过程就是要把映射打乱，让父RDD扩散到不同的子RDD上）

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230216102339.png)

### 1). 窄依赖

1. 子 RDD 的每个分区依赖于常数个父分区（即与数据规模无关)。

2. 输入输出一对一的算子，且结果 RDD 的分区结构不变，如 map、flatMap。

3. 输入输出一对一的算子，但结果 RDD 的分区结构发生了变化，如 union。

4. 从输入中选择部分元素的算子，如 filter、distinct、subtract、sample。

### 2). 宽依赖

1. 子 RDD 的每个分区依赖于所有父 RDD 分区
2. 对单个 RDD 基于 Key 进行重组和 reduce，如 groupByKey、reduceByKey
3. 对两个 RDD 基于 Key 进行 join 和重组，如 join



### 3). 依赖关系确定了 DAG 切分成 Stage 的方式

> 在对 Job 中的所有操作划分 Stage 时，一般会按照**倒序**进行。即从 Action 开始，遇到窄依赖操作，则划分到同一个执行阶段，遇到宽依赖操作，则划分一个新的执行阶段。
>
> 后面的 Stage 需要等待所有的前面的 Stage 执行完之后才可以执行，这样 Stage 之间根据依赖关系就构成了一个大粒度的 DAG。

**切割规则：**从后往前，遇到宽依赖就切割 Stage

1. RDD 之间的依赖关系形成一个 DAG 有向无环图，DAG 会提交给 DAGScheduler
2. DAGScheduler 会把 DAG 划分成相互依赖的多个 stage，划分 stage 的依据就是 RDD 之间的宽窄依赖。
3. 遇到宽依赖就划分 stage,每个 stage 包含一个或多个 task 任务。然后将这些 task 以 taskSet 的形式提交给 TaskScheduler 运行。

**DAG是有向无环图，代表着RDD的转换过程，其实就是代表着数据的流向。**

1. DAG是有边界的，有开始，有结束
2. 通过sparkContext创建RDD就是开始，触发action就会生成一个完整的DAG
3. DAG会被切分为多个Stage（阶段），切分的依据就是宽依赖（Shuffle），会先提交前面的stage，然后提交后面的stage，一个stage中根据最后一个RDD的分区出划分出多个Task，多个Task可以并行执行

# 三、RDD 基本操作

> Spark RDD编程的程序入口对象是SparkContext，而他最大的作用就是创建出第一个RDD出来。

## 1、RDD的创建

RDD的创建主要有2种方式：

1. 并行化集合创建（本地对象 转 分布式RDD）
2. 读取外部数据源

这种方式可能不太好理解，换个从数据源角度来分析：

1. 从**内存**里直接读取数据
2. 从**文件系统**中读取数据（文件种类众多，最常见的就是HDFS）

第一类方式是从内存里构造 RDD，需要使用 makeRDD 方法，代码如下所示：

```scala
val rdd01 = sc.makeRDD(List(l,2,3,4,5,6))
```

这个语句创建了一个由“1,2,3,4,5,6”六个元素组成的 RDD。

第二类方式是通过文件系统构造 RDD，代码如下所示：

```scala
val rdd:RDD[String] == sc.textFile("file:///D:/sparkdata.txt",1)	#这里例子使用的是本地文件系统，所以文件路径协议前缀是 file://
val rdd:RDD[String] == sc.textFile("hdfs://node:8020/home/sparkdata.txt",1)	#这里例子使用的是本地文件系统，所以文件路径协议前缀是 hdfs://
```

textFile方法：

-  第一个参数是传入的文件路径
- 第二个参数是最小的分区数量，可选
  - 并不代表是最终的分区数量；Spark会自己判断，在设备允许的范围内，参数2才有效果；

还有一个适合**小文件读取**的API——**wholeTextFiles**，他和textFiles有如下区别：

1. sc.textFiles(path) 能将path 里的所有文件内容读出，以文件中的每一行作为一条记录的方式，而**sc.wholeTextFiles()**返回的是[(K1, V1), (K2, V2)...]的**二元组形式**，其中K是文件路径，V是文件内容
   - 其中，V 将不再是 list 的方式将文件每行拆成一个 list的元素, 而是将整个文本的内容以字符串的形式读进来，也就是说`value = '...line1...\n...line2...\n'`；这时需要你自己去拆分每行
2. 用textFile时，它的partition的数量是与文件夹下的**文件数量**（实例中用3个xxx.log文件）相关，一个文件就是一个partition(既然3个文件就是：partition=3）。
   - **特别提醒**：这里的测试数据是几十byte，较小，如果每个文件较大，要根据相应切分原则切分
3. **wholeTextFiles**的partition数量是根**据用户指定或者文件大小来**（文件内的数据量少 有hdfs源码默认确定的）确定，与hdfs目录下的文件数量**无关**！ 所以说：**wholeTextFile通常用于读取许多小文件的需求**





## 2、RDD持久化

> 持久化存储是Spark非常重要的一个特性，通过持久化存储，提升Spark应用性能，以更好地满足实际需求。

### 1). RDD是过程化数据

RDD的计算是迭代的，当执行开启后，新的RDD会生成，与此同时，**老的RDD就会消失**。

**RDD 是惰性求值的**，而有时候希望能多次使用同一个 RDD。如果简单地对 RDD 调用行动操作，Spark 每次都会重算 RDD 及它的依赖，这样就会带来太大的消耗。为了避免多次计算同一个 RDD，可以让 Spark 对数据进行持久化。

Spark提供了 **persist** 和 **cache** 两个持久化函数，其中cache将RDD持久化到内存中，而persist则支持多种存储级别。两个持久化方法的主要区别是：cache()方法默认使用的是内存级别，其底层调用的是persist()方法。

被缓存的 RDD 被使用时，存取速度会被大大加速。一般情况下，Executor 内存的 60% 会分配给 cache，剩下的 40％ 用来执行任务。

缓存是容错的，如果一个 RDD 分片丢失，则可以通过构建它的转换来自动重构。

### 2). RDD持久化存储级别

Spark的提供了多种持久化级别，比如内存、磁盘、内存+磁盘等。具体来说，包括以下几种：

- MEMORY_ONLY

默认，表示将RDD作为反序列化的Java对象存储于JVM中，如果内存不够用，则部分分区不会被持久化，等到使用到这些分区时，会重新计算

- MEMORY_AND_DISK

将RDD作为反序列化的Java对象存储在JVM中，如果内存不足，超出的分区将会被存放在硬盘上

- MEMORY_ONLY_SER

将RDD序列化为Java对象进行持久化，每个分区对应一个字节数组。此方式比反序列化要节省空间，但是会占用更多CPU资源

- MEMORY_AND_DISK_SER

与MEMORY_ONLY_SER类似，如果内存放不下，则溢写到磁盘

- DISK_ONLY

将RDD的分区数据存储到磁盘

- MEMORY_ONLY_2，MEMORY_AND_DISK_2

与上面的方式类似，但是会将分区数据复制到两个集群

- OFF_HEAP(experimental)

与MEMORY_ONLY_SER类似，将数据存储到堆外内存off-heap，需要将off-heap开启

**在计算资源和存储资源开销上，常见级别中有这样的对比**：

| 级别                | 使用空间 | CPU时间 | 是否在内存 | 是否在磁盘 |
| ------------------- | -------- | ------- | ---------- | ---------- |
| MEMORY_ONLY         | 高       | 低      | 是         | 可能       |
| MEMORY_AND_DISK     | 高       | 中      | 部分       | 部分       |
| MEMORY_ONLY_SER     | 低       | 高      | 是         | 否         |
| MEMORY_AND_DISK_SER | 低       | 高      | 部分       | 部分       |
| DISK_ONLY           | 低       | 高      | 否         | 是         |

### 3). RDD的Checkpoint机制

除了cache和persist之外，Spark还提供了另外一种持久化：checkpoint，它能将RDD写入**分布式文件系统**，提供类似于数据库快照的功能;

它提供了一种相对而言更加可靠的数据持久化方式，把数据保存在分布式文件系统，**比如HDFS上**。这里就是利用了HDFS高可用性，高容错性（多副本）来最大程度保证数据的安全性。

和Cache的区别：

|          | Cache                                          | CheckPoint                                      |
| -------- | ---------------------------------------------- | ----------------------------------------------- |
| 开销     | 轻量                                           | 重量                                            |
| 存储位置 | 内存 or 硬盘                                   | 硬盘(HDFS)                                      |
| 安全性   | 不安全（不保留RDD血缘关系）                    | 安全（保留RDD血缘关系；HDFS多副本机制保证容错） |
| 性能     | 优（分散存储，各Executor并行执行，可在内存中） | 劣（集中存储，需网络I/O）                       |



### 4). 如何选择RDD持久化策略

主要是为了在CPU和内存消耗之间进行取舍，可以根据实际情况来选择持久化级别：

1. 优先使用MEMORY_ONLY，如果可以缓存所有数据的话，那么就使用这种策略。因为纯内存速度最快，而且没有序列化，不需要消耗CPU进行反序列化操作
2. 如果MEMORY_ONLY策略，无法存储的下所有数据的话，那么使用MEMORY_ONLY_SER，将数据进行序列化进行存储，纯内存操作还是非常快，只是要消耗CPU进行反序列化**（速度优先**）
3. 如果需要进行快速的**失败恢复**，那么就选择**带后缀为_2**的策略，进行数据的**备份**，这样在失败时，就不需要重新计算了
4. **能不使用DISK相关的策略，就不用使用，有的时候，从磁盘读取数据，还不如重新计算一次**

### 5). DStream持久化

除了RDD持久化的使用方式，其实DStream也是支持持久化的，同样是使用persist()与cache()方法。

**与RDD的持久化不同，DStream的默认持久性级别将数据序列化在内存中。**

持久化通常在有状态的算子中使用，比如窗口操作，默认情况下，虽然没有显性地调用持久化方法，但是底层已经帮用户做了持久化操作。



# 四、RDD 编程算子 - 概念和分类

> 不同于 MapReduce 仅支持 Map 和 Reduce 两种编程算子，Spark 提供了超过 80 种不同的 **Transformation** 和 **Action** 算子，如map, reduce, filter, groupByKey, sortByKey, foreach 等，并且采用函数式编程风格，实现相同的功能需要的代码量极大缩小。
>

## 1、转换算子 - Transformation

RDD 的转换操作是返回新的 RDD 的操作。**转换出来的 RDD 是惰性求值的**，只有在行动操作中用到这些 RDD 时才会被计算。

许多转换操作都是针对各个元素的，也就是说，这些转换操作每次只会操作 RDD 中的一个元素，不过并不是所有的转换操作都是这样的。

常用的RDD转换算子：

假设`rdd1 ={1,2,3,4}, rdd2 = {3,4,5}`

| 函数名                                       | 作用                                                         | 示例                                                         | 结果                             |
| -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------- |
| map()                                        | 将函数应用于RDD的每个元素，返回值是新的RDD                   | rdd1.map(x=>x+I)                                             | {2,3,4,4}                        |
| flatMap()                                    | 对RDD执行map操作，然后进行**解除嵌套**的操作                 | rdd1.flatMap(x=> x.to(3))<br />to函数相当于获取[x,3]之间的所有数字 | {1,2,3,2,3,3,3}                  |
| groupBy()                                    | 将rdd的数据进行分组;(输出是一个二元组,value值是一个地址，还需要map转换) | rdd1.groupBy(lambda num:'even' if (num % 2 ==0) else 'odd').map(lambda x:(x[0],list(x[1]))).collect()<br />另一种用mapValue来做转换：rdd1.groupBy(lambda num:'even' if (num % 2 ==0) else 'odd').mapValue(lambda x:list(x)).collect() | [('even',[2]),('odd',[1,3,3])]   |
| filter()                                     | 函数会过滤掉不符合条件的元素，返回值是新的RDD                | rdd1.filter(x=>x!=1)                                         | [2,3,3]                          |
| distinct()                                   | 将RDD里的元表进行去重操作                                    | rdd1.distinct()                                              | [1,2,3]                          |
| union()                                      | 生成包含两个RDD所有元素的新的RDD                             | rdd1.union(rdd2)                                             | [1,2,3,3,3,4,5]                  |
| intersection()                               | 求出两个RDD的共同元素                                        | rdd1.intersection(rdd2)                                      | [3]                              |
| subtract()                                   | 将原RDD里和参数RDD里相同的元素去掉                           | rdd1.subtract(rdd2)                                          | {1,2}                            |
| cartesian()                                  | 求两个RDD的笛卡儿积                                          | rdd1.cartesian(rdd2)                                         | [(1,3),(1,4),....,(3,5)]         |
| glom()                                       | 查看分区                                                     | rdd1.glom()                                                  |                                  |
| sortBy(func,ascending=False,numPartions=xxx) | 对rdd元素排序**（对二元组也同样适用）**                      | rdd.sortBy(lambda x:x[1],ascending=True,numPartitions=3)<br />注意：如果要求全局有序，分区数应该设置为1 | 对rdd中元素的value值按升序排列； |

对于K-V型二元组数据而言，有几个常用的操作：

例如，`rdd = sc.parallelize([('a',1),('a',11),('a',6),('b',3),('b',5)]); rdd2 = sc.parallelize([('c',6)])`

| 函数名            | 作用                                          | 示例                                                         | 结果                                            |
| ----------------- | --------------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------- |
| mapValues(func)   | 对二元组内部的value执行map操作                | rdd.mapValues(lambda x:x * 10) <br />相当于rdd.map(lambda x:(x[0],x[1] *10)) | [('a',10),('a',110),('a',60),('b',30),('b',50)] |
| groupByKey(func)  | 对二元组按key进行分组                         | rdd.groupByKey().map(lambda x:(x[0],list(x[1])))             | [('a',[1,11,6],['b',[3,5]])]                    |
| reduceByKey(func) | 对二元组按key分组后，用func逻辑进行聚合       | rdd.reduceByKey(lambda x,y:x+y)                              | [('a',18),('b',8)]                              |
| join()            | 对二元组的执行类似于SQL的内/外连接(按key关联) | rdd2.join(rdd)<br />rdd2.leftOuterJoin(rdd)<br />rdd2.rightOuterJoin(rdd) |                                                 |



## 2、行动算子 - Action

行动操作用于执行计算并按指定的方式输出结果。行动操作接受 RDD，但是**返回非 RDD**，即输出一个值或者结果。

假设`rdd={1,2,3,3}`

| 函数名                             | 作用                                                         | 示例                                                         | 结果                 |
| ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------- |
| collect()                          | 将RDD各个分区内的数据统一收集到Driver中；返回RDD的所有元素（list集合） | rdd.collect()<br /># 注意：这个算子是将RDD各个分区数据都拉取到Driver，如果数据量很大，可能会把Driver的内存撑爆 | {1,2,3,3}            |
| takeSample(param1, param2, param3) | 随机抽样RDD数据；<br />参数1(True/False)表示是否取重复数据；<br />参数2表示抽样个数；<br />参数3表示随机数种子(如果是同一个数字，那取出的结果肯定一致；一般不传，Spark会自动生成) | rdd.takeSample(False,2)                                      | [1,4]                |
| count()                            | RDD里元表的个数                                              | rdd.count()                                                  | 4                    |
| countByValue()                     | 各元素在RDD中的出现次数(**对一元组和二元组都适用**)          | rdd.countByValue()                                           | {(1,1),(2,1),(3,2)}  |
| countByKey()                       | 对RDD元素中的key进行计数                                     | rdd.countByKey()                                             |                      |
| take(num)                          | 从RDD中返回num个元素                                         | rdd.take(2)                                                  | {1,2}                |
| first()                            | 从RDD中返回第一个元素                                        | rdd.first()                                                  | 1                    |
| top(num)                           | 从RDD中，按照默认(降序)返回最前面的num个元素                 | rdd.top(2)                                                   | {3,3}                |
| takeOrdered(param1,func)           | 对RDD元素按照func规则排序（默认是升序）<br />参数1：获取数据个数<br />参数2：对排序数据进行更改(不会更改数据本身，只作用在排序的时候) | rdd.takeOrdered(3,lambda x:-x)<br /># 将排序的时候数字转换成负数，那原本正数最大变成了最小，实现了降序排列 | [3,3,2]              |
| reduce()                           | 并行**整合**所有RDD数据，如求和操作，**返回值不是RDD**       | rdd.reduce((x,y)=>x+y)                                       | 9                    |
| fold()(func)                       | 和reduce()功能样，但需要提供初始值；这个初始值会参与计算；**这个操作既会在分区内聚合，也会在分区间聚合** | rdd.fold(6)((x,y)=>x+y)                                      | 15<br /># 15 = 9 + 6 |
| foreach{func}                      | 对RDD的每个元表都使用特定的函数；他**不需要把所有的数据都发送给Driver之后再输出**(例如：collect，降低了开销，速度提升)，而是直接在client的容器中输出 | rdd.foreach(println)                                         | 打印每个元素         |
| savaAsTextFile{path}               | 将数据集的元表，以文本的形式保存到文件系统中；与foreach一样，不需要把数据发给Driver，直接由Executor输出到文件(所以文件数会和分区数一样) | rdd.saveAsTextFile(hdfs://home/test)                         |                      |
| saveAsSequenceFile{path}           | 将数据集的元表，以顺序文件格式保存到指定的日录下             | rdd.saveAsSequenceFile(hdfs://home/test)                     |                      |

和分区Partition相关的函数：

| 函数名                      | 作用                                                         | 示例                                                         | 结果                                                         |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| mapPartitions(func)         | 类似于map，但独立地在RDD的每一个分片上运行；假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,**一个函数一次处理所有分区**。 | rdd.mapPartitions(datas => { datas.map(date=>date*2) })      | 所有数据乘以2，                                              |
| foreachPartition()          | 和foreach一样的效果，只不过是针对整个分区                    | rdd.foreachPartition(println)                                | 按分区打印                                                   |
| partitionBy(param1, param2) | 对RDD自定义分区操作<br />参数1：重新分区后有几个分区<br />参数2：自定义规则func(返回值一定是int类型) | rdd.partitionBy(3,process)<br />def process(k):<br />if 'hadoop'==k: return 0<br />if 'spark' == k: return 1<br />return 2 | 'hadoop'在第一个分区；'spark'在第二个分区；其他的key都在第三个分区（函数要和分区数对应好） |
| repartition(num)            | 重新修改分区数，可增加（**不建议**）也可减少；<br />底层调用的是**coalesce()**算子 |                                                              |                                                              |

说明：

**mapPartitions**：

- 当数据量不太大的时候，可以用mapPartitions，可以提高运行效率；当数据量太大的时候，有可能会发生oom（因为一次网络I/O的数据量太大）
- 与map相比，降低了任务传递的次数，减少了占用 Executor 的数量，降低了网络 IO，但是如果分区内数据量过大，对每一个 Executor 都有压力
- 扩展：mapPartitionsWithIndex则是带上分区下标进行操作

**repartition**：

- 对分区数量进行更改的时候，一定要<font color='red'>慎重</font>
- 一般情况下，只在要求全局排序的时候设置分区数为1，其他情况都是建议由spark自身进行分区数管理
- **分区数增加，极大可能导致shuffle**，这会增加磁盘I/O
- 分区数变化，会影响并行计算（内存迭代的并行管道数量）



> 题外话，其他的小Tips: 
>
> CR时应该注意的部分：
>
> - **设计**：代码是否设计良好？这种设计是否适合当前系统？
> - **功能**：代码实现的行为与作者的期望是否相符？代码实现的交互界面是否对用户友好？
> - **复杂性**：代码可以更简单吗？如果将来有其他开发者使用这段代码，他能很快理解吗？
> - **测试**：这段代码是否有正确的、设计良好的自动化测试？
> - **命名**：在为变量、类名、方法等命名时，开发者使用的名称是否清晰易懂？
> - **注释**：所有的注释是否都一目了然？
> - **代码样式**：所有的代码是否都遵循代码样式？
> - **文档**：开发者是否同时更新了相关文档？
>
> **CR时候的代码行数**：提交Code Review的代码行数最好在400行以下。
>
> - 根据数据分析发现，从代码行数来看，超过400行的CR，缺陷发现率会急剧下降
> - 从CR速度来看，超过500行/小时后，Review质量也会大大降低，一个高质量的CR最好控制在一个小时以内
>
> 改进方案：
>
> - **善用工具**：IDEA打开编码规约实时检测，减少代码样式、编码规约等基础性问题
>   （阿里编码规约插件：https://github.com/alibaba/p3c/tree/master/idea-plugin）
>
> **举个例子：**
>
> 反例：
>
> ```java
> 
> private BillVO convertBillDTO2BillVO(BillDTO billDTO) {
>     if (billDTO == null) {
>         return null;
>     }
>     BillVO billVO = new BillVO();
>     Money cost = billDTO.getCost();
>     if (cost != null && cost.getAmount() != null) {
>         billVO.setCostDisplayText(String.format("%s %s", cost.getCurrency(), cost.getAmount()));
>     }
>     Money sale = billDTO.getSale();
>     if (sale != null && sale.getAmount() != null) {
>         billVO.setSaleDisplayText(String.format("%s %s", sale.getCurrency(), sale.getAmount()));
>     }
>     Money grossProfit = billDTO.getGrossProfit();
>     if (grossProfit != null && grossProfit.getAmount() != null) {
>         billVO.setGrossProfitDisplayText(String.format("%s %s", grossProfit.getCurrency(), grossProfit.getAmount()));
>     }
>     return billVO;
> }
> ```
>
> 正例：
>
> ```java
> 
> private static final String MONEY_DISPLAY_TEXT_PATTERN = "%s %s";
> 
> private BillVO convertBillDTO2BillVO(BillDTO billDTO) {
>     if (billDTO == null) {
>         return null;
>     }
>     BillVO billVO = new BillVO();
>     billVO.setCostDisplayText(buildMoneyDisplayText(billDTO.getCost()));
>     billVO.setSaleDisplayText(buildMoneyDisplayText(billDTO.getSale()));
>     billVO.setGrossProfitDisplayText(buildMoneyDisplayText(billDTO.getGrossProfit()));
>     return billVO;
> }
> 
> private String buildMoneyDisplayText(Money money) {
>     if (money == null || money.getAmount() == null) {
>         return StringUtils.EMPTY;
>     }
>     return String.format(MONEY_DISPLAY_TEXT_PATTERN, money.getCurrency(), money.getAmount().toPlainString());
> }
> ```



## 3、提交代码到YARN集群

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20230216211351.png)
