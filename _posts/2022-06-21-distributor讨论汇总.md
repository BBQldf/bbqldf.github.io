---
layout:     post
title:     多人音视频项目优化-讨论汇总
subtitle:   基础知识、扩展学习
date:       2022-06-21
author:     ldf
header-img: img/post-bg-vedio.jpg
catalog: true
tags:
    - 音视频项目重构
---

# 多人上线推送服务-讨论汇总

## 一、背景

多人上线推送服务接收来自分发服务推送的上线记录，然后查询用户所在群、讨论组、组织等的通话记录，下发对应的s2c消息。

代码位置：https://git.woa.com/g_QQRTC/qq_mav_login_push_distributer/blob/master/src/distributer.cpp

代码流程：

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220624102909.png)



### 现存问题：

1. distributor.cpp代码**只能服务于单机**，不支持多机部署。（同样的代码，在不同机器运行的效果一样，也就没有意义）
2. 没有容灾机制，如果机器挂掉之后，分发服务就挂了（可以考虑主从机制解决）
3. 单机内部运行distributor.cpp，fork()子进程的数目是**固定**的，原代码是fork()三个，通过共享内存方式通信。
4. 单机内部运行distributor.cpp时，不宜创建过多的子进程，需要考虑login_client（也是多进程模式）也要占用到很大比例的cpu负载。

## 二、上线推送分发器改进思路

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220624102857.png)

### 1、方案设计

~~基本想法：带虚拟节点的权重一致性hash（Ketama算法） + 权重随机（轮盘赌实现）~~（待确定，参考北极星的策略）

1. 定义服务器列表（每个服务器都有自己的ip）——init阶段
   - 在北极星进行注册，让ip与机器进行关联；每个服务器这个时候就可以从北极星上获取到全部服务器的ipList。

有了这个list，才能继续执行distributor代码逻辑（每个服务器上的distributor代码一样）：

1. 计算出本服务器的hash值（mod2^32，不是mod服务器个数，让服务器映射到hash环上），添加到TreeMap中（TreeMap中元素是从小到大的顺序排列的；key是虚拟节点，value是真实节点）

   - 带虚拟节点后，也可以加上权重；权重越多，虚拟节点也越多；

2. 计算用户数据key的hash值

3. 在map中取出大于此hash值列表

   - 有；则在列表中按权重随机取

   - 无；则取出map的第1~N个节点，然后权重随机算法选取



### 2、两个核心问题：数据缺失和重复

1. **数据缺失：**当一个distributorA服务器挂掉之后，其他的服务器没有及时地感知到A挂掉，他们并没有及时地更新转发链表，导致原本A服务器上的数据没有分发器去承载。（只是暂时，在其他服务器更新好之后就可以继续操作）
   - **造成的影响**：在A挂掉之后和更新列表之前这一段时间中的属于字段A的数据缺失，即这一段时间的这部分用户无法即使推送给用户——>评估：用户上线的消息丢失了，下游的服务不能响应；但是这些服务都是通知类型的，不会对用户的可用性造成影响
   - 另一方面，数据传输中可能本身就有丢失的情况
2. **数据重复：**现有一台新的分发机器distributorC加入到分发集群中，他将会开始转发属于字段C的用户列表；但是原本服务器A和B已经带有字段C，所以字段C上的数据被重复发送给消息推送服务器；
   - 举例：有0~9的字段，在只有A和B机器的时候，A负责双数（Xmod2==0），B负责单数（Xmod2==1）;当C机器加入以后，A要变成（Xmod3==0，即0,3,6,9），B要变成（Xmod2==1，1,4,7）；C要变成（Xmod3==2,即2,5,8）；但是A和B还没更新服务器list，所以仍然按单双数的规则分发，而C已经算出新的，将会重复转发2,5,8。理想情况下，数据被重复转发的比例为1/3
   - 造成的影响：在C启动之后和列表更新之前这一段时间中属于C字段的数据重复，即这一段时间内的这部分用户会重复收到来自群提示的消息——>评估：用户上线的消息堆积，下游的服务会消费两次（这里是不是可以考虑可重入？）；但是这些服务都是通知类型的，多次通知不会对用户可用性造成影响（意味着在多次执行的时候能得到正确的值，满足可重入性的定义）

### 3、方案优势

> 针对上面的两个问题，其实在代码层，就是有一个问题要回答：
>
> 为什么要用一致性hash，不用简单的源地址哈希（通过哈希函数计算得到一个数值，用该数值对服务器节点数进行取模，得到的结果便是要访问节点序号）

#### 1、可用性

> 这体现在服务器故障的时候

对于源地址hash，如果某个节点出现故障，会导致这个节点上的客户端无法使用，无法保证高可用。当然，这个方案可以在我们的算法框架中改进，因为这里有个北极星名字服务，各个服务器可以每隔一段时间check还存活的服务器，这时候取模就是实时调整的，也不会有太大问题。

#### 2、数据稳定性

> 这体现在服务器新增的时候

以上面的例子为例，有0~9的字段，在只有A和B机器的时候，A负责双数（Xmod2==0），B负责单数（Xmod2==1）;当C机器加入以后，A要变成（Xmod3==0，即0,3,6,9），B要变成（Xmod2==1，1,4,7）；C要变成（Xmod3==2,即2,5,8）；但是A和B还没更新服务器list，所以仍然按单双数的规则分发，而C已经算出新的，将会重复转发2,5,8。理想情况下，数据被重复转发的比例为1/3。

除了出现数据重复以外，可以看到A和B处理的数据也变了。对于一个消费者程序而言，他读到一块数据（一长串数据）之后，正在消费过程中（比如，一个buffer，里面包含了5条数据，已经消费了2条），但是这时候更新服务器列表，A的策略要马上调整，那剩余的三条数据怎么处理？A并不好决定，因为A上要处理的数据已经完全发生了变化；

但是一致性hash不会出现这个问题，因为一致性hash的稳定性，这个扩容的例子会变成，A负责大于5的数，B负责小于5的数（在环上处理）；C进来之后，可能就变成了A负责5-7的数，B负责2-5的数，它的缩小是稳定的（而不是随机的）



当然，这种源地址hash方法也有一个**优点：**相同的IP每次落在同一个节点，可以人为干预客户端请求方向，例如灰度发布（前提是后端服务器集群是稳定）



### 4、几个待确认的点

1. login_client的数据是全量的，但是是流数据吗？他应该是每个ms都会有一堆用户上线吧
2. 北极星的负载均衡API调用的时候，到底算法是怎么设计的（之前的只是几个算法，需要知道它的调用细节）
3. login_client的sdk是c++的，但是我们推荐重构的方案是go；这种跨语言的解决方案是Cgo，需要了解

答（hungting-20220627）：

1. login_client的数据是流数据，每个login_client会收到全量的数据，但是本地环形队列只会保存最近一段时间数据。
2. 北极星的API调用细节，这个看官方文档和API



## 三、扩展的三种上线推送分发器改进思路

上面的思路是，用户上线的全量数据推给具体的服务；

下面的方案都是跟客户端结合在一起的方案，用户上线的时候，主动拉取对应的服务数据。即，一个用户过来，主动到各个服务模块去拉取数据。

这样就不需要上线推送这一条链路。

### 1、所有的用户状态都写到存储里面

> 写入存储的数据是QQ多人业务，上线队列login_client是后台的一种触发方式把数据推送到用户，用户上线后客户端也可以主动发起请求来拉一次数据;

用户上线后，启动一个服务实例去拉数据

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220630162017.png)

问题：

1. 怎么保证消息的重复消费（一条消息没到，用户认为丢失了，就再次请求）

2. 怎么防止消息的丢失

答：参考redis的一些解决方法



### 2、用户状态实时地更新到MQ

上游是login服务，然后要分发login_client；现在login服务直接去写一个MQ，push服务（推送服务）直接去消费MQ；

- 那这个时候，PUSH服务就相当于要合并在一起；目前我是把PUSH服务拆分成一个个的小服务模块，现在是要把PUSH服务作为一个大型的数据推送，它消费到一个用户数据，那他就把小模块都走一遍；

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220624102636.png)

问题：

1. Kafka的partitions之间是无序的，如何保证用户的顺序消费。即怎么保证先上线的先通知？

答：这里的顺序性的要求要更加细粒度地定义一下。这个用户顺序性是指的全局有顺序性的要求，但是某些维度比如单个用户的消息（群通知，设备通知等），这里的顺序性其实重要性并不大；