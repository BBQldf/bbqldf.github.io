---
title:    微服务、docker&k8s、服务治理框架ETCD
subtitle:   DevOps敏捷开发
date:       2022-07-01
author:     ldf
header-img: img/post-bg-docker01.jpg
catalog: true
tags:
    - 分布式基础
    - 面试
---


# 微服务、docker&k8s、服务治理框架ETCD

# 一、微服务的优缺点

## 1、传统服务（单体、集群）的痛点

**单体架构：**

1. 单点问题，如果宕机所有的服务都不可用；
2. 耦合性太强：所有业务的功能模块都聚集在一起，代码量多，维护麻烦；关键如果这些服务相互依赖/影响，会造成端口冲突，CPU/内存共享问题
3. 扩展成本高：假设用户模块是一个CPU密集型的模块(涉及到大量的运算)那么我们需要替换更加牛逼的CPU，而我们的订单模块是一个IO密集模块（涉及大量的读写磁盘）,那我们需要替换更加牛逼的内存以及高效的磁盘。出现明显的短板问题
4. 不稳定：线上线下的环境不一样，在线上的版本可能会进行修改、调整，打包，导致测试的代码和线上运行的代码不一致。

这种架构只适合业务稳定，迭代周期长，运行稳定，就是修修bug ，改改数据的服务

**集群架构：**

相对于单点架构，~~增加了主从机制~~（不能这么说，主从是master和slaver；集群可以包含主从），相同的服务进行了水平扩展，只是改善了单点故障问题和提高请求的并发数（突破性能瓶颈，支持动态扩展）。

但是有这么几个问题需要考虑：

> 主要分为这几个大类：用户登录登出、权限控制、业务功能、定时任务。

1. **负载均衡的问题：**应用集群需要有一个组件来管理请求的分发。--常见的如Nginx
2. **session失效问题：**登录请求被节点1处理后session存储在节点1，后续的请求分发到节点2则需要重新登录。集群环境下session需要同步。
   - 方案：Tomcat自带的session复制、spring session+redis实现分布式session
3. **定时任务执行问题：**因为有很多机器在同时服务，对于定时任务就不好管理。怎么确保不会重复执行？最简单的办法就是定时任务拆分出来，只部署一个节点。方案：分布式job框架、分布式锁实现job的分配。
4. **缓存一致性问题**：原来缓存在本地的数据，需要保证数据的一致性，实现共享，只保存一份。（比如两个节点各缓存了一份不同版本的数据，就会出现同一个页面刷新，交替展示不一样的数据直到缓存失效）。
   - Redis



## 2、微服务的优点

为了支撑日益增长的庞大业务量，业界大量使用微服务架构。服务按照不同的维度进行拆分，互联网应用构建在不同的软件模块集上，这些软件模块可能是由不同的团队开发、可能使用不同的编程语言来实现、可能布在了几千台服务器；**（核心就是解耦，服务之间实现调用）**

微服务拆分原则：

1. 避免出现双向依赖，环形依赖，避免微服务之间频繁调用
2. 服务要根据情况拆分，CPU密集型和I/O频繁的服务要拆开
3. 服务拆分之后，数据库也要跟着拆分
4. 微服务之间的调用要做好，这意味着需要统一的序列化和反序列化
5. 服务拆分粒度要适中，拆分粒度不是越细越好，粒度需要符合弓箭原理及三个火枪手原则弓箭原理就是业务复杂度越高，团队人员越多，拆分出来的微服务可以越多**三个火枪手原则是**，一个微服务2-3个人开发维护最合适；（tx确实是这样的一个状态）

## 3、微服务的缺点

> 微服务中一个请求涉及几十个服务，若其中某个关键服务出现了失败，只知道有异常，但具体的异常在哪个服务引起的就需要进入每一个服务里面看日志；（问题就是解耦还是有连接）;服务的复杂度并没有显著降低

**微服务**的好处不用多说，然而微服务也是一把双刃剑，其**坏处就是系统太复杂**：

1. **故障定位难：**一次请求往往需要涉及到多个服务，这些服务很可能是由多个团队负责的。一旦出问题，只知道有异常，但具体的异常在哪个服务引起的就需要进入每一个服务里面看日志，这样的处理效率是非常低的。最坏的情况可能要拉上多个团队一起定位。（这在企业中应该是常见的，因为一个大服务肯定是好几个组一起开发的）
2. **容量评估难：**企鹅电竞每个月都就有好几场推广活动。活动形式还经常变化，导致流量入口经常不同。企鹅电竞有500多个模块，不同入口的流量导致各模块的qps增量是不同的，容量评估是一件难事。
3. **链路梳理难：**一个新人加入后台团队，他在这个微服务体系中接手一个模块，根本不知道自己身在何处，不知道自己的系统被谁依赖了，也不知道自己的系统下游依赖哪些服务，需要看文档，一行行根据代码来分析，费时费力。
4. **性能分析难：**一个服务依赖于后台多个服务， 如果某接口的耗时突然增加，开发得从自己开始，逐步分析各依赖接口的耗时情况。

**当然，请注意，这些问题和容器化的关系并不是直接的。**这些问题只是告诉我们需要开发一系列的工具来帮助微服务进行服务治理，比如stke（类比织云）、123平台、北极星，进行服务发现与配置，服务监控和看板分析等。（这个是针对面试的时候，问微服务的问题的时候可以说的！）

另外一种说法：

1. 增加了运维人员的工作量，以前只要部署一个war包，现在可能需要部署成百上千个war包 (k8s+docker+jenkis )
2. 服务之间相互调用，增加通信成本
3. 数据一致性问题(分布式事物问题)
4. 系能监控等,问题定位

# 二、Docker

> 想要搞懂Docker，其实看它的两句口号就行:
>
> - 第一句，是“**Build, Ship and Run**”。也就是，“搭建、发送、运行”，三板斧。
> - 第二句口号就是：“**Build once，Run anywhere**（搭建一次，到处能用）”。

Docker因为其自身优势，在云原生中有着重要地位，同时，在微服务中，有着广泛的应用。

- 更高效的利用系统资源
- **资源隔离**
- 一致的运行环境
- **持续交付和部署**（过编写Dockerfile构建出一个可以直接提供服务交付件(应用+依赖环境)，只要启动容器镜像，即可提供对外服务。）
- 更轻松的迁移
- 更轻松的维护和扩展



但是这样也将研发和运维的边界模糊了：

- 之前研发只需要关注功能，性能，稳定性，可扩展性，可测试性等等。引入了镜像之后，因为要自己去写 Dockerfile，要了解这个技术依赖和运行的环境倒底是什么，应用才能跑起来，原来这些都是相应运维人员负责的。
- 研发还需要额外关注应用的可运维性和运维成本，关注自己的应用是有状态的还是无状态的，有状态的运维成本就比较高。

## 1、服务的状态

> 在了解Docker之前，先了解下，什么叫做服务/应用的有状态和无状态

对服务器程序来说，究竟是有状态服务，还是无状态服务，其判断依旧是指两个来自相同发起者的请求在服务器端是否具备上下文关系。

- 如果是状态化请求，那么服务器端一般都要保存请求的相关信息，每个请求可以默认地使用**以前的请求信息**。
  - 状态化的服务器有更广阔的应用范围，比如MSN、网络游戏等服务器。他在服务端维护每个连接的状态信息，服务端在接收到每个连接的发送的请求时，可以从本地存储的信息来重现上下文关系。这样，客户端可以很容易使用缺省的信息，服务端也可以很容易地进行状态管理。比如说，当一个用户登录后，服务端可以根据用户名获取他的生日等先前的注册信息；而且在后续的处理中，服务端也很容易找到这个用户的历史信息。
  - **有状态服务可以实现事务**，事务简单来说就是多件事情组成一个集合，集合中每件事情要全部正确完成了，这个集合才算完成。如果集合中的事情有一个没有完成，即使其他事情完成了，也得将已完成的事情的相关数据恢复到原来的状态，就是回滚。**比如说，**同一个用户的业务逻辑，多次请求的数据可以存放在session中，当另外的几个请求处理完成后，从session中取出数据完成最终的处理。
- 对于无状态请求，服务器端所能够处理的过程必须全部来自于请求所携带的信息，以及其他服务器端自身所保存的、并且可以被所有请求所使用的公共信息。
  - 无状态的服务器程序，最著名的就是WEB服务器。每次HTTP请求和以前都没有啥关系，只是获取目标URI。得到目标内容之后，这次连接就被杀死，没有任何痕迹。

## 2、为什么要做无状态化和容器化？

> 阻碍单体架构变为分布式架构的关键点就在于状态的处理。如果状态全部保存在本地，无论是本地的内存，还是本地的硬盘，都会给架构的横向扩展带来瓶颈。

**状态分为分发、处理、存储几个过程，**如果对于一个用户的所有的信息都保存在一个进程中，则从分发阶段，就必须将这个用户分发到这个进程，否则无法对这个用户进行处理，然而当一个进程压力很大的时候，根本无法扩容，新启动的进程根本无法处理那些保存在原来进程的用户的数据，不能分担压力。(做成全部无状态的服务其实压力很大)

所以要将整个架构分成两个部分，无状态部分和有状态部分，而**业务逻辑的部分往往作为无状态的部分**，而将状态保存在有状态的中间件中，如缓存、数据库、对象存储、大数据平台、消息队列等。

> 在微服务化之前，建议先进行容器化，在容器化之前，建议先无状态化，当整个流程容器化了，以后的微服务拆分才会水到渠成。



## 3、容器化最佳实践——DevOps

> 很多人会将容器当成虚拟机来用，这是非常不正确的，而且容器所做的事情虚拟机都能做到。
>
> 如果部署的是一个传统的应用，这个应用启动速度慢，进程数量少，基本不更新，那么虚拟机完全能够满足需求。(也没必要容器化)
>
> 但是现在流量大，应用要经常更新。惟一的方法就是拆开，每个子模块自己更新；原来一个进程扛流量，现在多个进程一起扛——这就是微服务
>

但这个时候虚拟机就不行了，因为它每个镜像太大了。那就要有容器的概念：
- 从技术角度，能够使得大部分的内部配置可以放在镜像里面，
- 从流程角度，将环境配置这件事情，往前推了，推到了开发这里，要求开发完毕之后，就需要考虑环境部署的问题

但是这就相当于开发把运维的工作给做了，开发的老大愿意么？

这就不是技术问题了，**其实这就是DevOps**，DevOps（Development(开发)&Operations(运维)）不是不区分开发和运维，而是公司从组织到流程，能够打通，看如何合作，边界如何划分，对系统的稳定性更有好处。

- 虚拟化和容器，其实为DevOps提供了很好的前提条件。开发环境和部署环境都可以更好地隔离了，减小了相互之间的影响；并且版本的更新、回滚也更加方便，业务的上线也就更加敏捷！ 
- DevOps关注整个项目过程，涉及开发、运维等阶段，有点像项目管理；**并不是说是开发和运维的结合**。

![传统额软件开发流程分工——延伸出三类职业](https://raw.githubusercontent.com/BBQldf/PicGotest/master/d009b3de9c82d158ead4a6811ab107d0bd3e42eb.jpeg)

例如一个应用包含四个服务A、B、C、D，她们相互引用，相互依赖，如果使用了容器平台，则服务之间的服务发现就可以通过服务名进行了。

- A服务调用B服务，不需要知道B服务的IP地址，只需要在配置文件里面写入B服务服务名就可以了。（服务发现）
- 如果中间的节点宕机了，容器平台会自动将上面的服务在另外的机器上启动起来。容器启动之后，容器的IP地址就变了，但是不用担心，容器平台会自动将服务名B和新的IP地址映射好，A服务并无感知。这个过程叫做自修复和自发现。（容灾）
- 如果服务B遭遇了性能瓶颈，三个B服务才能支撑一个A服务，也不需要特殊配置，只需要将服务B的数量设置为3，A还是只需要访问服务B，容器平台会自动选择其中一个进行访问，这个过程称为弹性扩展（扩缩容）和负载均衡。

## 4、主要概念

### 3.1、Dockerfile

为什么一定要用Dockerfile，而不建议通过保存镜像的方式来生成镜像呢？

这样才能实现环境配置和环境部署代码化 ，将Dockerfile维护在Git里面，有版本控制，并且通过自动化的build的过程来生成镜像，而镜像中就是环境的配置和环境的部署，要修改环境应先通过Git上面修改Dockerfile的方式进行



### 3.2、容器镜像

- 通过Dockerfile可以生成容器镜像，容器的镜像是分层保存，对于Dockerfile中的每一个语句，生成一层容器镜像，如此叠加，每一层都有UUID。

- 容器镜像可以打一个版本号，放入统一的镜像仓库。



### 3.3、容器运行时隔离

- 容器运行时，是将容器镜像之上加一层可写入层，为容器运行时所看到的文件系统。容器运行时使用了两种隔离的技术：
  - 一种是看起来是隔离的技术，称为namespace，也即每个namespace中的应用看到的是不同的IP地址、用户空间、程号等。
  - 另一种是用起来是隔离的技术，称为CGroup，也即明明整台机器有很多的CPU、内存，而一个应用只能用其中的一部分。

### 3.4、镜像、容器、仓库

> Docker本身并不是容器，它是创建容器的工具，是应用容器引擎；
>
> Docker技术的三大核心概念，分别是：
>
> - **镜像（Image）**
> - **容器（Container）**
> - **仓库（Repository）**

还是以建房子那个例子来做说明：

1. 建个房子，于是我搬石头、砍木头、画图纸，一顿操作，终于把这个房子盖好了
2. 想搬到另一片空地去。这时候，按以往的办法，我只能再次搬石头、砍木头、画图纸、盖房子。
   - 但是，现在有一种魔法，可以把我盖好的房子复制一份，做成“镜像”，放在我的背包里。
3. 等我到了另一片空地，就用这个“镜像”，复制一套房子，摆在那边，拎包入住。

那个放在包里的“镜像”，就是Docker镜像。而我的背包，就是Docker仓库。我在空地上，用魔法造好的房子，就是一个Docker容器。——容器可以反复部署！

Docker镜像：

- 是一个特殊的文件系统。它除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（例如环境变量）。**镜像不包含任何动态数据，**其内容在构建之后也不会被改变。

单个镜像可以用来构建出容器，众多的镜像组合在一起，就变成了仓库。

- 负责对Docker镜像进行管理的，是Docker Registry服务（类似仓库管理员）。
- 最常使用的Registry公开服务，是官方的Docker Hub，这也是默认的Registry，并拥有大量的高质量的官方镜像。



# 三、K8s

>K8S，就是基于容器的集群管理平台，它的全称，是kubernetes。
>
>Kubernetes模块划分得更细，模块比较多，而且模块之间完全的松耦合，可以非常方便地进行定制化。



> Kubernetes是天然的微服务架构，Kubernetes 可以在物理或虚拟机集群上调度和运行应用程序容器。
>
> Kubernetes 还允许开发人员从物理和虚拟机’脱离’，从以主机为中心的基础架构转移到以**容器为中心**的基础架构，这样可以提供容器固有的全部优点和益处。
>
> 这才是真正的平台即服务（PAAS），以及基础架构即服务（IAAS）
>
> 难点其实是服务上云之后，与存储的交互。

Kubernetes 满足了生产中运行应用程序的许多常见的需求，例如：

- [Pod](https://kubernetes.io/docs/user-guide/pods/) 提供复合应用并保留一个应用一个容器的容器模型
- [挂载外部存储](https://kubernetes.io/docs/user-guide/volumes/)
- [Secret管理](https://kubernetes.io/docs/user-guide/secrets/)
- [应用健康检查](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
- [副本应用实例](https://kubernetes.io/docs/user-guide/replication-controller/)
- [横向自动扩缩容](https://kubernetes.io/docs/user-guide/horizontal-pod-autoscaling/)
- [服务发现](https://kubernetes.io/docs/user-guide/connecting-applications/)
- [负载均衡](https://kubernetes.io/docs/user-guide/services/)
- [滚动更新](https://kubernetes.io/docs/user-guide/update-demo/)
- [资源监测](https://kubernetes.io/docs/user-guide/monitoring/)
- [日志采集和存储](https://kubernetes.io/docs/user-guide/logging/overview/)
- [支持自检和调试](https://kubernetes.io/docs/user-guide/introspection-and-debugging/)
- [认证和鉴权](https://kubernetes.io/docs/admin/authorization/)

K8S使用过程中，涉及到的挑战有很多：

- 容器网络
- 路由与服务发现
- 监控
- 告警
- 多集群管理
- 镜像仓库
- 依赖IP/模块鉴权的服务(权限同步)
- 远程存储
- 远程日志
- 远程登录
- CI/CD
- 安全、审计

>当规模比较大，应用比较复杂的时候，则推荐Kubernetes。
>
>而且Kubernetes的数据结构的设计层次比较细，非常符合微服务的设计思想。例如从容器->Pods->Deployment->Service。

- 这种细粒度的划分，使得本来简单运行一个容器，被封装为这么多的层次，每次层有自己的作用，每一层都可以拆分和组合，这样带来一个很大的缺点，就是学习门槛高，为了简单运行一个容器，需要先学习一大堆的概念和编排规则。

- 但是当需要部署的业务越来越复杂时，场景越来越多时，你会发现Kubernetes这种细粒度设计的优雅。**比如，**对于Service来讲，除了提供内部服务之间的发现和相互访问外，还灵活设计了headless service，这使得很多游戏需要有状态的保持长连接有了很好的方式，另外访问外部服务时，例如数据库、缓存、headless service相当于一个DNS，使得配置外部服务简单很多。

## 1、主要概念

**一个K8S系统，通常称为一个K8S集群（Cluster）。**

这个集群主要包括两个部分：（主从的特性出来了；但是这个只是传统的主从，即一个是用于控制，另一个是为了计算）

- **一个Master节点（主节点）**
- **一群Node节点（计算节点）**

### 1.1 Master

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220703154842.png)

- API Server是整个系统的对外接口，供客户端和其它组件调用，相当于“营业厅”。

- Scheduler负责对集群内部的资源进行调度，相当于“调度室”。

- Controller manager负责管理控制器，相当于“大总管”。
- etcd负责服务发现和注册，它提供了数据TTL失效、数据改变监视、多值、目录监听、分布式锁原子操作等功能，可以方便的跟踪并管理集群节点的状态，相当于“保安”。（`可以参考本blog的第四节“ETCD-高可用的K/V存储和服务发现系统”`）

### 1.2 Nodes

![](https://raw.githubusercontent.com/BBQldf/PicGotest/master/20220703155133.png)

Node节点包括Docker、kubelet、kube-proxy、Fluentd、kube-dns（可选），还有就是**Pod**。（比较有实际意义的就是这个东西，其他的都是底层的配置）

- Docker，不用说了，创建容器的。（这里可以看到docker只是node下面的一个工具；也能很直接地看到k8s和docker之间的区别）
- Kubelet，主要负责监视指派到它所在Node上的Pod，包括创建、修改、监控、删除等。
- Kube-proxy，主要负责为Pod对象提供代理。
- Fluentd，主要负责日志收集、存储与查询。

#### 1.2.1 Pods

Pod是Kubernetes的基本操作单元，把**相关的一个或多个容器**构成一个Pod,通常Pod里的容器运行相同的应用。Pod包含的容器运行在同一个Minion (Host) 上，看作一个统一管理单元， 共享相同的volumes、network namespace/IP和Port空间。

一个比较让人关心的就是资源的分配。比如，IP地址在这里面是怎么区分的：

- Node IP：Node节点的IP地址，即物理网卡的IP地址。
- Pod IP：Pod的IP地址，即docker容器的IP地址，此为虚拟IP地址。
- Cluster IP：Service的IP地址，此为虚拟IP地址。



#### 1.2.2 Services

> 这个说实话不好理解，可以先立即成服务的抽象。
>
> 一个Service可以看作一组提供相同服务的Pod的对外访问接口。
>
> 在节点的粒度上面可以感受到：**node>service>pod**

Services也是Kubernetes的基本操作单元，是真实应用服务的抽象，每一个服务后面都有很多对应的容器来支持，通过Proxy的port和selector服务决定服务请求传递给后端提供服务的容器，对外表现为一个单-访问接口，外部不需要了解后端如何运行，这给扩展或维护后端带来很大的好处。





### 1.3 Replication Controllers

- Replication Controller确保任何时候Kubernetes集群中有指定数量的Pod副本(replicas)在运行。如果少于指定数量的Pod副本(replicas )，Replication Controller会启动新的Container,反之会“杀死” 多余的以保证数量不变。
- Replication Controller使用预先定义的Pod模板创建pods,一旦创建成功，Pod模板和创建的pods没有任何关联，可以修改Pod模板而不会对已创建pods有任何影响，也可以直接更新通过Replication Controller创建的pods。

### 1.4 Labels

- Labels是用于区分Pod、Service、 Replication Controller 的key/value键值对，Pod、 Service、 Replication Controller可以有多个L abel,但是每个L abel的key只能对应一个value。
- Labels 是Service和Replication Controller运行的基础，**为了将访问Service的请求转发给后端提供服务的多个容器，**正是通过标识容器的L abels来选择正确的容器。

- 同样，Replication Controller 也使用Labels来管理通过Pod模板创建的一组容器， 这样Replication Controller可以更加容易、方便地管理多个容器，无论有多少容器。

ok，概念就先了解这么多，具体的虚拟化实现是一个大工程，在腾讯，有专门的CSIG（腾讯云）和TCG（基础架构）部门来做；在华为，杭研所也有相关的部门；

还是掌握一些平台的使用比较有效！（比如，腾讯的stke）



# 四、ETCD-高可用的K/V存储和服务发现系统

> Etcd是CoreOS基于Raft协议开发的[分布式](https://so.csdn.net/so/search?q=分布式&spm=1001.2101.3001.7020)key-value存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。
>
>  在[分布式系统](https://so.csdn.net/so/search?q=分布式系统&spm=1001.2101.3001.7020)中，如何管理节点间的状态一直是一个难题，etcd像是专门为集群环境的服务发现和注册而涉及，它提供了数据TTL失效、数据改变监视、多值、目录监听、分布式锁原子操作等功能，可以方便的跟踪并管理集群节点的状态。

**主要功能：**

           1. **基本的key-value存储**
           2. **监听机制**
           3. key的过期及续约机制， 用于**监控和服务发现**
           4. 原子Compare And Swap和Compare And Delete, **用于分布式锁和leader选举**

[其实更业务级别一点的是：](https://www.cnblogs.com/peteremperor/p/16012320.html)

1. **配置中心：**分布式的键值存储系统，其优秀的读写性能、一致性和可用性的机制，非常适合用来做配置中心角色
2. **分布式锁：** 使用 Raft 算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁
3. **服务注册与服务发现**：和配置中心功能类似，不同之处在于服务注册和服务发现，还伴随着状态检测。从本质上说，服务发现就是要了解集群中是否有进程在监听udp或者tcp端口，并且通过名字就可以进行查找和链接。
4. **节点监控**： Watcher 机制，当某个节点消失或有变动时，Watcher 会第一时间发现并告知用户；节点可以设置TTL key，比如每隔 30s 发送一次心跳使代表该机器存活的节点继续存在，否则节点消失
5. **消息订阅和发布**：etcd内置watch机制，完全可以实现一个小型的消息订阅和发布组件。即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。通过这种方式可以做到分布式系统配置的集中式管理与动态更新。
6. **分布式队列**：与分布式锁的控制时序用法类似，即创建一个先进先出的队列，保证顺序。
7. **负载均衡**（kafka的应用）：etcd 集群化以后，每个 etcd 的核心节点都可以处理用户的请求。所以，把数据量小但是访问频繁的消息数据直接存储到 etcd 中也是个不错的选择，如业务系统中常用的二级代码表（在表中存储代码，在 etcd 中存储代码所代表的具体含义，业务系统调用查表的过程，就需要查找表中代码的含义）。
   - 利用 etcd 维护一个负载均衡节点表。etcd 可以监控一个集群中多个节点的状态，当有一个请求发过来后，可以轮询式的把请求转发给存活着的多个状态。类似 KafkaMQ，通过ZooKeeper来维护生产者和消费者的负载均衡。

[etcd性能表现：](https://link.zhihu.com/?target=https%3A//etcd.io/docs/v3.4.0/op-guide/performance/) （这个和redis类似其实，记住这个量级，在实际开发中是非常重要的参考）

- 读：1w ~ 10w 之间
- 写：1w左右

## 1、Raft协议

> raft协议本身是为了实现分布式系统一致性（选举的方式选举出leader，多次确认的方式来写入数值 ）
>
> 这个东西，其实在“拜占庭将军问题”中有介绍，在博客中也有介绍，可以参考`javaReview04 - 四、分布式 - 8、分布式一致性算法`，与它相类似的还有Paxos算法、ZAB算法
>
> 比如：Redis中哨兵集群中的哨兵领导者选举和Redis-Cluster中的主节点选举都是基于Raft算法实现的。

在ETCD中，etcd作为一个节点本身有三个状态（follower遵循者/Candidate候选人/leader意见领袖）。

1. 在初始化的时候是以follower来进行初始化，如果follower没有leader则会把自己转变为Candidate（150ms-300ms随机时间），则自己去拉票，只要超过半数的票则从Candidate变成leader。
2. 所有的集群都要听从leader的安排，leader会每时每刻保持心跳，让其他follower知道leader在线。
3.  有leader以后所有的修改都会从leader发起。假设设置一个数值，会先预提交，leader要通知其他follower进行设置数值，当集群超过半数的人修改完成commit，leader则对于自身设置数值进行commit。



**etcd 使用 Raft 算法保持了数据的强一致性，**某次操作存储到集群中的值必然是全局一致的，所以很容易实现**分布式锁**。锁服务有两种使用方式，一是保持独占，二是控制时序。

- 保持独占即所有获取锁的用户最终只有一个可以得到。etcd 为此提供了一套实现分布式锁原子操作 CAS（CompareAndSwap）的 API。通过设置prevExist值，可以保证在多个节点同时去创建某个目录时，只有一个成功。而创建成功的用户就可以认为是获得了锁。
- 控制时序，即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序。etcd 为此也提供了一套 API（自动创建有序键），对一个目录建值时指定为POST动作，这样 etcd 会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用 API 按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号。



## 2、[etcd整个写数据流程](https://blog.csdn.net/weixin_39540280/article/details/122007678)

![写数据流程步骤图解](https://raw.githubusercontent.com/BBQldf/PicGotest/master/41cd546f888d43aba05077eae38c742d.png)

1. 当客户端对etcd发起请求的时候，如果etcd不是leader的状态而是follower，follower则会将请求转发leader; 如果是leader后， 会对其进行预检查，检查（配额、限速、鉴权【判断请求是否合法】、包大小【需要小于1.5M，过大则会拒绝】）
2. 如果请求本身是合法的，会将请求转发给KVServer处理
3. KVserver一致性模块进行数据处理，一致性模块是基于raft协议实现的，这时候的数据本身是处于unstable状态
4. 当leader该数据处理unstable状态后，会通过rpc通知其他follower也来同步该数据，并且leader本身会在数据同步到日志模块【wal日志， wal日志通过fsync落盘到磁盘中】。而其他follow在同步该数据的时候，本身完成的是步骤3和数据同步到日志模块，follower一致性模块数据变成commited状态，当完成了这些后通过上次rpc返回响应体给leader
5. leader在收到了超过半数集群本身确认后，更新MatchIndex, 一致性模块中数据本身由unstable变化成commited状态。这时候通过**MVCC模块**【treeIndex和BoltDB开源组件组成】进行状态机的写入，将数据同步到**treeIndex**（这一步是保证一致性的关键，他在这里写入了版本号）【会更新modified版本[当前版本号]， generations信息[创建的版本，当前版本数，过往的所有版本号]】。再通过BoltDB落盘到磁盘中。这时候一致性模块数据由commited变化为applied状态。【**在这里如果没有要求数据强一致性，弱一致性的话，那么数据在commited状态就认为数据已经同步完成了**】。（注意这句话，如果有一致性要求，最后的applied才有意义）

6. 再通过heatbeat将数据同步到follower中MVCC模块中。最终完成数据的一致性。如下图所示。 【如果follower比leader落后好几个版本，leader会通过headbeat带到follower进行同步】。——（可见，MVCC这个东西真的用的挺广的）

## 3、kubernetes如何使用etcd

> 在kubernetes-master\pkg\registry\core\pod\storage\storage.go对于每个k8s object都有一个这个文件负责对象的存储操作

我觉得，在K8s中，对比etcd的其实是它的存储功能：

- etcd诞生之日起，就定位成一种分布式存储系统，并在k8s 服务注册和服务发现中，为大家所认识。它偏重的是节点之间的通信和一致性的机制保证，并不强调单节点的读写性能。

- 而redis最早作为缓存系统出现在大众视野，也注定了redis需要更加侧重于读写性能和支持更多的存储模式，它不需要care一致性，也不需要侧重事务，因此可以达到很高的读写性能。

**总结一下，**redis常用来做缓存系统，etcd常用来做分布式系统中一些关键数据的存储服务，**比如服务注册和服务发现**。



## 4、kafka怎么使用etcd

> 2021年4月19日，Kafka 2.8.0正式发布，其中最引人瞩目的就是kafka通过自我管理的仲裁来替代ZooKeeper。
>
> 为什么kafka不再用zookeeper，而是用etcd。主要就是消息发布与订阅、服务发现等功能需要依靠**选举**，而etcd做得更加轻量级！
>
> 其实，准确的说，Kafka在2.8版本中正式废弃了Zookeeper，拥抱Raft，而raft支持得很好的就是etcd。etcd站在了巨人的肩膀上~

### 4.1 etcd vs Zookeeper

相较之下，Zookeeper有如下缺点：

- 复杂。Zookeeper的部署维护复杂，管理员需要掌握一系列的知识和技能；**而Paxos强一致性算法也是素来以复杂难懂而闻名于世**；另外，Zookeeper的使用也比较复杂，需要安装客户端，官方只提供了java和C两种语言的接口。
- Java编写。这里不是对Java有偏见，而是Java本身就偏向于重型应用，它会引入大量的依赖。而运维人员则普遍希望机器集群尽可能简单，维护起来也不易出错。
- 发展缓慢。Apache基金会项目特有的“Apache Way”在开源界饱受争议，其中一大原因就是由于基金会庞大的结构以及松散的管理导致项目发展缓慢。

而etcd作为一个后起之秀，其优点也很明显。

- 简单。使用Go语言编写部署简单；使用HTTP作为接口使用简单；**使用Raft算法保证强一致性让用户易于理解**。
- 数据持久化。etcd默认数据一更新就进行持久化。
- 安全。etcd支持SSL客户端安全认证。

### 4.2、Kafka对一致性的需求

Kafka中存在众多的Leader选举。比如，一个主题可以拥有多个分区(数据分片)，每一个数据分片可以配置多个副本，**如何保证一个分区的数据在多个副本之间的一致性成为一个迫切的需求**。

**Zookeeper的致命弱点**：（集群+选举都有漏洞）

1. Zookeeper是**集群部署**，只要集群中超过半数节点存活，即可提供服务，例如一个由3个节点的Zookeeper，允许1个Zookeeper节点宕机，集群仍然能提供服务。但Zookeeper的设计是CP模型，即要保证数据的强一致性，必然在可用性方面做出牺牲。
   - 在 Kafka 集群比较大，分区数很多的时候，ZooKeeper 存储的元数据就会很多，如果写入的数据量过大，ZooKeeper 的性能和稳定性就会下降，可能导致 Watch 的延时或丢失。
   - 像一些类似乐字节这样的大公司，可能要支持百万分区级别，这目前的 Kafka 单集群架构下是无法支持稳定运行的，也就是目前单集群可以承载的分区数有限。
2. Zookeeper集群中也存在所谓的Leader节点和从节点，Leader节点负责写，Leader与从节点可用接受读请求，**但在Zookeeper内部节点在选举时整个Zookeeper无法对外提供服务**。当然正常情况下选举会非常快，但在异常情况下就不好说了，例如Zookeeper节点发生full Gc，此时造成的影响将是毁灭性的。
   - Zookeeper节点如果频繁发生Full Gc，此时与客户端的会话将超时，由于此时无法响应客户端的心跳请求(Stop World)，从而与会话相关联的临时节点将被删除，注意，此时是所有的临时节点会被删除，Zookeeper依赖的事件通知机制将失效，整个集群的选举服务将失效。

随着分布式领域相关技术的不断完善，**去中心化**的思想逐步兴起，去Zookeeper的呼声也越来越高，在这个进程中涌现了一个非常优秀的算法：Raft协议。
