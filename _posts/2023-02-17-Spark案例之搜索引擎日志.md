---
layout:     post
title:     Spark案例
subtitle:   PySpark-搜索引擎日志分析
date:       2023-02-17
author:     ldf
header-img: img/post-bg-spark01.png
catalog: true
tags:
    - Spark
    - 系统
---

# 一、需求分析

针对搜狗实验室的语料数据-用户查询日志（网页搜索用户查询及点击记录）进行Spark数据分析

## 1、数据来源

搜索引擎查询日志库：包括约1个月(2008年6月)Sogou搜索引擎部分网页查询需求及用户点击情况的网页查询日志数据集合。

格式说明：

| 访问时间 | 用户ID            | 查询词 | 该URL在返回结果中的排名 | 用户点击的顺序号 | 用户点击的URL         |
| -------- | ----------------- | ------ | ----------------------- | ---------------- | --------------------- |
| 23:00:12 | 21312314234000123 | 安全   | 8                       | 3                | http://www.123.com    |
| 12:34:07 | 9879345123123     | 大数据 | 1                       | 1                | http://www.taeasc.com |

其中，用户ID是根据用户使用浏览器访问搜索引擎时的Cookie信息自动赋值，即同一次使用浏览器输入的不同查询对应同一个用户ID。

## 2、三大需求

1. 搜索关键词的统计
   - 字段切割
   - 词频统计
2. 用户搜索点击统计
   - `用户ID`和`查询词`两个字段需要单独拼接分组，并完成统计
3. 搜索时间段统计
   - `访问时间`字段分组、统计、排序

## 3、jieba分词库

jieba 是目前表现较为不错的 Python 中文分词组件。jieba库的分词原理：利用一个中文词库，确定汉字之间的关联概率，汉字间概率大的组成词组，形成分词结果。除了分词，用户还可以添加自定义的词组。

jieba库支持四种分词模式：

- **精确模式**，试图将句子最精确地切开，并且不存在冗余数据，适合文本分析：`jieba.cut（text, cut_all=False）`
- **全模式**，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义：`jieba.cut(text, cut_all=True)`
  - 会有冗余：比如`我来到吉林省长春市`会被分词为`我/来到/吉林/吉林省/省长/长春/长春市`
- **搜索引擎模式**，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词：`jieba.cut_for_search(text)`
  - 文本匹配任务中使用切词粒度更精细的搜索模式较为合适（cut_for_search），粒度太大的话，会导致没有相同词，从而文本相似度很低，尤其是短文本匹配任务中。
- **paddle模式**，利用PaddlePaddle深度学习框架，训练序列标注（双向GRU）网络模型实现分词。同时支持词性标注。（应该是百度提供的分词模型）：`jieba.cut(text, use_paddle=True)`

#### 自定义词典：

```
//自定义词典使用
import jieba
test_sent = "李小福是创新办主任也是云计算方面的专家"
jieba.load_userdict("E://userdict.txt")
words = jieba.cut(test_sent)
print(list(words))
```

常见的应用环境是各网络平台对**违禁词的查询搜索处理**，以及网站也对用户**个人信息**的处理，对购物方面**评价信息**的处理等等。

# 二、搜索关键词统计

> 搜索关键词的统计

先创建SparkContext对象：

```python
config = SparkConf().setAppName("test").setMaster("local[*]")
sc = SparkContext(conf = config)
```

1. **读取数据文件**

```python
file_rdd = sc.textFile("../data/input/SogouQ.txt")
```

2. **过滤出有效数据**：map操作对每个数据元素进行切分，切分规则`\t`

```python
split_rdd = file_rdd.map(lambda x:x.split("\t"))	# 数据是List类型，每个字段是str
```

3. 因为这个数据是基础数据，会被重复使用，作一个**持久化**

```python
split_rdd.persist(StroageLevel.DISK_ONLY)
```

4. 把搜索的关键词字段取出来

```python
context_rdd = split_rdd.map(lambda x:x[2])
```

5. 解除嵌套

```python
words_rdd = context_rdd.flatMap(context_jieba)		# 把各种list进行解除嵌套

def context_jieba(data):
	seq = jieba.cut_for_search(data)	# 把数据进行分词
	l = list()
	for word in seq:		# 分词后组成一个list
		l.append(word)
	return l
```

6. **数据清洗**：jieba分词的一些逻辑不太好，需要在清洗一下（这个时候可以给jieba提供我们自定义的分词字典`jieba.load_userdict`）

```python
filtered_rdd = words_rdd.filter(filter_words)	# 去掉一些无用词
final_words = filter_rdd.map(append_words)	# 把其中一些词进行替换

def filter_words(data):
	return data not in ["123","asdas","123das"]

def append_words(data):	# 把之前去掉的数据，和其他数据拼接在一起，才是我们需要的数据；
    if data == "x": data = "x123"
    if data == "y": data = "yasdas"
	if data == "z": data = "z123das"
    return (data,1)	# 直接把这个元素变成二元组，方便后面的计数
```

7. 对单词进行分组，求和；然后把数据排序后输出

```python
final_words.reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1], ascending = False, numPartitions = 1).foreach(println)

final_words.reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1], ascending = False, numPartitions = 1).take(5).foreach(println)	# 只去前5个
```



# 三、用户搜索点击统计

前面的操作还是一样，先创建SparkContext对象

1. 把用户的ID和用户的内容抽取出来，组成新的元组

```python
user_content_rdd = split_rdd.map(lambda x:(x[1],x[2]))	# x[1]是用户ID，x[2]是搜索关键词
```

2. 对用户搜索的关键字分词后，再和用户ID组合

```python
user_word_rdd = user_content_rdd.flatMap(extract_user_word)

def extract_user_word(data):
	userId = data[0]
	content = data[1]
	words = list(jieba.cut_for_search(content))
	if word in words: # 对数据进行过滤
		if filter_words(word):	# 如果是要被cut掉的数据，要重新进行拼接
			(userId + "_" + append_words(word)[0], 1)	# 把用户id和关键词进行组合后，形成新的元组
```

3. 把内容进行**求和**；**按value进行聚合**后输出

```python
final_user_word_rdd = user_content_rdd.reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],ascending = False, numPartition=1).foreach(println)
'''
输出：
[('378371283764_scala',1204),('435782734123_java',1103),('34857893473242_安全',546),...]
'''
```

# 四、搜索时间段统计

1. 取出搜索时间字段

```python
time_rdd = split_rdd.map(lambda x:x[0])
time_rdd.map(lambda x:(x.split(":")[0],1))	# 把“小时”的字段取出来
```

2. 求和；排序后输出

```
hour_rdd.reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1], ascending = False, numPartitions = 1).collect().foreach(println)
```

**最后通过saveAsTextFile方法把操作结果保存到指定位置的指定目录中**